{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defactoring Pace of Change\n",
    "\n",
    "We are interested in what the process and practice looks like of 'defactoring' code. Defactoring code can be understood as close reading the source code of a particular software. This may serve multiple purposes. The process might be geared towards peer reviewing code, it may be done to understand methodological aspects and implications, it may be used as a tool to disseminate and teach such new methodologies or how to use them. Here we use the process of defactoring to produce what might be the first critical edition of any source code in the domain of digital humanities. We unpack the code that Ted Underwood wrote to do the analysis that was reported on in the article he wrote together with Jordan Sellers *How Quickly Do Literary Standards Change?* (Underwood, T. & Sellers, J., 2015. *How Quickly Do Literary Standards Change?* Available at: https://figshare.com/articles/How_Quickly_Do_Literary_Standards_Change_/1418394\n",
    "\n",
    "The code that we are  going to defactor can be found in the Github repository where Underwood open sourced it: https://github.com/tedunderwood/paceofchange\n",
    "\n",
    "There is no given definition of 'defactoring' or its practice. We expect there to be some agreement about the aim and purpose of the process (that can be broadly understood as attempting to understand what a particular piece of code is doing). How this process of understanding is implemented is not prescribed and may take many forms. A potential approach is simply 'using the code' in similar analytic processes, however that would not generate much insights into the actual code that in that case is treated as a black box. Backward engineering where one—in th most narrow understanding of this tactic to unpack code—tries to estimate what code is doing by comparing input and output may give some more insights. Writing automated tests that gauge the different responses to input might yet be another approach. Here however we are interested in a deep and intimate understanding of code [Frabretti 2012] to understand how it is adding or changing methodology and how it could be critiqued. For this we think meticulously deconstructing the code—which it allows us to do as it is open source—works best. \n",
    "\n",
    "Code is made up of instructions that are knit together in a process that may included repetition and process paths that branch out and merge back into a main process. Even fairly simple code may thus result in a 'garden of forking paths' that allows for a combinatorial explosion of possible paths that could be walked. It would require a book-sized examination to do full justice to the code and all its possible execution pathways. Even though Underwood's code is not even particular long in comparison to other code bases, there are multiple potential pathways and various choices as to what data to use, liek rivers and tributaries of data and computation. To fully realize defactoring as a critical method possibly means to explicate all possible routes through some code base. This is certainly infeasble within the scope of this article. It would also require considerable computational hardware and assistance. \n",
    "\n",
    "Thus it is infeasible to represent here *all* execution paths, let alone deeply inspecting them. We chose therefore to deconstruct so that we follow 1 possible execution path through Underwood and Sellers' code that we think presents a good trade off between getting to know the code and actually being able to finish deconstructing it. \n",
    "\n",
    "In technical terms the deconstruction that is presented here is a refactoring of Underwood and Sellers' code so that it can be represented as a single computational list of computing instructions, on in one namespace. Underwood and Sellers purposely divided their code into logical and meaningfull parts, modules, and functions that work together. For a code author this is a means of understanding, keeping track, and controling the process of creating and executing an analysis. We found however that to gain insight in how the code works and especially to narrate and disseminate its working it is useful to refactor the code into what is usually understood as a poor coding practice, namely making it all one single long strongly integrated process. This makes the code resemble more of a linear narrative. We may have stumbled here on a difference between the nature of code and text, or the relative mutual excluding of forms of representation that are in one form, divided and branched, useful to a software developer (to organize a process of analysis), and in another form, linear and narrative, more useful for a human interpreter. What we observed also is that the process of deconstruction literature and code are not symetrical but mirrored. Where deconstruction of literature usually involves it being taking apart into its various components, meanings, and contetualizations, we found that deconstructing softare by defactoring means to get an understanding of the code, its functioning, and meaning by integrating its different and disaparate parts into one single linear computational narrative. Able and 'good practices' informed code, in other words seems already deconstructed (or 'refactored') into modules and composable parts. Underwood and Sellers in this manner deconstructed the problem of analyzing poems with the specific methodology they envisioned. For all practical purposes we effectively are turning well articulated code into sub-optimal code full of 'hacks' and 'code smells'. However, we do think this leverages our ability to understand the narrative that the code also is.\n",
    "\n",
    "We found it useful to intersperes the various logial parts of the code, those parts that seem to handle a clearly bounded 'step' in the algorithmic process, with narrative that reports on how we understood the code and its functioning at that moment of deconstruction. The Jupyter Notebook is a good fit for presenting this exploration of the code. It allows us to present a fully functioning (executable) code path, divided into steps that can be read and commented by us. Reading (and executing along the way) this notebook therefore gives the reader a close resembling of the experience of how we as deconstructionists 'close read' the code.\n",
    "\n",
    "To support ourselves in the reading process (and hopefully also the reader now) we found it useful to keep track of the 'state' of the code as it was executing. We implemented this by listing all the 'active' variables and there values at each step of the process. The explanation of each step is therefore also ammended with a listing of these variables. \n",
    "\n",
    "Integrating code parts that were purposefully decoupled is not without risk of introducing bugs and cripling the code. For instance, if there are two variables with the same name but different function in two distinct codeparts, putting these code parts into one computational narrative will result in these variables conflicting, possibly overwriting one with the other with uncertain results or broken code as a consequence. Luckily the code of Underwood and Sellers was of such composition that we did not run into these kind of troubles and where potential conflict occured it was easy enough to counter it by renaming variables or introducing little bits of helper code that do not change the general signature of the code. We have altered the code written by Underwood and Sellers. The letter is changed in some places, but we contend that the spirit of the work is the same. \n",
    "\n",
    "Remark that the changes and additions we introduced to the code indeed are interpretations and emandations of the code: they are critical interventions just like a textual scholar would undertake, for instance, when critically editing and interpreting a historic text. We do indeed think that the Jupyter Notebook (or rather its content) that is the result of our close examination of Underwood's and Seller's code can be considered the first 'critical edition' of a codebase within the digital humanities. As with firsts, we do realize that our method is probably imperfect and susceptible to many improvements. That however is exactly the purpose of our work here: to explore the valuable and feasible forms of code criticism and its epistemic role in the humanities. [TODO: this needs some STS facing additions too.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is *Defactoring*\n",
    "\n",
    "*Defactoring* is a process or possible a tool for \"[opening the black box](https://en.wikipedia.org/wiki/Blackboxing)\" of computational and data intensive scholarship. It is similar to the process of *[refactoring](https://en.wikipedia.org/wiki/Code_refactoring)* in that we are \"restructuring existing computing code without changing its external behavior.\" Typically, refactoring takes code and bundles it into a separate structure, such as a function or module, to make it more reusable and recombinable. Our efforts here have done just the opposite. We have taken code that was broken up over several functions and files and pulled it into a single, linear narrative. This process invokes *deconstructive* analytic in which we have *internally* vivisected Underwood and Sellers' code, but externally there is little change to the behavior or outputs. These reconfigurations enable us to unpack, drill-down, and dig into the code within the same platform we use to write about the code.\n",
    "\n",
    "Defactoring, as a method of analysis, is deeply imbricated with a technical platform (just as Underwood & Sellers' data, code, and analysis are as well). But rather than pushing the code into a distant repository separate from the prosaic narrative, we compose a *[computational narrative](http://blog.jupyter.org/2015/07/07/project-jupyter-computational-narratives-as-the-engine-of-collaborative-data-science/)* whereby the data, code, and expository text are bundled together. This means we have created a document that can be read by both humans and machines. \n",
    "\n",
    "Such a feat is made possible by the [Jupyter Notebook](http://jupyter.com). The particular [affordances](https://en.wikipedia.org/wiki/Affordance) of the Notebook allow us to weave code, data, and prose together into a single narrative that is simultaneously readable and executable. Given our purpose, to develop a method for critically engaging the code of computational scholarship, it is imperative to foreground Underwood & Sellers' code and Notebooks *afford* a technical mechanism for doing just that.\n",
    "\n",
    "### So what did we do?\n",
    "\n",
    "At a practical level, we have *forked* code from the [git repository](https://github.com/tedunderwood/paceofchange) to create a new *branch* containing this Notebook you are reading now. \n",
    "\n",
    "Underwood and Sellers' have crafted their code such that it relatively easy to replicate their results by simply running a single command. One of the first tasks was to look at the code and follow the path of execution from that initial command. In reading through the code (as contained in multiple `.py` files) we could piece together a rough idea of how Underwood and Sellers' performed their data preparation, normalization, and analysis. However, `.py` files leave much to be desired when it comes to readability and annotateablility. \n",
    "\n",
    "We then copied python code from the various files in the repository into a Jupyter Notebook. Not all of the code in the repository was copied into the notebook for three main reasons. First, not all of the code included in the files is actually needed to replicate the analysis process. The repository includes a bunch of extra code, one could call it \"cruft\", from exploratory analysis or earlier iterations of the analytical process. For example, the file `SonicScrewdiver.py` is never called upon although, based upon the name, we might hypothesize it is an important, catch-all, modules for a variety of different tasks. Other sections of the code, are valid code, but function calls are commented out (such as `binormal_select`) and never executed. We have opted not to include these unused functions or code blocks. Second, not all of the possible execution paths are being analyzed in this Notebook. There six allowable options for slicing the data in `replicate.py`, the entry-point for re-running the analysis. We opted to follow the default, \"full\", which will model the entire dataset. Third, there is a large amount of code that is part of third party libraries, such as the logistic regression implementation that is part of `scikit-learn`, for a multitude of reasons (including practicality) we have decided to \"step over\", and not copy third-party code into this notebook. When trying to follow the path of execution for any program one needs to recognize it is \"[turtles all the way down](https://en.wikipedia.org/wiki/Turtles_all_the_way_down).\" To make our task possible, we have to make a decision about how far down the path of execution the defactoring process will go. We make a decision to focus only on the code written by Underwood and Sellers and leave an analysis of subsequent third-party libraries to a later, and more ambitious, defactoring effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Defactoring code from python files to the notebook](notebook_resources/defactoring.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Underwood and Seller's code is stored within four files, `replicate.py`, `parallel_crossvalidate.py`, `metafilter.py`, and `modlingprocess.py`. Through the process of *defactoring* we have copied|move|transferred|imported code from text files to a Jupyter Notebook. This process has transformed the flow of the code, but not the outputs. \n",
    "\n",
    "In order for the code to execute seamlessly within the notebook, we had to make minor changes and tweaks to the code. These changes fall into 5 categories:\n",
    "\n",
    "* *defactoring functions* - This is the most significant of the changes. When we defactor a function we take the function's code and move it to the global namespace. This has the effect of elimiating the function and just making it part of the main execution path. \n",
    "* *defactoring function calls* - When a function has been defactored, it can no longer be called since there is no explicit definitional code.\n",
    "* *defactoring definitions* - Not all functions can be fully defactored. Functions that are called more than once or those that are short have been kept as re-usable functions. Defactoring Definition cells define the functions above the code cells that use them (preventing errors).\n",
    "* *defactoring namespace* - Because we have defactored some of the functions and their function calls some of the variables in the namespace need to be mapped to eachother. This happens we the return value of a defactored function needs to be stored in a differently named variable.\n",
    "* *defactoring inspections* - When we want to inspect the state of the process, we insert an inspection cell that prints the values of the variables of interest.\n",
    "* *defactoring import* - Because the code is reliant upon external and third party functions, we need to import that code into the global namespace. This cell contains all of those imports.\n",
    "\n",
    "\n",
    "One of the advantages to defactoring a function is it affords us the ability to insert critical commentary (in the form of markdown cells) into the code constituting the function itself. An unfortunate side-effect is that this makes keeping track of one's place in the code a bit difficult. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code expressed below has nine steps:\n",
    "* [Setting Parameters](#Setting-Parameters) - Specifies parameters for the loading, cleaning, and labeling of data as well as sets conditions for the logistic regression.\n",
    "* [Preparing Metadata](#Preparing-MetaData) - Generates a list of *.tsv files from the `poems/` directory. \n",
    "    * [Cleaning Metadata](#Cleaning-Metadata) - Loads the metadata file, `poemetadata.csv` and performs some cleaning of the metadata to make labeling easier.\n",
    "    * [Sorting Data](#Sorting-Data) - Sort the volumes into two bins, reviewed and not reviewed using the cleaned metadata.\n",
    "* [Transforming Words into Features](#Transforming-Words-into-Features) - Identifies the 3,200 most common words in the corpus. Those most common words will be the features for the regression.\n",
    "    * [Filtering Authors](#Filtering-Authors) - Removes poems by authors who have been reviewed.\n",
    "    * [Filtering Words](#Filtering-Words) - Remove any words from the poem data that are not in the most-common feature list.\n",
    "* [Training Predictive Models](#Training-Predictive-Models) - Run a separate logistic regression for each volume, using a single volume as held-out data and measure each model's predictive power.\n",
    "* [Modeling Coefficients](#Modeling-Coefficients) - Run a single logistic regression over all the data to inspect the salient coefficients.\n",
    "* [Saving Output](#Saving-Output) - Save the results of the predictions and the coefficents to disk as CSV files.\n",
    "* [Plotting Results](#Plotting Results) - Generate a plot showing the accuracy of the predictive models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the data\n",
    "\n",
    "\n",
    "Before we dive into the code, we had a question about \"what do they mean by *volume*?\" in the Pace of Change article.\n",
    "\n",
    "The answer, to be confirmed, lives in the [Understanding Genre in a Collection of a Million Volumes](https://figshare.com/articles/Understanding_Genre_in_a_Collection_of_a_Million_Volumes_Interim_Report/1281251). Basically, the idea of \"volume\" comes from the Hathi Trust.\n",
    "\n",
    "\n",
    "\n",
    "Volume is equivalent to book. Volume is the Hathi Trust unit. \n",
    "\n",
    "> We worked with HathiTrust, which contains the aggregated collections of large public and university libraries; in the period we’re considering (1820-1919), that gave us a collection of roughly 758,400 books in English, of which about 53,200 include significant amounts of poetry. This doesn’t exhaustively cover print culture; it’s still a sample, with particular selection biases. (page 5)\n",
    "\n",
    "It would seem the 53,200 number comes from the fact that Ted has page level genre information. The details of this can be found in an additional figshare repository, [Page-leve genre metadata](https://figshare.com/articles/Page_Level_Genre_Metadata_for_English_Language_Volumes_in_HathiTrust_1700_1922/1279201) where:\n",
    "\n",
    "> Volumes of pentry often include proce introduction, or front and back matter; this was trimmed using publicly-available metadata. (page 33)\n",
    "\n",
    "This is how he was able to extract individual poems from the Hathi Trust data.\n",
    "\n",
    "TODO: We are going to need to be a bit more formal about how we represent the processes that created the data used in this code.\n",
    "\n",
    "Joris and Matt had short discussion the potential for confounds in the *random* sample which might have skewed that sample because it had the potential to include reviewed poetry. Pages 33 and 34 for the Pace of Change talk about the sampling and data preparation. We needed to talk through exactly what they did to understand it fully. After our discussion we agreed with their reasoning, but we would have liked to see a few more numbers. For example, \n",
    "\n",
    "> when a stray volume from the random set turns up near the top of our model’s list of books likely to be reviewed, it does turn out that many of those authors are reasonably well-known (Rupert Brooke, Elaine Goodale Eastman). (page 33)\n",
    "\n",
    "We would like to see more supporting data about the ratio between well-known and not-well-known authors popping up at the top of the model's list of books. Perhaps this is splitting hairs, but it would be good to have this supporting data. This however raises the question about how to formalize the \"well-knownness\" of a particular author. This utlimately might be more of a commentary upon our total ignorance of 19th century poetry, Matt and Joris are not English literary scholars (Joris is a Dutch literary scholar) and as such neither of us know very much about 19th century poets. Basically, we don't know much about Rupert Brooke and Elaine Goodale Eastman. This is perhaps a minor nitpick.\n",
    "\n",
    "Our question is \"How many *well-known* authors were in the random sample?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the shape of the data\n",
    "\n",
    "The poems live in the `poems` directory. We extracted the first 20 lines of the file \"dul1.ark+=13960=t5fb5xg2z.poe.tsv\" to give a feel what they look like. Basically they represent the vocabulary frequency lists of each poem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",\t2745\r\n",
      "the\t1445\r\n",
      "and\t1182\r\n",
      ".\t672\r\n",
      "of\t468\r\n",
      "to\t442\r\n",
      ":\t386\r\n",
      "in\t384\r\n",
      ";\t324\r\n",
      "a\t253\r\n",
      "but\t228\r\n",
      "his\t223\r\n",
      "he\t218\r\n",
      "|'s|\t211\r\n",
      "with\t198\r\n",
      "—\t197\r\n",
      "that\t188\r\n",
      "on\t187\r\n",
      "they\t172\r\n",
      "for\t171\r\n"
     ]
    }
   ],
   "source": [
    "!head -n20 poems/dul1.ark+\\=13960\\=t5fb5xg2z.poe.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what we see are the token (not just word, but punctuation too) frequences for the volume. For this particular document, there are 2,745 commas, 1445 instances of the word `the`, and 1182 instances of the word `and`. \n",
    "\n",
    "In addition to data in the files in the `poems/` directory, there is a file, `poemeta.csv` that stores the metadata about each of the volumes. We can look at the first two lines of that file to see the headers and the first row of the data to see what the data points look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docid,actualdate,inferreddate,firstpub,recept,recordid,OCLC,author,imprint,enumcron,title,pubrev,judge,impaud,yrrev,pubname,birth,gender,nationality,othername,notes,canon\r\n",
      "loc.ark+=13960=t8sb4zz1q,1921,1921,1921,addcanon,537314,,\"Lawrence, D. H.\",New York;T. Seltzer;1921.,,Tortoises,,,,,,1885,m,uk,,,y\r\n"
     ]
    }
   ],
   "source": [
    "!head -n2 poemeta.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is the list of column headers of the metadata file, we can infer what they mean based upon their title, but there is no data dictionary to explicitly describe what these headers mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diving into the Code\n",
    "\n",
    "We know that `replicate.py` will be our main entry-point for this analysis because the README tells us to replicate the analysis we should run the command:\n",
    "\n",
    "> python3 replicate.py full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we will do below is copy the code from `replicate.py` into this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### DEFACTORING IMPORT\n",
    "import os\n",
    "import csv \n",
    "import random \n",
    "from collections import Counter \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from multiprocessing import Pool \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note to the reader. If you see a red warning box don't fret, this is a by-product of our effort to Dockerize this analysis. Hopefully this will someday go away.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Parameters\n",
    "\n",
    "So this is not exact copy of replicate.py, instead we have *defactored* the code by starting at the branch point at line 45 of replicate.py (git commit e2b5b8f9a86d3f80360865a6628488619f7849d6). Basically everything after `if command == 'full':` and before `elif command == 'quarters':` \n",
    "\n",
    "MJ: The 'rest' of this module basically splits of 6 branches of the process related to the six possible ways of carving up the data `{\"full\", \"quarters\", \"nations\", \"genders\", \"canon\", \"halves\"}`, each option sets the different paramaters for the analytic process associated with that specific command. \n",
    "\n",
    "For reasons of feasibility we will only examine the first branching off tied to the command 'full'.\n",
    "'Full' means: \n",
    "> process all 700 volumes model represented in Fig. 1 of the article \n",
    "\n",
    "(cf. \"model the full 700-volume dataset using default settings\" above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PATHS.\n",
    "\n",
    "sourcefolder = 'poems/'\n",
    "extension = '.poe.tsv'\n",
    "classpath = 'poemeta.csv'\n",
    "outputpath = 'mainmodelpredictions.csv'\n",
    "\n",
    "## EXCLUSIONS.\n",
    "\n",
    "excludeif = dict()\n",
    "excludeif['pubname'] = 'TEM'\n",
    "# We're not using reviews from Tait's.\n",
    "\n",
    "excludeif['recept'] = 'addcanon'\n",
    "# We don't ordinarily include canonical volumes that were not in either sample.\n",
    "# These are included only if we're testing the canon specifically.\n",
    "\n",
    "excludeifnot = dict()\n",
    "excludeabove = dict()\n",
    "excludebelow = dict()\n",
    "\n",
    "excludebelow['firstpub'] = 1700\n",
    "excludeabove['firstpub'] = 1950\n",
    "sizecap = 360\n",
    "\n",
    "# For more historically-interesting kinds of questions, we can limit the part\n",
    "# of the dataset that gets TRAINED on, while permitting the whole dataset to\n",
    "# be PREDICTED. (Note that we always exclude authors from their own training\n",
    "# set; this is in addition to that.) The variables futurethreshold and\n",
    "# pastthreshold set the chronological limits of the training set, inclusive\n",
    "# of the threshold itself.\n",
    "\n",
    "## THRESHOLDS\n",
    "\n",
    "futurethreshold = 1925\n",
    "pastthreshold = 1800\n",
    "\n",
    "# CLASSIFY CONDITIONS\n",
    "\n",
    "positive_class = 'rev'\n",
    "category2sorton = 'reviewed'\n",
    "datetype = 'firstpub'\n",
    "numfeatures = 3200\n",
    "regularization = .00007\n",
    "\n",
    "\n",
    "paths = (sourcefolder, extension, classpath, outputpath)\n",
    "exclusions = (excludeif, excludeifnot, excludebelow, excludeabove, sizecap)\n",
    "thresholds = (pastthreshold, futurethreshold)\n",
    "classifyconditions = (category2sorton, positive_class, datetype, numfeatures, regularization)\n",
    "\n",
    "### DEFACTORING FUNCTION CALL\n",
    "### rawaccuracy, allvolumes, coefficientuples = pc.create_model(paths, exclusions, thresholds, classifyconditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell above is the specification of a bunch of parameters. These parameters are a set of buttons and knobs which are used to tweak the performance and execution of the computational modeling process. For example, an important variable in the cell above is `regularization` since it specifies the regularization parameter for the logistic regression. What is not well documented here, is why the value .00007 was chosen over other values. \n",
    "\n",
    "The last line in the code cell above, which we have documented with the comment `DEFACTORING FUNCTION CALL` is the point in replicate.py where the script calls out to a function, `create_model()` in a separate file, `parallel_crossvalidate.py`. As part of the defactoring method we draw in the `create_model()` function to incorporate it into our computational narrative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('poems/', '.poe.tsv', 'poemeta.csv', 'mainmodelpredictions.csv')\n",
      "({'recept': 'addcanon', 'pubname': 'TEM'}, {}, {'firstpub': 1700}, {'firstpub': 1950}, 360)\n",
      "(1800, 1925)\n",
      "('reviewed', 'rev', 'firstpub', 3200, 7e-05)\n"
     ]
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "### what are the values being passed to create_model\n",
    "print(paths)\n",
    "print(exclusions)\n",
    "print(thresholds)\n",
    "print(classifyconditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Metadata\n",
    "\n",
    "The code below captures the `create_model()` function, which does computational work that we don't want to *step over* because it is the documentation of Underwood and Seller's analysis. We don't want to treat it as a black box, we want to open it up. So the code cells below *step into* the function and explore its contents in the notebook's global namespace.\n",
    "\n",
    "What is important to recognize about this block of code is that it is only executed once (in our tracing of an execution path), so its decomposition from a function and into the notebook doesn't cause problems with execution. There will be other functions in the narrative below that cannot be fully defactored, because they are repeatedly executed or are part of other python libraries not written by Underwood and Sellers. Those written by Underwood and Sellers are documented in the notebook and those part of other libraries are imported but remain undocumented.\n",
    "\n",
    "Given the size and complexity of the `create_model()` function, the code has broken up over a series of cells to allow for critical discussion of the computational work being executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### DEFACTORING FUNCTION\n",
    "### def create_model(paths, exclusions, thresholds, classifyconditions):\n",
    "''' This is the main function in the module.\n",
    "It can be called externally; it's also called\n",
    "if the module is run directly.\n",
    "'''\n",
    "verbose = False\n",
    "\n",
    "if not sourcefolder.endswith('/'):\n",
    "    sourcefolder = sourcefolder + '/'\n",
    "\n",
    "# This just makes things easier.\n",
    "\n",
    "# Get a list of files.\n",
    "allthefiles = os.listdir(sourcefolder)\n",
    "# random.shuffle(allthefiles)\n",
    "\n",
    "volumeIDs = list()\n",
    "volumepaths = list()\n",
    "\n",
    "for filename in allthefiles:\n",
    "\n",
    "    if filename.endswith(extension):\n",
    "        volID = filename.replace(extension, \"\")\n",
    "        # The volume ID is basically the filename minus its extension.\n",
    "        # Extensions are likely to be long enough that there is little\n",
    "        # danger of accidental occurrence inside a filename. E.g.\n",
    "        # '.fic.tsv'\n",
    "        path = sourcefolder + filename\n",
    "        volumeIDs.append(volID)\n",
    "        volumepaths.append(path)\n",
    "        \n",
    "### DEFACTORING FUNCTION CALL\n",
    "### metadict = metafilter.get_metadata(classpath, volumeIDs, excludeif, excludeifnot, excludebelow, excludeabove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code assembles a list of volume identifiers (`volumeIDs`) and file paths (`volumepaths`) by readings the directory listing of files in the `poems/` directory (`sourcefolder`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poemeta.csv\n",
      "['dul1.ark+=13960=t5fb5xg2z', 'dul1.ark+=13960=t75t4h116', 'dul1.ark+=13960=t84j19z0d', 'ellisbell1848', 'emilydickinson', 'gerardmhopkins1918', 'hardywessexpoems1898', 'hvd.32044010164861', 'hvd.32044018706432', 'hvd.32044020453569']\n",
      "{'recept': 'addcanon', 'pubname': 'TEM'}\n",
      "{}\n",
      "{'firstpub': 1700}\n",
      "{'firstpub': 1950}\n"
     ]
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "### what are the values of the variables being passed to get_metadata()\n",
    "### TODO: write a function that when given a list of variables, prints out their name & values\n",
    "print(classpath)\n",
    "print(volumeIDs[0:10]) # this is a long list so only showing the first 10 values\n",
    "print(excludeif)\n",
    "print(excludeifnot)\n",
    "print(excludebelow)\n",
    "print(excludeabove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Metadata\n",
    "\n",
    "Before we can fully dive into the code contents of the `get_metadata()` function, we need to define a couple helper functions. These small functions perform some minor data transformations that are repeatedly used in the `get_metadata()` block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### DEFACTORING FUNCTION DEFINITION\n",
    "### we need these helper functions for execute the next code cell\n",
    "\n",
    "def dirty_pairtree(htid):\n",
    "    period = htid.find('.')\n",
    "    prefix = htid[0:period]\n",
    "    postfix = htid[(period+1): ]\n",
    "    if '=' in postfix:\n",
    "        postfix = postfix.replace('+',':')\n",
    "        postfix = postfix.replace('=','/')\n",
    "    dirtyname = prefix + \".\" + postfix\n",
    "    return dirtyname\n",
    "\n",
    "def forceint(astring):\n",
    "    try:\n",
    "        intval = int(astring)\n",
    "    except:\n",
    "        intval = 0\n",
    "\n",
    "    return intval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a long block of code that is difficult to break apart because of the `with open` block of code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poemeta.csv\n",
      "tossing loc.ark:/13960/t8sb4zz1q\n",
      "tossing mdp.39015013402501\n",
      "tossing mdp.39015011913525\n",
      "tossing hardywessexpoems189.hardywessexpoems1898\n",
      "tossing gerardmhopkins191.gerardmhopkins1918\n",
      "tossing loc.ark:/13960/t3fx82c2q\n",
      "tossing emilydickinso.emilydickinson\n",
      "tossing ellisbell184.ellisbell1848\n",
      "We have 8 volumes in missing in metadata, and\n",
      "0 volumes missing in the directory.\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "### DEFACTORING FUNCTION \n",
    "### def get_metadata(classpath, volumeIDs, excludeif, excludeifnot, excludebelow, excludeabove):\n",
    "'''\n",
    "As the name would imply, this gets metadata matching a given set of volume\n",
    "IDs. It returns a dictionary containing only those volumes that were present\n",
    "both in metadata and in the data folder.\n",
    "\n",
    "It also accepts four dictionaries containing criteria that will exclude volumes\n",
    "from the modeling process.\n",
    "'''\n",
    "print(classpath)\n",
    "metadict = dict()\n",
    "\n",
    "with open(classpath, encoding = 'utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "\n",
    "    anonctr = 0\n",
    "\n",
    "    for row in reader:\n",
    "        volid = dirty_pairtree(row['docid'])\n",
    "        theclass = row['recept'].strip()\n",
    "\n",
    "        # I've put 'remove' in the reception column for certain\n",
    "        # things that are anomalous.\n",
    "        if theclass == 'remove':\n",
    "            continue\n",
    "\n",
    "        bail = False\n",
    "        for key, value in excludeif.items():\n",
    "            if row[key] == value:\n",
    "                bail = True\n",
    "        for key, value in excludeifnot.items():\n",
    "            if row[key] != value:\n",
    "                bail = True\n",
    "        for key, value in excludebelow.items():\n",
    "            if forceint(row[key]) < value:\n",
    "                bail = True\n",
    "        for key, value in excludeabove.items():\n",
    "            if forceint(row[key]) > value:\n",
    "                bail = True\n",
    "\n",
    "        if bail:\n",
    "            print(\"tossing \"+volid) ## DEFACTORING CODE\n",
    "            continue\n",
    "\n",
    "        birthdate = forceint(row['birth'])\n",
    "\n",
    "        pubdate = forceint(row['inferreddate'])\n",
    "\n",
    "        gender = row['gender'].rstrip()\n",
    "        nation = row['nationality'].rstrip()\n",
    "\n",
    "        #if pubdate >= 1880:\n",
    "            #continue\n",
    "\n",
    "        if nation == 'ca':\n",
    "            nation = 'us'\n",
    "        elif nation == 'ir':\n",
    "            nation = 'uk'\n",
    "        # I hope none of my Canadian or Irish friends notice this.\n",
    "\n",
    "        notes = row['notes'].lower()\n",
    "        author = row['author']\n",
    "        if len(author) < 1 or author == '<blank>':\n",
    "            author = \"anonymous\" + str(anonctr)\n",
    "            anonctr += 1\n",
    "\n",
    "        title = row['title']\n",
    "        canon = row['canon']\n",
    "\n",
    "        # I'm creating two distinct columns to indicate kinds of\n",
    "        # literary distinction. The reviewed column is based purely\n",
    "        # on the question of whether this work was in fact in our\n",
    "        # sample of contemporaneous reviews. The obscure column incorporates\n",
    "        # information from post-hoc biographies, which trumps\n",
    "        # the question of reviewing when they conflict.\n",
    "\n",
    "        if theclass == 'random':\n",
    "            obscure = 'obscure'\n",
    "            reviewed = 'not'\n",
    "        elif theclass == 'reviewed':\n",
    "            obscure = 'known'\n",
    "            reviewed = 'rev'\n",
    "        elif theclass == 'addcanon':\n",
    "            print(\"this is executing\") ## DEFACTORING CODE\n",
    "            obscure = 'known'\n",
    "            reviewed = 'addedbecausecanon'\n",
    "        else:\n",
    "            print(\"Missing class\" + theclass)\n",
    "\n",
    "        if notes == 'well-known':\n",
    "            obscure = 'known'\n",
    "        if notes == 'obscure':\n",
    "            obscure = 'obscure'\n",
    "\n",
    "        if canon == 'y':\n",
    "            if theclass == 'addcanon':\n",
    "                actually = 'Norton, added'\n",
    "            else:\n",
    "                actually = 'Norton, in-set'\n",
    "        elif reviewed == 'rev':\n",
    "            actually = 'reviewed'\n",
    "        else:\n",
    "            actually = 'random'\n",
    "\n",
    "        metadict[volid] = dict()\n",
    "        metadict[volid]['reviewed'] = reviewed\n",
    "        metadict[volid]['obscure'] = obscure\n",
    "        metadict[volid]['pubdate'] = pubdate\n",
    "        metadict[volid]['birthdate'] = birthdate\n",
    "        metadict[volid]['gender'] = gender\n",
    "        metadict[volid]['nation'] = nation\n",
    "        metadict[volid]['author'] = author\n",
    "        metadict[volid]['title'] = title\n",
    "        metadict[volid]['canonicity'] = actually\n",
    "        metadict[volid]['pubname'] = row['pubname']\n",
    "        metadict[volid]['firstpub'] = forceint(row['firstpub'])\n",
    "\n",
    "# These come in as dirty pairtree; we need to make them clean.\n",
    "\n",
    "cleanmetadict = dict()\n",
    "allidsinmeta = set([x for x in metadict.keys()])\n",
    "allidsindir = set([dirty_pairtree(x) for x in volumeIDs])\n",
    "missinginmeta = len(allidsindir - allidsinmeta)\n",
    "missingindir = len(allidsinmeta - allidsindir)\n",
    "print(\"We have \" + str(missinginmeta) + \" volumes in missing in metadata, and\")\n",
    "print(str(missingindir) + \" volumes missing in the directory.\")\n",
    "print(allidsinmeta - allidsindir)\n",
    "\n",
    "for anid in volumeIDs:\n",
    "    dirtyid = dirty_pairtree(anid)\n",
    "    if dirtyid in metadict:\n",
    "        cleanmetadict[anid] = metadict[dirtyid]\n",
    "\n",
    "### DEFACTORING FUNCTION RETURN\n",
    "### return cleanmetadict               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': 'Arnold, Matthew,',\n",
       " 'birthdate': 1822,\n",
       " 'canonicity': 'Norton, in-set',\n",
       " 'firstpub': 1855,\n",
       " 'gender': 'm',\n",
       " 'nation': 'uk',\n",
       " 'obscure': 'known',\n",
       " 'pubdate': 1855,\n",
       " 'pubname': 'ER',\n",
       " 'reviewed': 'rev',\n",
       " 'title': 'Poems'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "### looking up an ID listed earlier\n",
    "cleanmetadict['dul1.ark+=13960=t5fb5xg2z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### DEFACTORING NAMESPACE \n",
    "metadict = cleanmetadict  # put the data into the global namespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What just happened here? Well, the code above loaded the `poemeta.csv` file and filtered out a bunch of rows (based upon the `excludeif`, `excludeifnot`, `excludeabove`, and `excludebelow` variables) and also normalizes some of the `nation` data (normalizing is a pretty clinical way of lumping Canada with the United States and Ireland with the UK). Nationality is not a factor in the Pace of Change analysis, but it is interesting to see this code here, it implies this code is used in other analyses.\n",
    "\n",
    "The other important thing this code cell does is split the `recept` column in the `poemeta.csv` into two columns, `obscure` and `reviewed`. There is a bit of logic here that we do not fully grasp at this point. From what we can tell from the code and Ted's comment, there poems that are reviewed, there are poems that are obscure, and there are poems that are not in the reviewed set but are never-the-less part of the cannon. This means they are \"known\" and, according to Ted's comment, trumps the conflict when the author is known (`obscure = 'known'`) but not explicitly in the reviewed set. \n",
    "\n",
    "We have discovered after adding some `# DEFACTORING CODE` snippets that this code never actually runs. All of the poems with the 'addcanon' property are tossed out and the conflict, where the poem is known by in the random set, never appears to occur. Conjector: is this a remnant of Ted refactoring the code due to changes in the analysis process or just working with different data or something we cannot possible conceive. What was the author's intent? \n",
    "\n",
    "We know that poems with the 'addcanon' in the 'recept' column are being excluded because they are included in the `excludeif` dictionary. Why? The code in the first code cell provides somewhat of an explanation\n",
    "\n",
    "```\n",
    "excludeif['recept'] = 'addcanon'\n",
    "# We don't ordinarily include canonical volumes that were not in either sample.\n",
    "# These are included only if we're testing the canon specifically.\n",
    "```\n",
    "\n",
    "It should noted we spent a considerable amount of time interpreting the code that handled this particular situation before realizing that it would never be executed because of the settings in the `excludeif` dictionary. That makes us look stupid, but we also now have a more intimate understanding and relationship with the code. or maybe we are still stupid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now that we have a list of volumes with metadata, we can select the groups of IDs\n",
    "# that we actually intend to contrast. If we want to us more or less everything,\n",
    "# this may not be necessary. But in some cases we want to use randomly sampled subsets.\n",
    "\n",
    "# The default condition here is\n",
    "\n",
    "# category2sorton = 'reviewed'\n",
    "# positive_class = 'rev'\n",
    "# sizecap = 350\n",
    "# A sizecap less than one means, no sizecap.\n",
    "\n",
    "### DEFACTORING FUNCTION CALL\n",
    "### IDsToUse, classdictionary = metafilter.label_classes(metadict, category2sorton, positive_class, sizecap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting Training Data\n",
    "\n",
    "We have DEFACTORED the metafilter.label_classes() function. This function reads the metadata properties and puts all entries into one of two bins: *positive* or *negative*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### DEFACTORING FUNCTION\n",
    "### def label_classes(metadict, category2sorton, positive_class, sizecap):\n",
    "''' This takes as input the metadata dictionary generated\n",
    "by get_metadata. It subsets that dictionary into a\n",
    "positive class and a negative class. Instances that belong\n",
    "to neither class get ignored.\n",
    "'''\n",
    "\n",
    "all_instances = set([x for x in metadict.keys()])\n",
    "\n",
    "# The first stage is to find positive instances.\n",
    "\n",
    "all_positives = set()\n",
    "\n",
    "for key, value in metadict.items():\n",
    "    if value[category2sorton] == positive_class:\n",
    "        all_positives.add(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the author's code distinguishes the red triangles from the grey dots in figure one. If poem metadata has the value 'rev' for the 'reviewed' property then it is labeled as a positive. The next cell does the same for all negatives and removes any items that were added because of the canon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_negatives = all_instances - all_positives\n",
    "iterator = list(all_negatives)\n",
    "for item in iterator:\n",
    "    if metadict[item]['reviewed'] == 'addedbecausecanon':\n",
    "        all_negatives.remove(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negative labels are assigned to all instances that are not in the set of positive instances. There is additional code that filters out anything with 'addedbecausecannon' set for the 'reviewed' property, but this code should never execute. This is a vestige of testing with the canon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360\n"
     ]
    }
   ],
   "source": [
    "if sizecap > 0 and len(all_positives) > sizecap:\n",
    "    positives = random.sample(all_positives, sizecap)\n",
    "else:\n",
    "    positives = list(all_positives)\n",
    "    print(len(all_positives))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we see `360` being printed by the code above we know he is not sampling from the positives list. Ted's comment above, seems to indicate the sizecap is 350, but if you look at the code in the first code cell, the sizecap is *actually* 360...which just so happens to be the number of positive labeled instances. Funny. Looks like Ted updated his sizecap, but didn't update his comment.\n",
    "\n",
    "How was the sizecap determined? Performance and scalability? Perhaps some of the test sets were much larger than 360 instances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If there's a sizecap we also want to ensure classes have\n",
    "# matching sizes and roughly equal distributions over time.\n",
    "\n",
    "numpositives = len(all_positives)\n",
    "\n",
    "if sizecap > 0 and len(all_negatives) > numpositives:\n",
    "    if not 'date' in category2sorton:\n",
    "        available_negatives = list(all_negatives)\n",
    "        negatives = list()\n",
    "\n",
    "        for anid in positives:\n",
    "            date = metadict[anid]['pubdate']\n",
    "\n",
    "            available_negatives = sort_by_proximity(available_negatives, metadict, date)\n",
    "            selected_id = available_negatives.pop(0)\n",
    "            negatives.append(selected_id)\n",
    "\n",
    "    else:\n",
    "        # if we're dividing classes by date, we obvs don't want to\n",
    "        # ensure equal distributions over time.\n",
    "\n",
    "        negatives = random.sample(all_negatives, sizecap)\n",
    "\n",
    "else:\n",
    "    negatives = list(all_negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So most of the code in the cell above does not execute because we are not sampling from the positives distribution. \n",
    "\n",
    "This code cell makes an un-excuted reference to a function `sort_by_proximity()` which we are not including because it is not part of the execution path we are documenting in this notebook. This has provoked a conversation about if we should remove the code from the conditional blocks from the notebook. Yet, keeping them maintains the residue of Ted's development and thinking about what he needs to do with the data and in the analysis. \n",
    "\n",
    "These issues point to properties of code that make it difficult to review or critique, that is, we are in this case, reviewing a single execution path of the code, not the code itself. So we are dealing with a *code-criticism conundrum*: What is the required or adequate breadth and depth of the critique? The decision to include or not include `sort_by_proximity()` is a breadth issue. How broad should we be in including code that does not execute? Note, we are including code from a conditional block that doesn't execute, but are not going out the additional step to include non-executed function defined elsewhere in the code. The decision to include or not include code from the standard library, code not written by Ted, is a depth issue. In the code cell above, there are many functions we are *stepping over*, like `len`, `list`, `append`, `pop`, `random.sample`, etc., because they are black-boxed. There is no need to critique or test or inspect those functions because they have been tested and evaluated  thoroughly outside of the scope of Ted's project [We need a strong REF here].\n",
    "\n",
    "Full reflexivity here would also mean that we note that the 'rules of the game' for code criticism aren't quite clear yet and therefore we are possibly feeling our way through an emerging methodological standard of practice for code criticism. As we see vestiges of Ted evolution in thinking in his code, this notebook is capturing the evolution of our thinking about DEFACTORING as a practice. \n",
    "\n",
    "REF: Hiller and Joris about the tension between code's textual and processual dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "We have 360 positive, and\n",
      "360 negative instances.\n"
     ]
    }
   ],
   "source": [
    "# Now we have two lists of ids.\n",
    "\n",
    "IDsToUse = set()\n",
    "classdictionary = dict()\n",
    "\n",
    "print()\n",
    "print(\"We have \" + str(len(positives)) + \" positive, and\")\n",
    "print(str(len(negatives)) + \" negative instances.\")\n",
    "\n",
    "for anid in positives:\n",
    "    IDsToUse.add(anid)\n",
    "    classdictionary[anid] = 1\n",
    "\n",
    "for anid in negatives:\n",
    "    IDsToUse.add(anid)\n",
    "    classdictionary[anid] = 0\n",
    "\n",
    "for key, value in metadict.items():\n",
    "    if value['reviewed'] == 'addedbecausecanon':\n",
    "        IDsToUse.add(key)\n",
    "        classdictionary[key] = 0\n",
    "# We add the canon supplement, but don't train on it.\n",
    "\n",
    "### DEFACTORING FUNCTION RETERN\n",
    "### return IDsToUse, classdictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Words into Features\n",
    "\n",
    "\n",
    "Now we return to the execution of the `create_model` function from `parallel_crossvalidate.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### DEFACTORING FUNCTION DEFINITIONS\n",
    "### We need to define the infer_date function\n",
    "\n",
    "def infer_date(metadictentry, datetype):\n",
    "    if datetype == 'pubdate':\n",
    "        return metadictentry[datetype]\n",
    "    elif datetype == 'firstpub':\n",
    "        firstpub = metadictentry['firstpub']\n",
    "        if firstpub > 1700 and firstpub < 1950:\n",
    "            return firstpub\n",
    "        else:\n",
    "            return metadictentry['pubdate']\n",
    "    else:\n",
    "        sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make a vocabulary list and a volsize dict\n",
    "wordcounts = Counter()\n",
    "\n",
    "volspresent = list()\n",
    "orderedIDs = list()\n",
    "\n",
    "positivecounts = dict()\n",
    "negativecounts = dict()\n",
    "\n",
    "for volid, volpath in zip(volumeIDs, volumepaths):\n",
    "    if volid not in IDsToUse:\n",
    "        continue\n",
    "    else:\n",
    "        volspresent.append((volid, volpath))\n",
    "        orderedIDs.append(volid)\n",
    "\n",
    "    date = infer_date(metadict[volid], datetype)\n",
    "    if date < pastthreshold or date > futurethreshold:\n",
    "        continue\n",
    "    else:\n",
    "        with open(volpath, encoding = 'utf-8') as f:\n",
    "            for line in f:\n",
    "                fields = line.strip().split('\\t')\n",
    "                if len(fields) > 2 or len(fields) < 2:\n",
    "                    # print(line)\n",
    "                    continue\n",
    "                word = fields[0]\n",
    "                if len(word) > 0 and word[0].isalpha():\n",
    "                    count = int(fields[1])\n",
    "                    wordcounts[word] += 1\n",
    "                    # for initial feature selection we use the number of\n",
    "                    # *documents* that contain a given word,\n",
    "                    # so it's just +=1.\n",
    "\n",
    "vocablist = [x[0] for x in wordcounts.most_common(numfeatures)]\n",
    "\n",
    "# vocablist = binormal_select(vocablist, positivecounts, negativecounts, totalposvols, totalnegvols, 3000)\n",
    "# Feature selection is deprecated. There are cool things\n",
    "# we could do with feature selection,\n",
    "# but they'd improve accuracy by 1% at the cost of complicating our explanatory task.\n",
    "# The tradeoff isn't worth it. Explanation is more important.\n",
    "# So we just take the most common words (by number of documents containing them)\n",
    "# in the whole corpus. Technically, I suppose, we could crossvalidate that as well,\n",
    "# but *eyeroll*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200\n",
      "['of', 'as', 'at', 'and', 'is', 'i', 'in', 'a', 'the', 'for', 'not', 'with', 'but', 'by', 'all', 'that', 'to', 'on', 'his', 'who', 'they', 'from', 'be', 'one', 'it', 'when', 'or', 'this', 'now', 'was', 'he', 'no', 'like', 'their', 'we', 'them', 'so', 'have', 'her', 'its', 'there', 'will', 'then', 'are', 'see', 'where', 'heart', 'were', 'him', 'an']\n"
     ]
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "### What is in the vocablist variable?\n",
    "print(len(vocablist))\n",
    "print(vocablist[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an important section of code because it contains the code for the selection of the 3200 word-features, or \"variables\" as he calls them on page 35 (the methodology appendix), used in the logistic regression. Most notable is the code that has been commented out at the very end of the block. The heuristic for feature selection was simply to select the 3200 most common words, a simple and easy to explain technique (also easy to implement with Python's Counter collection). The comment discusses an alternative feature selection technique using binormal selection which he has implemented in the function `binormal_selection`. Because this code is commented out, we do not explore it in-depth here. More interesting is the rationale about *why* it has been commented out:\n",
    "\n",
    "> There are cool things we could do with feature selection, but they'd improve accuracy by 1% at the cost of complicating our explanatory task.\n",
    "\n",
    "What we are seeing in this comment is a road not taken, which normally the DEFACTORING method would ignore, but the code, the comment, and the implications are crucially important. This reveals much about about Ted's reasoning on the effort and energy he wishes to invest in *explaining* method. There is judgment about the expertise of his audience. This points to a crucial problem at the intersection of humanities and computer science where the dissemination of description of statistical methods are inhibiting or hindering the full realization of the epistemlogical richness of computation as a method of inquiry. Ted is holding himself back, a form of self-censorship, because of a perceived Audience That cannot understand a binormal feature selection technique without a significant amount of work to explain its epistomolgical implications. \n",
    "\n",
    "We think this snippet of code is significant and justifies our method because it is only through close code review were we able to uncover these traces of Ted's thinking and experimentation. We can see epistomolary roads not taken because of strenuous dialectical relationship between the computational potentiality and the disciplinary acceptability. \n",
    "\n",
    "There is more for us to say and think about this vinette. But we need to move on.\n",
    "\n",
    "Aside: we couldn't help ourselves and did a little digging into the `binormal_selection` function. There is an oblique reference to \"see forman\" which we think refers to George Forman who has written several articles on feature selection in texts. We should confirm with Ted.\n",
    "- http://dl.acm.org/citation.cfm?id=944974\n",
    "- http://link.springer.com/chapter/10.1007%2F3-540-45681-3_13\n",
    "- We think this is the paper: http://www.hpl.hp.com/techreports/2007/HPL-2007-32R1.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "donttrainon = list()\n",
    "\n",
    "# Here we create a list of volumed IDs not to be used for training.\n",
    "# For instance, we have supplemented the dataset with volumes that\n",
    "# are in the Norton but that did not actually occur in random\n",
    "# sampling. We want to make predictions for these, but never use\n",
    "# them for training.\n",
    "\n",
    "for idx1, anid in enumerate(orderedIDs):\n",
    "    reviewedstatus = metadict[anid]['reviewed']\n",
    "    date = infer_date(metadict[anid], datetype)\n",
    "    if reviewedstatus == 'addedbecausecanon':\n",
    "        donttrainon.append(idx1)\n",
    "    elif date < pastthreshold or date > futurethreshold:\n",
    "        donttrainon.append(idx1)\n",
    "\n",
    "authormatches = [list(donttrainon) for x in range(len(orderedIDs))]\n",
    "# For every index in authormatches, identify a set of indexes that have\n",
    "# the same author. Obvs, there will always be at least one.\n",
    "\n",
    "# Since we are going to use these indexes to exclude rows, we also add\n",
    "# all the ids in donttrainon to every volume\n",
    "\n",
    "for idx1, anid in enumerate(orderedIDs):\n",
    "    thisauthor = metadict[anid]['author']\n",
    "    for idx2, anotherid in enumerate(orderedIDs):\n",
    "        otherauthor = metadict[anotherid]['author']\n",
    "        if thisauthor == otherauthor and not idx2 in authormatches[idx1]:\n",
    "            authormatches[idx1].append(idx2)\n",
    "\n",
    "for alist in authormatches:\n",
    "    alist.sort(reverse = True)\n",
    "\n",
    "# I am reversing the order of indexes so that I can delete them from\n",
    "# back to front, without changing indexes yet to be deleted.\n",
    "# This will become important in the modelingprocess module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "### What volumes are we not training on?\n",
    "print(donttrainon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is blank, what this tells us is all volumes are being used in the training. Lets inspect a few more variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720\n",
      "720\n",
      "[582, 499, 420, 368, 200, 7]\n",
      "[582, 499, 420, 368, 200, 7]\n",
      "uc2.ark+=13960=t3fx7436h\n",
      "uc1.b4104728\n",
      "{'birthdate': 1809, 'nation': 'uk', 'title': 'In memoriam', 'obscure': 'known', 'pubdate': 1861, 'firstpub': 1849, 'pubname': 'ER', 'author': 'Tennyson, Alfred Tennyson,', 'canonicity': 'Norton, in-set', 'gender': 'm', 'reviewed': 'rev'}\n",
      "{'birthdate': 1809, 'nation': 'uk', 'title': 'Idyls of the King', 'obscure': 'known', 'pubdate': 1859, 'firstpub': 1859, 'pubname': 'WR', 'author': 'Tennyson, Alfred Tennyson,', 'canonicity': 'Norton, in-set', 'gender': 'm', 'reviewed': 'rev'}\n"
     ]
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "print(len(orderedIDs))\n",
    "print(len(authormatches))\n",
    "print(authormatches[7])\n",
    "print(authormatches[582])\n",
    "print(orderedIDs[582])\n",
    "print(orderedIDs[499])\n",
    "print(metadict['uc2.ark+=13960=t3fx7436h'])\n",
    "print(metadict['uc1.b4104728'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all rote data preparation. The authors compile a list that shouldn't be used as training data if the date is incorrect or if the books are part of the \"addedbecauseofcanon\" class (representing a well known author). From this list they compile a list of authors who have been reviewed and then removing their other works from the list. \n",
    "\n",
    "The inspection code above was necessary for us to understanding the complicated data structure they created.\n",
    "\n",
    "The list `authormatches` is a list of all the poems and each is a list of other poems by the same author. Essentially this data structure is describing the relations of each poem to other poems, that relation being \"other poems by the same author\". It took us a bit of work to figure out because all of this is obscured by the fact the relations are expressed by list indexes. We are working with abstractions which we assume are important for the `modelingprocess` which is invoked below.\n",
    "\n",
    "The purpose of this is because\n",
    "\n",
    "> ...we did exclude who were already in our reviewed sample for a given genre. (page 34)\n",
    "\n",
    "What we seen in the code is the necessary steps to be able to exclude author already in the reviewed sample, but we don't yet see any reference to genre. Either this is implicit (because all these data are the same genre, poetry) or hasn't been addressed as of yet in the code.\n",
    "\n",
    "Aside: we should try an annotate the author's narrative in the article to point to the specific moments in the code where he is describing (in english) what is happening in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### DEFACTORING DEFINITIONS\n",
    "\n",
    "usedate = False\n",
    "# Leave this flag false unless you plan major\n",
    "# surgery to reactivate the currently-deprecated\n",
    "# option to use \"date\" as a predictive feature.\n",
    "\n",
    "def get_features(wordcounts, wordlist):\n",
    "    numwords = len(wordlist)\n",
    "    wordvec = np.zeros(numwords)\n",
    "    for idx, word in enumerate(wordlist):\n",
    "        if word in wordcounts:\n",
    "            wordvec[idx] = wordcounts[word]\n",
    "\n",
    "    return wordvec\n",
    "\n",
    "# In an earlier version of this script, we sometimes used\n",
    "# \"publication date\" as a feature, to see what would happen.\n",
    "# In the current version, we don't. Some of the functions\n",
    "# and features remain, but they are deprecated. E.g.:\n",
    "\n",
    "def get_features_with_date(wordcounts, wordlist, date, totalcount):\n",
    "    numwords = len(wordlist)\n",
    "    wordvec = np.zeros(numwords + 1)\n",
    "    for idx, word in enumerate(wordlist):\n",
    "        if word in wordcounts:\n",
    "            wordvec[idx] = wordcounts[word]\n",
    "\n",
    "    wordvec = wordvec / (totalcount + 0.0001)\n",
    "    wordvec[numwords] = date\n",
    "    return wordvec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `get_features` simply takes the wordcounts from the parsed poem and filters out any words that are not part of `wordlist` which is the selected features described above.\n",
    "\n",
    "Note, we are `including get_features_with_date` even though it is not called beacuse it is part of Underwood and Seller's comment narrative above. While it is present in the code below it is never called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "volsizes = dict()\n",
    "voldata = list()\n",
    "classvector = list()\n",
    "\n",
    "for volid, volpath in volspresent:\n",
    "\n",
    "    with open(volpath, encoding = 'utf-8') as f:\n",
    "        voldict = dict()\n",
    "        totalcount = 0\n",
    "        for line in f:\n",
    "            fields = line.strip().split('\\t')\n",
    "            if len(fields) > 2 or len(fields) < 2:\n",
    "                continue\n",
    "\n",
    "            word = fields[0]\n",
    "            count = int(fields[1])\n",
    "            voldict[word] = count\n",
    "            totalcount += count\n",
    "\n",
    "    date = infer_date(metadict[volid], datetype)\n",
    "    date = date - 1700\n",
    "    if date < 0:\n",
    "        date = 0\n",
    "\n",
    "    if usedate:\n",
    "        features = get_features_with_date(voldict, vocablist, date, totalcount)\n",
    "        voldata.append(features)\n",
    "    else:\n",
    "        features = get_features(voldict, vocablist)\n",
    "        voldata.append(features / (totalcount + 0.001))\n",
    "\n",
    "\n",
    "    volsizes[volid] = totalcount\n",
    "    classflag = classdictionary[volid]\n",
    "    classvector.append(classflag)\n",
    "\n",
    "data = pd.DataFrame(voldata)\n",
    "\n",
    "sextuplets = list()\n",
    "for i, volid in enumerate(orderedIDs):\n",
    "    listtoexclude = authormatches[i]\n",
    "    asixtuple = data, classvector, listtoexclude, i, usedate, regularization\n",
    "    sextuplets.append(asixtuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code opens up each of the files in the poems/ directory, parses the file, does some data cleaning with respect to the structure of those files and with dates of poems older than 1700 (this is probably vestage code because we are not using date as a predictive feature. The important bit is the call to the `get_features` function which throws out the word features that are not part of the list of selected word features as determined by the `most_common()` call above and stored in the `vocablist` variable. \n",
    "\n",
    "\n",
    "We are curious about this code:\n",
    "\n",
    "```\n",
    "voldata.append(features / (totalcount + 0.001))\n",
    "```\n",
    "\n",
    "We understand he is normalizing the data to make volumes of differing sizes comparable. Turning absolute frequencies into relative frequencies. However, we don't know why he is adding 0.001 to the total count. Is this to prevent a potential divide by zero? NOTE: we think that the 0.001 is added so the logistic regression doesn't result in infinite at zero when running the cost function.\n",
    "\n",
    "Sextuplets is a list of tuples with six values. This is the datastructure which will be passed to the modeling process. Each row of `sextuplets` contains all of the necessary data structures to model each poem. because we are generating a new model for each poem, we need to pass a couple parameters to the modeling process (such as which poems to ignore because they are by the same author, `listtoexclude`, when modeling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A quick investigation of the six key datastructures\n",
    "\n",
    "The modeling function described in the next section of the notebook relies upon six data structures from \n",
    "\n",
    "- data: a document features matrix. Word features are the columns and volumes are the rows. 720 x 3200\n",
    "- classvector: the classification of documents as either 'reviewed' (1) or 'random' (0).\n",
    "- listtoexclude: the list of poems to ignore because they are the same author\n",
    "- i: the index of the volume\n",
    "- usedate: a flag indicating if date is a feature. It is false in the default execution path.\n",
    "- regularization: a parameter for the scikit-learn LogisticRegression function. hardcoded parameter from the beginning of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0         1         2         3         4         5         6     \\\n",
      "0  0.017014  0.003417  0.002581  0.042972  0.005708  0.005708  0.013961   \n",
      "1  0.020229  0.003349  0.003084  0.031172  0.006467  0.008257  0.013928   \n",
      "2  0.029097  0.004157  0.006532  0.029691  0.010095  0.007126  0.013658   \n",
      "3  0.005976  0.003351  0.002401  0.035708  0.002776  0.006001  0.008302   \n",
      "4  0.018415  0.006372  0.003620  0.032099  0.001786  0.009678  0.011995   \n",
      "\n",
      "       7         8         9       ...         3190    3191      3192  \\\n",
      "0  0.009198  0.052534  0.006217    ...     0.000000  0.0000  0.000036   \n",
      "1  0.009484  0.051733  0.004510    ...     0.000099  0.0000  0.000066   \n",
      "2  0.013658  0.049881  0.007126    ...     0.000000  0.0000  0.000000   \n",
      "3  0.017079  0.042335  0.006626    ...     0.000000  0.0002  0.000025   \n",
      "4  0.012671  0.037674  0.005551    ...     0.000000  0.0000  0.000024   \n",
      "\n",
      "       3193      3194      3195      3196      3197      3198      3199  \n",
      "0  0.000036  0.000000  0.000000  0.000036  0.000036  0.000000  0.000036  \n",
      "1  0.000000  0.000033  0.000000  0.000033  0.000033  0.000033  0.000000  \n",
      "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "3  0.000025  0.000000  0.000000  0.000000  0.000000  0.000000  0.000050  \n",
      "4  0.000024  0.000048  0.000024  0.000024  0.000000  0.000024  0.000000  \n",
      "\n",
      "[5 rows x 3200 columns]\n",
      "[1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0]\n",
      "[582, 499, 420, 368, 200, 7]\n",
      "582\n",
      "False\n",
      "7e-05\n"
     ]
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "print(sextuplets[582][0].head()) #data\n",
    "print(sextuplets[582][1]) #classvector \n",
    "print(sextuplets[582][2]) #listtoexclude\n",
    "print(sextuplets[582][3]) #i\n",
    "print(sextuplets[582][4]) #usedate\n",
    "print(sextuplets[582][5]) #regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Predictive Models\n",
    "\n",
    "We are now about to step down into the very heart of the project, the modeling of each individual text.\n",
    "\n",
    "To do this we need to bring the function, model_one_volume into the global namespace of the notebook. This means we need to dig into a new module, modelingprocess and extract a few functions from it. The main modeling functin, `model_one_volume` depends upon two helper functions, `normalizearray()` and `sliceframe.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### DEFACTORING DEFINITION\n",
    "\n",
    "# modelingprocess.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def sliceframe(dataframe, yvals, excludedrows, testrow):\n",
    "    numrows = len(dataframe)\n",
    "    newyvals = list(yvals)\n",
    "    for i in excludedrows:\n",
    "        del newyvals[i]\n",
    "        # NB: This only works if we assume that excluded rows\n",
    "        # has already been sorted in descending order !!!!!!!\n",
    "        # otherwise indexes will slide around as you delete\n",
    "\n",
    "    trainingset = dataframe.drop(dataframe.index[excludedrows])\n",
    "\n",
    "    newyvals = np.array(newyvals)\n",
    "    testset = dataframe.iloc[testrow]\n",
    "\n",
    "    return trainingset, newyvals, testset\n",
    "\n",
    "def normalizearray(featurearray, usedate):\n",
    "    '''Normalizes an array by centering on means and\n",
    "    scaling by standard deviations. Also returns the\n",
    "    means and standard deviations for features.\n",
    "    '''\n",
    "\n",
    "    numinstances, numfeatures = featurearray.shape\n",
    "    means = list()\n",
    "    stdevs = list()\n",
    "    lastcolumn = numfeatures - 1\n",
    "    for featureidx in range(numfeatures):\n",
    "\n",
    "        thiscolumn = featurearray.iloc[ : , featureidx]\n",
    "        thismean = np.mean(thiscolumn)\n",
    "\n",
    "        thisstdev = np.std(thiscolumn)\n",
    "\n",
    "        if (not usedate) or featureidx != lastcolumn:\n",
    "            # If we're using date we don't normalize the last column.\n",
    "            means.append(thismean)\n",
    "            stdevs.append(thisstdev)\n",
    "            featurearray.iloc[ : , featureidx] = (thiscolumn - thismean) / thisstdev\n",
    "        else:\n",
    "            print('FLAG')\n",
    "            means.append(thismean)\n",
    "            thisstdev = 0.1\n",
    "            stdevs.append(thisstdev)\n",
    "            featurearray.iloc[ : , featureidx] = (thiscolumn - thismean) / thisstdev\n",
    "            # We set a small stdev for date.\n",
    "\n",
    "    return featurearray, means, stdevs\n",
    "\n",
    "def model_one_volume(data5tuple):\n",
    "    data, classvector, listtoexclude, i, usedate, regularization = data5tuple\n",
    "    trainingset, yvals, testset = sliceframe(data, classvector, listtoexclude, i)\n",
    "    newmodel = LogisticRegression(C = regularization)\n",
    "    trainingset, means, stdevs = normalizearray(trainingset, usedate)\n",
    "    newmodel.fit(trainingset, yvals)\n",
    "\n",
    "    testset = (testset - means) / stdevs\n",
    "    prediction = newmodel.predict_proba(testset.reshape(1, -1))[0][1] #DEFACTOR FIX\n",
    "    if i % 50 == 0:\n",
    "        print(i)\n",
    "    # print(str(i) + \"  -  \" + str(len(listtoexclude)))\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "What is happening here:\n",
    "\n",
    "- iterates over every volume\n",
    "- removes that volume and any other volumes by the same author from the training set\n",
    "- normalizes the training set by computing the z-score for each feature/feature set\n",
    "- fits the model on the z-scores\n",
    "- normalizes the test data by computing the z-score\n",
    "- using the fitted model, predicts the probability of the test data that it is either reviewed or random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### sliceframe()\n",
    "\n",
    "This function prepares the data for training a model for a specific volume. Given all of the data, all of the classifications, a list of volumes to exclude, and the index of the specific volume to model, this function removes the specific volume and the volumes by the same authors (indicated by `excluderows`) from the training data set (because we are holding out one volume to be classified by the logistic regression). This function then returns a training set, a list of classifications, and the held-out volume to-be-classified once the model has been trained. \n",
    "\n",
    "\n",
    "#### normalizearray()\n",
    "\n",
    "This function computes the z-score for each value in the training set. That is, it loops over each each column in the data structure, subtracts the column mean from each value, and then divides that by the standard deviation. \n",
    "\n",
    "\n",
    "Question:\n",
    "- Why is he normalizing the data by computing z-scores?\n",
    "\n",
    "> In training the model we “normalize” word frequencies by the standard deviation for each word (across the whole dataset). So when we use the model to illuminate specific passages, we also divide coefficients by the standard deviation. This tells us, roughly, how much a single occurrence of a given word would affect the model’s prediction, which is what we’re trying to dramatize when we quote a passage. (page 35)\n",
    "\n",
    "\n",
    "The answer appears to have something to do with using the model's coefficients in the interpretation of the effect of individual words.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "\n",
    "Our investigation into the mechanisms of logistic regression were driven by the question \"what is the regularization value?\"\n",
    "\n",
    "This lead us spend some time watching [Andrew Ng explain classification using Logistic Regression](https://class.coursera.org/ml-005/lecture) on his Coursera course.\n",
    "\n",
    "At a very high level, logistic regression is a machine learning algorithm for performing classification. Logistic regression works by estimating the parameters of a function, the *hypothesis representation*, that divides a multidimensional space into two parts (note, in this case we are talking about binomial or binary logistic regression, which classifies things into one of two bins). The hypothesis representation describes a line that winds its way through the space creating what is called the *decision boundary.* Every data point that lands on one side the boundary gets one label and every data point on the other side of the boundary gets the other label. Similar to linear regression, the goal is to find the best hypothesis representation, that is, the function that best draws a line that divides the space given your already classified data. Once you have a good hypothesis representation, and appropriately *fit* model, you can begin to classify *new* data.\n",
    "\n",
    "The key to logistic regression is estimating the parameters of the hypothesis representation. We can derive the parameters by using the *features* of existing data combined with their already known labels; this is called *training data*. The modeling process, the function call to `newmodel.fit(trainingset, yvals)` in Ted's code above, uses training data–the matrix of poem features in the `data` variable and known labels ('reviewed' or 'random') in the `classvector` variable–to \"learn\" the parameters through a process called *gradient descent* (note: scikit-learn uses a different process called [*coordinate descent*](http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression) which is exceptionally complex). How gradient descent or the more advanced optimization functions like coordinate descent work are well beyond the scope of the discussion (and our explanatory power) so we will just nod and gesture towards the mathematical magic performed by the `newmodel.fit(trainingset, yvals)` function call.\n",
    "\n",
    "##### Overfitting\n",
    "\n",
    "One of the problems when fitting a logistic regression model is a tendency towards *overfitting*. Crudely this means the model, the function and set of parameters, you estimated have tailored themselves so that they are overly optimized to the data you have. As such, the model becomes less useful for *prediction* or classifying any new data you might encounter. In Ted's case, he is fitting a model based upon all of the poems and their classifications *except one (or a few by the same author)* which he then uses to predict if the *held out* poem was 'reviewed' or 'random.' If he *overfits* the model, it will to a terrible job guessing the status of the held out poem. \n",
    "\n",
    "Regularization is a technique for logistic regression (and other machine learning algorithms) which helps smooth out the tendency toward overfitting with some more mathematical gymnastics that we don't quite have the power to explain with word, but we can explain visually. The diagram below shows how regularization can help with the fitness of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Regularization](notebook_resources/regression_figures.png)\n",
    "\n",
    "*On the left side is a linear regression which doesn't quite fit the data. In the middle is an overfit logistic regression. On right side is a regularized logistic regression.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the diagrams show, the regularized logistic expression (the right side) does have a bit of error, there are pink and blue dots on the wrong sides of the decision boundary, but as more data get added it will generally be more right than the overfitted model as represented by the middle diagram (the squiggly decision boundary). \n",
    "\n",
    "The LinearRegression function of the scikit-learn library allows users to specify a regularization parameter when instantiating a model (`newmodel = LogisticRegression(C = regularization)` in Underwood and Seller's code). They have set the `regularization` parameter to 0.00007. Our question is *why?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the Regression\n",
    "\n",
    "The code cell below is the scaffolding code that runs the modeling function. This code cell takes a long time to execute, it is training a new model for each book. It uses Python's built in parallel processing modules (`Pool(processes = 4)`) to speed up the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning multiprocessing.\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "450\n",
      "500\n",
      "400\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "Multiprocessing concluded.\n"
     ]
    }
   ],
   "source": [
    "# Now do leave-one-out predictions.\n",
    "print('Beginning multiprocessing.')\n",
    "\n",
    "pool = Pool(processes = 4)\n",
    "res = pool.map_async(model_one_volume, sextuplets)\n",
    "\n",
    "# After all files are processed, write metadata, errorlog, and counts of phrases.\n",
    "res.wait()\n",
    "resultlist = res.get()\n",
    "\n",
    "assert len(resultlist) == len(orderedIDs)\n",
    "\n",
    "logisticpredictions = dict()\n",
    "for i, volid in enumerate(orderedIDs):\n",
    "    logisticpredictions[volid] = resultlist[i]\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "print('Multiprocessing concluded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: some commentary here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "truepositives = 0\n",
    "truenegatives = 0\n",
    "falsepositives = 0\n",
    "falsenegatives = 0\n",
    "allvolumes = list()\n",
    "\n",
    "with open(outputpath, mode = 'w', encoding = 'utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    header = ['volid', 'reviewed', 'obscure', 'pubdate', 'birthdate', 'gender', 'nation', 'allwords', 'logistic', 'author', 'title', 'pubname', 'actually', 'realclass']\n",
    "    writer.writerow(header)\n",
    "    for volid in IDsToUse:\n",
    "        metadata = metadict[volid]\n",
    "        reviewed = metadata['reviewed']\n",
    "        obscure = metadata['obscure']\n",
    "        pubdate = infer_date(metadata, datetype)\n",
    "        birthdate = metadata['birthdate']\n",
    "        gender = metadata['gender']\n",
    "        nation = metadata['nation']\n",
    "        author = metadata['author']\n",
    "        title = metadata['title']\n",
    "        canonicity = metadata['canonicity']\n",
    "        pubname = metadata['pubname']\n",
    "        allwords = volsizes[volid]\n",
    "        logistic = logisticpredictions[volid]\n",
    "        realclass = classdictionary[volid]\n",
    "        outrow = [volid, reviewed, obscure, pubdate, birthdate, gender, nation, allwords, logistic, author, title, pubname, canonicity, realclass]\n",
    "        writer.writerow(outrow)\n",
    "        allvolumes.append(outrow)\n",
    "\n",
    "        if logistic > 0.5 and classdictionary[volid] > 0.5:\n",
    "            truepositives += 1\n",
    "        elif logistic <= 0.5 and classdictionary[volid] < 0.5:\n",
    "            truenegatives += 1\n",
    "        elif logistic <= 0.5 and classdictionary[volid] > 0.5:\n",
    "            falsenegatives += 1\n",
    "        elif logistic > 0.5 and classdictionary[volid] < 0.5:\n",
    "            falsepositives += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Coefficients\n",
    "\n",
    "\n",
    "The code below represents a shift in the focal object of the analysis. In the previous section 720 distinct logistic regressions were trained in order to predict the classification of a single, held-out poem. This is the data that was used to produce the main figure, *Figure 1. Predicted probabilities that volumes come from the reviewed set.* \n",
    "\n",
    "The code below generates a single logistic regression model, trained n *all of the data* with nothing held-out (at least when using the 'full' execution path). The reason no data are held out is because this model is not being used for prediction purposes. Instead, the properties of this model are interrogated directly to better understand how individual features, words, had an effect upon the prediction. This is the analysis that allowed them to label individuals worlds as either red or blue based upon their effect when quoting individual passages.\n",
    "\n",
    "\n",
    "This isn't using computational modeling to *predict* a phenomena, it is using the model to *explore* and *explain* patterns and features of the phenomena. This code describes a process where by the model is being deployed for *exploratory data analysis*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "donttrainon.sort(reverse = True)\n",
    "trainingset, yvals, testset = sliceframe(data, classvector, donttrainon, 0)\n",
    "newmodel = LogisticRegression(C = regularization)\n",
    "trainingset, means, stdevs = normalizearray(trainingset, usedate)\n",
    "newmodel.fit(trainingset, yvals)\n",
    "\n",
    "coefficients = newmodel.coef_[0] * 100\n",
    "\n",
    "coefficientuples = list(zip(coefficients, (coefficients / np.array(stdevs)), vocablist + ['pub.date']))\n",
    "coefficientuples.sort()\n",
    "if verbose:\n",
    "    for coefficient, normalizedcoef, word in coefficientuples:\n",
    "        print(word + \" :  \" + str(coefficient))\n",
    "\n",
    "print()\n",
    "accuracy = (truepositives + truenegatives) / len(IDsToUse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to above, we don't quite understand is why Ted is normalizing the coefficients. What we don't know is what set of values, the coefficients or the normalized coefficients he is using when he highlights words as either red or blue in the narrative.\n",
    "\n",
    "> In training the model we “normalize” word frequencies by the standard deviation for each word (across the whole dataset). So when we use the model to illuminate specific passages, we also divide coefficients by the standard deviation. This tells us, roughly, how much a single occurrence of a given word would affect the model’s prediction, which is what we’re trying to dramatize when we quote a passage. (page 35)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3200"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "### how many coeficients are there?\n",
    "len(newmodel.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Output\n",
    "\n",
    "the code below generates the `mainmodelcoefficients.csv` which contains the word, its coefficient and its normalized coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coefficientpath = outputpath.replace('.csv', '.coefs.csv')\n",
    "with open(coefficientpath, mode = 'w', encoding = 'utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for triple in coefficientuples:\n",
    "        coef, normalizedcoef, word = triple\n",
    "        writer.writerow([word, coef, normalizedcoef])\n",
    "\n",
    "### DEFACTORING FUNCTION RETURN\n",
    "### return accuracy, allvolumes, coefficientuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### DEFACTORING NAMESPACE\n",
    "rawaccuracy = accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Results\n",
    "\n",
    "The final function of the analysis is to test the accuracy of the model(s). The code below generates a best fit line, using [numpy.polyfit](http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.polyfit.html), for all of the results. He then calculates which are predicted-as-reviewed, which is those that fall above this dividing line\n",
    "\n",
    "The accuracy of the dividing line is computed by the total number of predicted-as-reviewed that were actually reviewed divided by the total.\n",
    "\n",
    "Note, this is the accuracy of the dividing line, not the accuracy of the model(s) to predict the reviewed stats of the poems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD7CAYAAABkO19ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngoJChuP/A6iIoqYw3GZCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb38c33a668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### DEFACTORING FUNCTION\n",
    "### def diachronic_tilt(allvolumes, modeltype, datelimits):\n",
    "\n",
    "### DEFACTORING FUNCTION PARAMETERS\n",
    "modeltype = 'linear'\n",
    "datelimits = []\n",
    "''' Takes a set of predictions produced by a model that knows nothing about date,\n",
    "and divides it along a line with a diachronic tilt. We need to do this in a way\n",
    "that doesn't violate crossvalidation. I.e., we shouldn't \"know\" anything\n",
    "that the model didn't know. We tried a couple of different ways to do this, but\n",
    "the simplest and actually most reliable is to divide the whole dataset along a\n",
    "linear central trend line for the data!\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "listofrows = list()\n",
    "classvector = list()\n",
    "\n",
    "# DEPRECATED\n",
    "# if modeltype == 'logistic' and len(datelimits) == 2:\n",
    "#     # In this case we construct a subset of data to model on.\n",
    "#     tomodeldata = list()\n",
    "#     tomodelclasses = list()\n",
    "#     pastthreshold, futurethreshold = datelimits\n",
    "\n",
    "for volume in allvolumes:\n",
    "    date = volume[3]\n",
    "    logistic = volume[8]\n",
    "    realclass = volume[13]\n",
    "    listofrows.append([logistic, date])\n",
    "    classvector.append(realclass)\n",
    "\n",
    "    # DEPRECATED\n",
    "    # if modeltype == 'logistic' and len(datelimits) == 2:\n",
    "    #     if date >= pastthreshold and date <= futurethreshold:\n",
    "    #         tomodeldata.append([logistic, date])\n",
    "    #         tomodelclasses.append(realclass)\n",
    "\n",
    "y, x = [a for a in zip(*listofrows)]\n",
    "plt.axis([min(x) - 2, max(x) + 2, min(y) - 0.02, max(y) + 0.02])\n",
    "reviewedx = list()\n",
    "reviewedy = list()\n",
    "randomx = list()\n",
    "randomy = list()\n",
    "\n",
    "for idx, reviewcode in enumerate(classvector):\n",
    "    if reviewcode == 1:\n",
    "        reviewedx.append(x[idx])\n",
    "        reviewedy.append(y[idx])\n",
    "    else:\n",
    "        randomx.append(x[idx])\n",
    "        randomy.append(y[idx])\n",
    "\n",
    "plt.plot(reviewedx, reviewedy, 'ro')\n",
    "plt.plot(randomx, randomy, 'k+')\n",
    "\n",
    "if modeltype == 'logistic':\n",
    "    # all this is DEPRECATED\n",
    "    print(\"Hey, you're attempting to use the logistic-tilt option\")\n",
    "    print(\"that we deactivated. Go in and uncomment the code.\")\n",
    "\n",
    "    # if len(datelimits) == 2:\n",
    "    #     data = pd.DataFrame(tomodeldata)\n",
    "    #     responsevariable = tomodelclasses\n",
    "    # else:\n",
    "    #     data = pd.DataFrame(listofrows)\n",
    "    #     responsevariable = classvector\n",
    "\n",
    "    # newmodel = LogisticRegression(C = 100000)\n",
    "    # newmodel.fit(data, responsevariable)\n",
    "    # coefficients = newmodel.coef_[0]\n",
    "\n",
    "    # intercept = newmodel.intercept_[0] / (-coefficients[0])\n",
    "    # slope = coefficients[1] / (-coefficients[0])\n",
    "\n",
    "    # p = np.poly1d([slope, intercept])\n",
    "\n",
    "elif modeltype == 'linear':\n",
    "    # what we actually do\n",
    "\n",
    "    z = np.polyfit(x, y, 1)\n",
    "    p = np.poly1d(z)\n",
    "    slope = z[0]\n",
    "    intercept = z[1]\n",
    "\n",
    "plt.plot(x,p(x),\"b-\")\n",
    "plt.show(block = False)\n",
    "\n",
    "x = np.array(x, dtype='float64')\n",
    "y = np.array(y, dtype='float64')\n",
    "classvector = np.array(classvector)\n",
    "dividingline = intercept + (x * slope)\n",
    "predicted_as_reviewed = (y > dividingline)\n",
    "really_reviewed = (classvector == 1)\n",
    "\n",
    "accuracy = sum(predicted_as_reviewed == really_reviewed) / len(classvector)\n",
    "\n",
    "### DEFACTORING FUNCTION RETURN\n",
    "### return accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### DEFACTORING NAMESPACE\n",
    "tiltaccuracy = accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If we divide the dataset with a horizontal line at 0.5, accuracy is:  0.775\n",
      "Divided with a line fit to the data trend, it's  0.791666666667\n"
     ]
    }
   ],
   "source": [
    "### DEFACTORING FUNCTION CALL\n",
    "### tiltaccuracy = diachronic_tilt(allvolumes, 'linear', []) \n",
    "\n",
    "print('If we divide the dataset with a horizontal line at 0.5, accuracy is: ', str(rawaccuracy))\n",
    "\n",
    "print(\"Divided with a line fit to the data trend, it's \", str(tiltaccuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "This notebook contains all of the code necessary to run through a full execution of the computational analysis.\n",
    "\n",
    "\n",
    "Open questions for Underwood & Sellers:\n",
    "- why does he normalize the word frequences (using `normalizearray()`) before modeling?\n",
    "- Why does he normalize the coefficients ( divided by standard deviation)?\n",
    "\n",
    "Basically, we don't have a firm understanding of why he is normalizing, mainly because we don't understand the meaning or significance of these steps on the interpretibility of the data. These are probably quite normal practices when performing logistic regression with these kinds of data, but there is a taken for granted quality to these actions that we'd like to see described as part of the methodology (or at least citations to a best practice).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Full concluding remarks are in-progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography ##\n",
    "\n",
    "Frabetti, F., 2012. Have the Humanities Always Been Digital? In *Understanding Digital Humanities.* London; New York: Palgrave Macmillan, pp. 161–171.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
