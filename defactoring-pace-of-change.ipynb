{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defactoring Pace of Change\n",
    "\n",
    "\n",
    "TODO: introductory remarks about the defactoring process\n",
    "\n",
    "One of the byproducts of defactoring is a deconstruction of the structure of the code. (heh) Yet, the implementation of what we doing is completely different, at least conceptually, from deconstruction\n",
    "\n",
    "There is something processually different betwen deconstructing a text and deconstructing code. Where you would take a text and try to identify its meaningful sub-parts and their relations, with code, we are mirroring that process and putting the carefully deconstructed parts back together into one large linear narrative.\n",
    "\n",
    "Good code is already deconstructed (refactored) into modules and composable parts, we are in effect turning good code into sub-optimal code.\n",
    "\n",
    "\n",
    "NOtes on defactoring and writing computational narratives in the Notebook:\n",
    "\n",
    "- What do you do with helper functions? How should helper functions be interwoven into a linear computational narrative?\n",
    "- the global namespace is polluted as fuck\n",
    "- We actually infer 1 possible execution path in this notebook (analyzing *all* exectution paths would be humanly infeasible: what do we do with code that isn't part of that path, do we represent it or not?\n",
    "- What do we do with code that isn't executed as a result of the settings by Ted? We are including some of the non executed code (because ??) but not all of the non executed code. We don't have a very good rationale for this at the moment.\n",
    "- Observation: It would require a book-sized examination to do full justice to code and execution pathways of this article. There are multiple potential pathways, rivers and tributaries of data and computation. To fully realize DEFACTORING as a critical method is to explicate all possibilities...but to do this is infeasble (at least without computational assistance). What would it mean to computationally explore these alternatives? \n",
    "\n",
    "\n",
    "\n",
    "We have altered Ted Underwood's code, the letter is changed, but the spirit of the work is the same. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the data\n",
    "\n",
    "\n",
    "Before we dive into the code, we had a question about \"what do they mean by *volume*?\" in the Pace of Change article.\n",
    "\n",
    "The answer, to be confirmed, lives in the [Understanding Genre in a Collection of a Million Volumes](https://figshare.com/articles/Understanding_Genre_in_a_Collection_of_a_Million_Volumes_Interim_Report/1281251). Basically, the idea of \"volume\" comes from the Hathi Trust.\n",
    "\n",
    "\n",
    "\n",
    "Volume is equivalent to book. Volume is the Hathi Trust unit. \n",
    "\n",
    "> We worked with HathiTrust, which contains the aggregated collections of large public and university libraries; in the period we’re considering (1820-1919), that gave us a collection of roughly 758,400 books in English, of which about 53,200 include significant amounts of poetry. This doesn’t exhaustively cover print culture; it’s still a sample, with particular selection biases. (page 5)\n",
    "\n",
    "It would seem the 53,200 number comes from the fact that Ted has page level genre information. The details of this can be found in an additional figshare repository, [Page-leve genre metadata](https://figshare.com/articles/Page_Level_Genre_Metadata_for_English_Language_Volumes_in_HathiTrust_1700_1922/1279201) where:\n",
    "\n",
    "> Volumes of pentry often include proce introduction, or front and back matter; this was trimmed using publicly-available metadata. (page 33)\n",
    "\n",
    "This is how he was able to extract individual poems from the Hathi Trust data.\n",
    "\n",
    "TODO: We are going to need to be a bit more formal about how we represent the processes that created the data used in this code.\n",
    "\n",
    "Joris and Matt had short discussion the potential for confounds in the *random* sample which might have skewed that sample because it had the potential to include reviewed poetry. Pages 33 and 34 for the Pace of Change talk about the sampling and data preparation. We needed to talk through exactly what they did to understand it fully. After our discussion we agreed with their reasoning, but we would have liked to see a few more numbers. For example, \n",
    "\n",
    "> when a stray volume from the random set turns up near the top of our model’s list of books likely to be reviewed, it does turn out that many of those authors are reasonably well-known (Rupert Brooke, Elaine Goodale Eastman). (page 33)\n",
    "\n",
    "We would like to see more supporting data about the ratio between well-known and not-well-known authors popping up at the top of the model's list of books. Perhaps this is splitting hairs, but it would be good to have this supporting data. This however raises the question about how to formalize the \"well-knownness\" of a particular author. This utlimately might be more of a commentary upon our total ignorance of 19th century poetry, Matt and Joris are not English literary scholars (Joris is a Dutch literary scholar) and as such neither of us know very much about 19th century poets. Basically, we don't know much about Rupert Brooke and Elaine Goodale Eastman. This is perhaps a minor nitpick.\n",
    "\n",
    "Our question is \"How many *well-known* authors were in the random sample?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the shape of the data\n",
    "\n",
    "The poems live in the `poems` directory. Here is what they look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",\t2745\r\n",
      "the\t1445\r\n",
      "and\t1182\r\n",
      ".\t672\r\n",
      "of\t468\r\n",
      "to\t442\r\n",
      ":\t386\r\n",
      "in\t384\r\n",
      ";\t324\r\n",
      "a\t253\r\n",
      "but\t228\r\n",
      "his\t223\r\n",
      "he\t218\r\n",
      "|'s|\t211\r\n",
      "with\t198\r\n",
      "—\t197\r\n",
      "that\t188\r\n",
      "on\t187\r\n",
      "they\t172\r\n",
      "for\t171\r\n"
     ]
    }
   ],
   "source": [
    "!head -n20 poems/dul1.ark+\\=13960\\=t5fb5xg2z.poe.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "poems are represented as word frequency counts. \n",
    "\n",
    "\n",
    "We should also be mindful of what information is captured by the metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docid,actualdate,inferreddate,firstpub,recept,recordid,OCLC,author,imprint,enumcron,title,pubrev,judge,impaud,yrrev,pubname,birth,gender,nationality,othername,notes,canon\r\n",
      "loc.ark+=13960=t8sb4zz1q,1921,1921,1921,addcanon,537314,,\"Lawrence, D. H.\",New York;T. Seltzer;1921.,,Tortoises,,,,,,1885,m,uk,,,y\r\n",
      "uc1.b3342759,1919,1919,1919,random,7930862,,\"Wigren, Bessie C.\",Boston;The Poet Lore Company;c1919,,Summer wind,,,,,,1874,f,us,,,\r\n",
      "uc1.b4100590,1918,1918,1918,reviewed,6154122,2143179,\"Waugh, Alec,\",London;G. Richards;1918.,,Resentment,rev,,,1918,EGO,1898,m,uk,,,\r\n",
      "uc1.b3340220,1918,1918,1918,reviewed,7917249,12688503,\"Nightingale, M.\",Oxford [Oxfordshire;B.H. Blackwell;1918.,,Verses wise and otherwise,rev,neg,,1919,EGO,1879,f,uk,,,\r\n",
      "uc2.ark+=13960=t0ft8gj1k,1918,1918,1918,reviewed,7657411,2518108,\"Faber, Geoffrey,\",\"Oxford;B. H. Blackwell;New York;Longmans, Green & Co.;1918.\",,In the valley of vision,rev,,,1918,EGO,1889,m,uk,,,\r\n",
      "uc2.ark+=13960=t6k070h7g,1918,1918,1918,reviewed,6607061,2809404,\"Farjeon, Eleanor,\",\"Oxford;B.H. Blackwell;New York;Sold in America by Longmans, Green;1918\",,Sonnets and poems,rev,,,1918,EGO,1881,f,uk,,,\r\n",
      "miun.abr7660.0001.001,1918,1918,1918,reviewed,389498,736313,\"Ford, Ford Madox,\",London;John Lane;New York;John Lane company;1918.,,\"On Heaven, and poems written on active service\",rev,,,1918,EGO,1873,m,uk,,,\r\n",
      "mdp.39015008296884,1918,1918,1918,random,1187193,,\"Thomas, Edward,\",London;Selwyn & Blount;1918.,,Last poems,,,,,,1878,m,uk,,Well-known,y\r\n",
      "njp.32101067680247,1918,1918,1918,random,8963885,,\"Carmichael, Amy,\",Edinburgh;Oliphant;1918,,Made in the pans,,,,,,1867,f,uk,,,\r\n",
      "loc.ark+=13960=t3902s17h,1918,1918,1918,random,9577666,,\"Gruse, Edward.\",Boston;The Gorham Press;1918.,,Echoes of democracy,,,,,,1884,m,us,,,\r\n",
      "loc.ark+=13960=t40s0803k,1918,1918,1918,random,9568238,,\"Jenness, Burt Franklin,\",\"Boston, The Cornhill Company;c1918\",,Man-o'-war rhymes,,,,,,1876,m,us,,,\r\n",
      "loc.ark+=13960=t7pn9zm6f,1918,1918,1918,random,7665565,,\"McClure, John Peebles,\",New York;A. A. Knopf;1918.,,Airs and ballads,,,,,,1893,m,us,,,\r\n",
      "mdp.39015013402501,1918,1918,1918,addcanon,537645,,\"Lawrence, D. H.\",New York;B. W. Huebsch;1918.,,Look! We Have Come Through,,,,,,1885,m,uk,,,y\r\n",
      "uc1.b3626228,1917,1917,1917,reviewed,309384,795110,\"Aiken, Conrad,\",Boston;The Four Seas Company;1917.,,\"Nocturne of remembered spring, and other poems\",rev,,,1918,EGO,1889,m,us,,,\r\n",
      "uc2.ark+=13960=t80k27j8j,1917,1917,1917,reviewed,1838375,1006049,\"Eliot, T. S.\",London;The Egoist Ltd. ...;1917.,,Prufrock and other observations,rev,pos,,1917,EGO,1888,m,us,,,y\r\n",
      "uc1.b4100506,1917,1917,1917,reviewed,6154086,3545363,\"Campbell, Joseph,\",Dublin;London;Maunsel;1917.;(Dublin;George Roberts),,Earth of Cualann,rev,pos,,1917,EGO,1879,m,ir,Seosamh Mac Cathmhaoil,,\r\n",
      "loc.ark+=13960=t0dv25v7j,1917,1917,1917,random,9565056,,\"Reed, Helen Leah,\",Boston;De Wolfe and Fiske;c1917.,,\"Memorial Day, and other verse (original and translated)\",,,,,,1860,f,us,,,\r\n",
      "loc.ark+=13960=t76t19455,1917,1917,1917,random,6517209,,\"Bachtell, Paul B.\",Los Angeles;Baumgardt Publishing Company;1917.,,At Lucifer's portals,,,,,,1890,m,us,,,\r\n",
      "uc2.ark+=13960=t6736mr5n,1917,1917,1917,random,6228739,,\"Aikins, Carroll.\",\"Boston;Sherman, French & Company;1917.\",,Poems,,,,,,1888,m,ca,,,\r\n"
     ]
    }
   ],
   "source": [
    "!head -n20 poemeta.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diving into `replicate.py`\n",
    "\n",
    "So this is not exact copy of replicate.py, instead we have *defactored* the code by starting at the branch point at line 45 of replicate.py (git commit e2b5b8f9a86d3f80360865a6628488619f7849d6). Basically everything after `if command == 'full':` and before `elif command == 'quarters':` \n",
    "\n",
    "MJ: The 'rest' of this module basically splits of 6 branches of the process related to the six possible ways of carving up the data `{\"full\", \"quarters\", \"nations\", \"genders\", \"canon\", \"halves\"}`, each option sets the different paramaters for the analytic process associated with that specific command. \n",
    "\n",
    "For reasons of feasibility we will only examine the first branching off tied to the command 'full'.\n",
    "'Full' means: \n",
    "> process all 700 volumes model represented in Fig. 1 of the article \n",
    "\n",
    "(cf. \"model the full 700-volume dataset using default settings\" above)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PATHS.\n",
    "\n",
    "sourcefolder = 'poems/'\n",
    "extension = '.poe.tsv'\n",
    "classpath = 'poemeta.csv'\n",
    "outputpath = 'mainmodelpredictions.csv'\n",
    "\n",
    "## EXCLUSIONS.\n",
    "\n",
    "excludeif = dict()\n",
    "excludeif['pubname'] = 'TEM'\n",
    "# We're not using reviews from Tait's.\n",
    "\n",
    "excludeif['recept'] = 'addcanon'\n",
    "# We don't ordinarily include canonical volumes that were not in either sample.\n",
    "# These are included only if we're testing the canon specifically.\n",
    "\n",
    "excludeifnot = dict()\n",
    "excludeabove = dict()\n",
    "excludebelow = dict()\n",
    "\n",
    "excludebelow['firstpub'] = 1700\n",
    "excludeabove['firstpub'] = 1950\n",
    "sizecap = 360\n",
    "\n",
    "# For more historically-interesting kinds of questions, we can limit the part\n",
    "# of the dataset that gets TRAINED on, while permitting the whole dataset to\n",
    "# be PREDICTED. (Note that we always exclude authors from their own training\n",
    "# set; this is in addition to that.) The variables futurethreshold and\n",
    "# pastthreshold set the chronological limits of the training set, inclusive\n",
    "# of the threshold itself.\n",
    "\n",
    "## THRESHOLDS\n",
    "\n",
    "futurethreshold = 1925\n",
    "pastthreshold = 1800\n",
    "\n",
    "# CLASSIFY CONDITIONS\n",
    "\n",
    "positive_class = 'rev'\n",
    "category2sorton = 'reviewed'\n",
    "datetype = 'firstpub'\n",
    "numfeatures = 3200\n",
    "regularization = .00007"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell above is the specification of a bunch of parameters. It is a pretty inn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rawaccuracy, allvolumes, coefficientuples = pc.create_model(paths, exclusions, thresholds, classifyconditions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the point in replicate.py where the script calls out to a function in a separate file, `parallel_crossvalidate.py`. \n",
    "\n",
    "This function, `create_model`, does computational work that we don't want to *step over* because it is integral to the computational narrative. So, rather than step over the function, we are going *step into* the function an copy its contents into the notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we are now stepping into the file `prallel_crossvalidate.py` and specifically diving down into the `create_model` function. We have copied the contents of `create_model` below "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diving into `parallel_crossvalidate.py`\n",
    "\n",
    "specifically, we are diving into the `create_model` function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os # DEFACTORING CODE\n",
    "\n",
    "\n",
    "# def create_model(paths, exclusions, thresholds, classifyconditions):\n",
    "\n",
    "''' This is the main function in the module.\n",
    "It can be called externally; it's also called\n",
    "if the module is run directly.\n",
    "'''\n",
    "verbose = False\n",
    "\n",
    "if not sourcefolder.endswith('/'):\n",
    "    sourcefolder = sourcefolder + '/'\n",
    "\n",
    "# This just makes things easier.\n",
    "\n",
    "# Get a list of files.\n",
    "allthefiles = os.listdir(sourcefolder)\n",
    "# random.shuffle(allthefiles)\n",
    "\n",
    "volumeIDs = list()\n",
    "volumepaths = list()\n",
    "\n",
    "for filename in allthefiles:\n",
    "\n",
    "    if filename.endswith(extension):\n",
    "        volID = filename.replace(extension, \"\")\n",
    "        # The volume ID is basically the filename minus its extension.\n",
    "        # Extensions are likely to be long enough that there is little\n",
    "        # danger of accidental occurrence inside a filename. E.g.\n",
    "        # '.fic.tsv'\n",
    "        path = sourcefolder + filename\n",
    "        volumeIDs.append(volID)\n",
    "        volumepaths.append(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diving into `metafilter.get_metadata()`\n",
    "\n",
    "`metadict = metafilter.get_metadata(classpath, volumeIDs, excludeif, excludeifnot, excludebelow, excludeabove)`\n",
    "\n",
    "Ted's code calls out to `metafilter.py` which contains several functions for reading the metadata csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'recept': 'addcanon', 'pubname': 'TEM'}\n",
      "{}\n",
      "{'firstpub': 1700}\n",
      "{'firstpub': 1950}\n"
     ]
    }
   ],
   "source": [
    "# reminder of how Ted (or is it us?) sets the variables above\n",
    "print(excludeif)\n",
    "print(excludeifnot)\n",
    "print(excludebelow)\n",
    "print(excludeabove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DEFACTORING\n",
    "# we need these helper functions for execute the next code cell\n",
    "import csv \n",
    "\n",
    "def dirty_pairtree(htid):\n",
    "    period = htid.find('.')\n",
    "    prefix = htid[0:period]\n",
    "    postfix = htid[(period+1): ]\n",
    "    if '=' in postfix:\n",
    "        postfix = postfix.replace('+',':')\n",
    "        postfix = postfix.replace('=','/')\n",
    "    dirtyname = prefix + \".\" + postfix\n",
    "    return dirtyname\n",
    "\n",
    "def forceint(astring):\n",
    "    try:\n",
    "        intval = int(astring)\n",
    "    except:\n",
    "        intval = 0\n",
    "\n",
    "    return intval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poemeta.csv\n",
      "tossing loc.ark:/13960/t8sb4zz1q\n",
      "tossing mdp.39015013402501\n",
      "tossing mdp.39015011913525\n",
      "tossing hardywessexpoems189.hardywessexpoems1898\n",
      "tossing gerardmhopkins191.gerardmhopkins1918\n",
      "tossing loc.ark:/13960/t3fx82c2q\n",
      "tossing emilydickinso.emilydickinson\n",
      "tossing ellisbell184.ellisbell1848\n",
      "We have 8 volumes in missing in metadata, and\n",
      "0 volumes missing in the directory.\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "# def get_metadata(classpath, volumeIDs, excludeif, excludeifnot, excludebelow, excludeabove):\n",
    "'''\n",
    "As the name would imply, this gets metadata matching a given set of volume\n",
    "IDs. It returns a dictionary containing only those volumes that were present\n",
    "both in metadata and in the data folder.\n",
    "\n",
    "It also accepts four dictionaries containing criteria that will exclude volumes\n",
    "from the modeling process.\n",
    "'''\n",
    "print(classpath)\n",
    "metadict = dict()\n",
    "\n",
    "with open(classpath, encoding = 'utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "\n",
    "    anonctr = 0\n",
    "\n",
    "    for row in reader:\n",
    "        volid = dirty_pairtree(row['docid'])\n",
    "        theclass = row['recept'].strip()\n",
    "\n",
    "        # I've put 'remove' in the reception column for certain\n",
    "        # things that are anomalous.\n",
    "        if theclass == 'remove':\n",
    "            continue\n",
    "\n",
    "        bail = False\n",
    "        for key, value in excludeif.items():\n",
    "            if row[key] == value:\n",
    "                bail = True\n",
    "        for key, value in excludeifnot.items():\n",
    "            if row[key] != value:\n",
    "                bail = True\n",
    "        for key, value in excludebelow.items():\n",
    "            if forceint(row[key]) < value:\n",
    "                bail = True\n",
    "        for key, value in excludeabove.items():\n",
    "            if forceint(row[key]) > value:\n",
    "                bail = True\n",
    "\n",
    "        if bail:\n",
    "            print(\"tossing \"+volid) # DEFACTORING CODE\n",
    "            continue\n",
    "\n",
    "        birthdate = forceint(row['birth'])\n",
    "\n",
    "        pubdate = forceint(row['inferreddate'])\n",
    "\n",
    "        gender = row['gender'].rstrip()\n",
    "        nation = row['nationality'].rstrip()\n",
    "\n",
    "        #if pubdate >= 1880:\n",
    "            #continue\n",
    "\n",
    "        if nation == 'ca':\n",
    "            nation = 'us'\n",
    "        elif nation == 'ir':\n",
    "            nation = 'uk'\n",
    "        # I hope none of my Canadian or Irish friends notice this.\n",
    "\n",
    "        notes = row['notes'].lower()\n",
    "        author = row['author']\n",
    "        if len(author) < 1 or author == '<blank>':\n",
    "            author = \"anonymous\" + str(anonctr)\n",
    "            anonctr += 1\n",
    "\n",
    "        title = row['title']\n",
    "        canon = row['canon']\n",
    "\n",
    "        # I'm creating two distinct columns to indicate kinds of\n",
    "        # literary distinction. The reviewed column is based purely\n",
    "        # on the question of whether this work was in fact in our\n",
    "        # sample of contemporaneous reviews. The obscure column incorporates\n",
    "        # information from post-hoc biographies, which trumps\n",
    "        # the question of reviewing when they conflict.\n",
    "\n",
    "        if theclass == 'random':\n",
    "            obscure = 'obscure'\n",
    "            reviewed = 'not'\n",
    "        elif theclass == 'reviewed':\n",
    "            obscure = 'known'\n",
    "            reviewed = 'rev'\n",
    "        elif theclass == 'addcanon':\n",
    "            print(\"this is executing\") # DEFACTORING CODE\n",
    "            obscure = 'known'\n",
    "            reviewed = 'addedbecausecanon'\n",
    "        else:\n",
    "            print(\"Missing class\" + theclass)\n",
    "\n",
    "        if notes == 'well-known':\n",
    "            obscure = 'known'\n",
    "        if notes == 'obscure':\n",
    "            obscure = 'obscure'\n",
    "\n",
    "        if canon == 'y':\n",
    "            if theclass == 'addcanon':\n",
    "                actually = 'Norton, added'\n",
    "            else:\n",
    "                actually = 'Norton, in-set'\n",
    "        elif reviewed == 'rev':\n",
    "            actually = 'reviewed'\n",
    "        else:\n",
    "            actually = 'random'\n",
    "\n",
    "        metadict[volid] = dict()\n",
    "        metadict[volid]['reviewed'] = reviewed\n",
    "        metadict[volid]['obscure'] = obscure\n",
    "        metadict[volid]['pubdate'] = pubdate\n",
    "        metadict[volid]['birthdate'] = birthdate\n",
    "        metadict[volid]['gender'] = gender\n",
    "        metadict[volid]['nation'] = nation\n",
    "        metadict[volid]['author'] = author\n",
    "        metadict[volid]['title'] = title\n",
    "        metadict[volid]['canonicity'] = actually\n",
    "        metadict[volid]['pubname'] = row['pubname']\n",
    "        metadict[volid]['firstpub'] = forceint(row['firstpub'])\n",
    "\n",
    "# These come in as dirty pairtree; we need to make them clean.\n",
    "\n",
    "cleanmetadict = dict()\n",
    "allidsinmeta = set([x for x in metadict.keys()])\n",
    "allidsindir = set([dirty_pairtree(x) for x in volumeIDs])\n",
    "missinginmeta = len(allidsindir - allidsinmeta)\n",
    "missingindir = len(allidsinmeta - allidsindir)\n",
    "print(\"We have \" + str(missinginmeta) + \" volumes in missing in metadata, and\")\n",
    "print(str(missingindir) + \" volumes missing in the directory.\")\n",
    "print(allidsinmeta - allidsindir)\n",
    "\n",
    "for anid in volumeIDs:\n",
    "    dirtyid = dirty_pairtree(anid)\n",
    "    if dirtyid in metadict:\n",
    "        cleanmetadict[anid] = metadict[dirtyid]\n",
    "\n",
    "metadict = cleanmetadict #DEFACTORING CODE changed this from return cleanmetadict to setting the metadict variable which is expected below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What just happened here? Well, the code above loaded the `poemeta.csv` file and filtered out a bunch of rows (based upon the `excludeif`, `excludeifnot`, `excludeabove`, and `excludebelow` variables) and also normalizes some of the `nation` data (normalizing is a pretty clinical way of lumping Canada with the United States and Ireland with the UK). Nationality is not a factor in the Pace of Change analysis, but it is interesting to see this code here, it implies this code is used in other analyses.\n",
    "\n",
    "The other important thing this code cell does is split the `recept` column in the `poemeta.csv` into two columns, `obscure` and `reviewed`. There is a bit of logic here that we do not fully grasp at this point. From what we can tell from the code and Ted's comment, there poems that are reviewed, there are poems that are obscure, and there are poems that are not in the reviewed set but are never-the-less part of the cannon. This means they are \"known\" and, according to Ted's comment, trumps the conflict when the author is known (`obscure = 'known'`) but not explicitly in the reviewed set. \n",
    "\n",
    "We have discovered after adding some `# DEFACTORING CODE` snippets that this code never actually runs. All of the poems with the 'addcanon' property are tossed out and the conflict, where the poem is known by in the random set, never appears to occur. Conjector: is this a remnant of Ted refactoring the code due to changes in the analysis process or just working with different data or something we cannot possible conceive. What was the author's intent? \n",
    "\n",
    "We know that poems with the 'addcanon' in the 'recept' column are being excluded because they are included in the `excludeif` dictionary. Why? The code in the first code cell [TODO: name cells for PROV naming and narrative reference]  provide somewhat of an explanation\n",
    "\n",
    "```\n",
    "excludeif['recept'] = 'addcanon'\n",
    "# We don't ordinarily include canonical volumes that were not in either sample.\n",
    "# These are included only if we're testing the canon specifically.\n",
    "```\n",
    "\n",
    "It should noted we spent a considerable amount of time interpreting the code that handled this particular situation before realizing that it would never be executed because of the settings in the `excludeif` dictionary. That makes us look stupid, but we also now have a more intimate understanding and relationship with the code. or maybe we are still stupid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### label_classes\n",
    "\n",
    "We have flattened [pulled in? not sure what term to use here, we need some terminology for DEFACTORING as a method] the metafilter.label_classes() function. This function reads the metadata properties and puts all entries into one of two bins: *positive* or *negative*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now that we have a list of volumes with metadata, we can select the groups of IDs\n",
    "# that we actually intend to contrast. If we want to us more or less everything,\n",
    "# this may not be necessary. But in some cases we want to use randomly sampled subsets.\n",
    "\n",
    "# The default condition here is\n",
    "\n",
    "# category2sorton = 'reviewed'\n",
    "# positive_class = 'rev'\n",
    "# sizecap = 350\n",
    "# A sizecap less than one means, no sizecap.\n",
    "\n",
    "#IDsToUse, classdictionary = label_classes(metadict, category2sorton, positive_class, sizecap)\n",
    "\n",
    "import random # DEFACTORING CODE\n",
    "\n",
    "# def label_classes(metadict, category2sorton, positive_class, sizecap):\n",
    "''' This takes as input the metadata dictionary generated\n",
    "by get_metadata. It subsets that dictionary into a\n",
    "positive class and a negative class. Instances that belong\n",
    "to neither class get ignored.\n",
    "'''\n",
    "\n",
    "all_instances = set([x for x in metadict.keys()])\n",
    "\n",
    "# The first stage is to find positive instances.\n",
    "\n",
    "all_positives = set()\n",
    "\n",
    "for key, value in metadict.items():\n",
    "    if value[category2sorton] == positive_class:\n",
    "        all_positives.add(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where ted's code distinguishes the red triangles from the grey dots in figure one. If poem metadata has the value 'rev' for the 'reviewed' property then it is labeled as a positive. The next cell does the same for all negatives and removes any items that were added because of the canon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_negatives = all_instances - all_positives\n",
    "iterator = list(all_negatives)\n",
    "for item in iterator:\n",
    "    if metadict[item]['reviewed'] == 'addedbecausecanon':\n",
    "        all_negatives.remove(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negative labels are assigned to all instances that are not in the set of positive instances. There is additional code that filters out anything with 'addedbecausecannon' set for the 'reviewed' property, but this code should never execute. This is a vestige of testing with the canon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360\n"
     ]
    }
   ],
   "source": [
    "if sizecap > 0 and len(all_positives) > sizecap:\n",
    "    positives = random.sample(all_positives, sizecap)\n",
    "else:\n",
    "    positives = list(all_positives)\n",
    "    print(len(all_positives))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we see `360` being printed by the code above we know he is not sampling from the positives list. Ted's comment above, seems to indicate the sizecap is 350, but if you look at the code in the first code cell, the sizecap is *actually* 360...which just so happens to be the number of positive labeled instances. Funny. Looks like Ted updated his sizecap, but didn't update his comment.\n",
    "\n",
    "How was the sizecap determined? Performance and scalability? Perhaps some of the test sets were much larger than 360 instances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If there's a sizecap we also want to ensure classes have\n",
    "# matching sizes and roughly equal distributions over time.\n",
    "\n",
    "numpositives = len(all_positives)\n",
    "\n",
    "if sizecap > 0 and len(all_negatives) > numpositives:\n",
    "    if not 'date' in category2sorton:\n",
    "        available_negatives = list(all_negatives)\n",
    "        negatives = list()\n",
    "\n",
    "        for anid in positives:\n",
    "            date = metadict[anid]['pubdate']\n",
    "\n",
    "            available_negatives = sort_by_proximity(available_negatives, metadict, date)\n",
    "            selected_id = available_negatives.pop(0)\n",
    "            negatives.append(selected_id)\n",
    "\n",
    "    else:\n",
    "        # if we're dividing classes by date, we obvs don't want to\n",
    "        # ensure equal distributions over time.\n",
    "\n",
    "        negatives = random.sample(all_negatives, sizecap)\n",
    "\n",
    "else:\n",
    "    negatives = list(all_negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So most of the code in the cell above does not execute because we are not sampling from the positives distribution. \n",
    "\n",
    "This code cell makes an un-excuted reference to a function `sort_by_proximity()` which we are not including because it is not part of the execution path we are documenting in this notebook. This has provoked a conversation about if we should remove the code from the conditional blocks from the notebook. Yet, keeping them maintains the residue of Ted's development and thinking about what he needs to do with the data and in the analysis. \n",
    "\n",
    "These issues point to properties of code that make it difficult to review or critique, that is, we are in this case, reviewing a single execution path of the code, not the code itself. So we are dealing with a *code-criticism conundrum*: What is the required or adequate breadth and depth of the critique? The decision to include or not include `sort_by_proximity()` is a breadth issue. How broad should we be in including code that does not execute? Note, we are including code from a conditional block that doesn't execute, but are not going out the additional step to include non-executed function defined elsewhere in the code. The decision to include or not include code from the standard library, code not written by Ted, is a depth issue. In the code cell above, there are many functions we are *stepping over*, like `len`, `list`, `append`, `pop`, `random.sample`, etc., because they are black-boxed. There is no need to critique or test or inspect those functions because they have been tested and evaluated  thoroughly outside of the scope of Ted's project [We need a strong REF here].\n",
    "\n",
    "Full reflexivity here would also mean that we note that the 'rules of the game' for code criticism aren't quite clear yet and therefore we are possibly feeling our way through an emerging methodological standard of practice for code criticism. As we see vestiges of Ted evolution in thinking in his code, this notebook is capturing the evolution of our thinking about DEFACTORING as a practice. \n",
    "\n",
    "REF: Hiller and Joris about the tension between code's textual and processual dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "We have 360 positive, and\n",
      "360 negative instances.\n"
     ]
    }
   ],
   "source": [
    "# Now we have two lists of ids.\n",
    "\n",
    "IDsToUse = set()\n",
    "classdictionary = dict()\n",
    "\n",
    "print()\n",
    "print(\"We have \" + str(len(positives)) + \" positive, and\")\n",
    "print(str(len(negatives)) + \" negative instances.\")\n",
    "\n",
    "for anid in positives:\n",
    "    IDsToUse.add(anid)\n",
    "    classdictionary[anid] = 1\n",
    "\n",
    "for anid in negatives:\n",
    "    IDsToUse.add(anid)\n",
    "    classdictionary[anid] = 0\n",
    "\n",
    "for key, value in metadict.items():\n",
    "    if value['reviewed'] == 'addedbecausecanon':\n",
    "        IDsToUse.add(key)\n",
    "        classdictionary[key] = 0\n",
    "# We add the canon supplement, but don't train on it.\n",
    "\n",
    "# return IDsToUse, classdictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## popping out of label_classes()\n",
    "\n",
    "\n",
    "Now we return to the execution of the `create_model` function from `parallel_crossvalidate.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DEFACTORING \n",
    "# We need this import and functions for the code cell below to run\n",
    "\n",
    "from collections import Counter \n",
    "\n",
    "def infer_date(metadictentry, datetype):\n",
    "    if datetype == 'pubdate':\n",
    "        return metadictentry[datetype]\n",
    "    elif datetype == 'firstpub':\n",
    "        firstpub = metadictentry['firstpub']\n",
    "        if firstpub > 1700 and firstpub < 1950:\n",
    "            return firstpub\n",
    "        else:\n",
    "            return metadictentry['pubdate']\n",
    "    else:\n",
    "        sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make a vocabulary list and a volsize dict\n",
    "wordcounts = Counter()\n",
    "\n",
    "volspresent = list()\n",
    "orderedIDs = list()\n",
    "\n",
    "positivecounts = dict()\n",
    "negativecounts = dict()\n",
    "\n",
    "for volid, volpath in zip(volumeIDs, volumepaths):\n",
    "    if volid not in IDsToUse:\n",
    "        continue\n",
    "    else:\n",
    "        volspresent.append((volid, volpath))\n",
    "        orderedIDs.append(volid)\n",
    "\n",
    "    date = infer_date(metadict[volid], datetype)\n",
    "    if date < pastthreshold or date > futurethreshold:\n",
    "        continue\n",
    "    else:\n",
    "        with open(volpath, encoding = 'utf-8') as f:\n",
    "            for line in f:\n",
    "                fields = line.strip().split('\\t')\n",
    "                if len(fields) > 2 or len(fields) < 2:\n",
    "                    # print(line)\n",
    "                    continue\n",
    "                word = fields[0]\n",
    "                if len(word) > 0 and word[0].isalpha():\n",
    "                    count = int(fields[1])\n",
    "                    wordcounts[word] += 1\n",
    "                    # for initial feature selection we use the number of\n",
    "                    # *documents* that contain a given word,\n",
    "                    # so it's just +=1.\n",
    "\n",
    "vocablist = [x[0] for x in wordcounts.most_common(numfeatures)]\n",
    "\n",
    "# vocablist = binormal_select(vocablist, positivecounts, negativecounts, totalposvols, totalnegvols, 3000)\n",
    "# Feature selection is deprecated. There are cool things\n",
    "# we could do with feature selection,\n",
    "# but they'd improve accuracy by 1% at the cost of complicating our explanatory task.\n",
    "# The tradeoff isn't worth it. Explanation is more important.\n",
    "# So we just take the most common words (by number of documents containing them)\n",
    "# in the whole corpus. Technically, I suppose, we could crossvalidate that as well,\n",
    "# but *eyeroll*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200\n",
      "['that', 'but', 'not', 'is', 'all', 'and', 'with', 'in', 'the', 'of', 'to', 'a', 'for', 'by', 'as', 'on', 'at', 'i', 'one', 'who', 'it', 'from', 'his', 'when', 'they', 'be', 'no', 'their', 'or', 'this', 'he', 'like', 'now', 'was', 'so', 'them', 'have', 'we', 'then', 'there', 'where', 'see', 'are', 'will', 'its', 'her', 'were', 'heart', 'over', 'day']\n"
     ]
    }
   ],
   "source": [
    "# DEFACTORING INSPECTION\n",
    "# What is in vocablist?\n",
    "print(len(vocablist))\n",
    "print(vocablist[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an important section of code because it contains the code for the selection of the 3200 word-features, or \"variables\" as he calls them on page 35 (the methodology appendix), used in the logistic regression. Most notable is the code that has been commented out at the very end of the block. The heuristic for feature selection was simply to select the 3200 most common words, a simple and easy to explain technique (also easy to implement with Python's Counter collection). The comment discusses an alternative feature selection technique using binormal selection which he has implemented in the function `binormal_selection`. Because this code is commented out, we do not explore it in-depth here. More interesting is the rationale about *why* it has been commented out:\n",
    "\n",
    "> There are cool things we could do with feature selection, but they'd improve accuracy by 1% at the cost of complicating our explanatory task.\n",
    "\n",
    "What we are seeing in this comment is a road not taken, which normally the DEFACTORING method would ignore, but the code, the comment, and the implications are crucially important. This reveals much about about Ted's reasoning on the effort and energy he wishes to invest in *explaining* method. There is judgment about the expertise of his audience. This points to a crucial problem at the intersection of humanities and computer science where the dissemination of description of statistical methods are inhibiting or hindering the full realization of the epistemlogical richness of computation as a method of inquiry. Ted is holding himself back, a form of self-censorship, because of a perceived Audience That cannot understand a binormal feature selection technique without a significant amount of work to explain its epistomolgical implications. \n",
    "\n",
    "We think this snippet of code is significant and justifies our method because it is only through close code review were we able to uncover these traces of Ted's thinking and experimentation. We can see epistomolary roads not taken because of strenuous dialectical relationship between the computational potentiality and the disciplinary acceptability. \n",
    "\n",
    "There is more for us to say and think about this vinette. But we need to move on.\n",
    "\n",
    "Aside: we couldn't help ourselves and did a little digging into the `binormal_selection` function. There is an oblique reference to \"see forman\" which we think refers to George Forman who has written several articles on feature selection in texts. We should confirm with Ted.\n",
    "- http://dl.acm.org/citation.cfm?id=944974\n",
    "- http://link.springer.com/chapter/10.1007%2F3-540-45681-3_13\n",
    "- We think this is the paper: http://www.hpl.hp.com/techreports/2007/HPL-2007-32R1.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "donttrainon = list()\n",
    "\n",
    "# Here we create a list of volumed IDs not to be used for training.\n",
    "# For instance, we have supplemented the dataset with volumes that\n",
    "# are in the Norton but that did not actually occur in random\n",
    "# sampling. We want to make predictions for these, but never use\n",
    "# them for training.\n",
    "\n",
    "for idx1, anid in enumerate(orderedIDs):\n",
    "    reviewedstatus = metadict[anid]['reviewed']\n",
    "    date = infer_date(metadict[anid], datetype)\n",
    "    if reviewedstatus == 'addedbecausecanon':\n",
    "        donttrainon.append(idx1)\n",
    "    elif date < pastthreshold or date > futurethreshold:\n",
    "        donttrainon.append(idx1)\n",
    "\n",
    "authormatches = [list(donttrainon) for x in range(len(orderedIDs))]\n",
    "# For every index in authormatches, identify a set of indexes that have\n",
    "# the same author. Obvs, there will always be at least one.\n",
    "\n",
    "# Since we are going to use these indexes to exclude rows, we also add\n",
    "# all the ids in donttrainon to every volume\n",
    "\n",
    "for idx1, anid in enumerate(orderedIDs):\n",
    "    thisauthor = metadict[anid]['author']\n",
    "    for idx2, anotherid in enumerate(orderedIDs):\n",
    "        otherauthor = metadict[anotherid]['author']\n",
    "        if thisauthor == otherauthor and not idx2 in authormatches[idx1]:\n",
    "            authormatches[idx1].append(idx2)\n",
    "\n",
    "for alist in authormatches:\n",
    "    alist.sort(reverse = True)\n",
    "\n",
    "# I am reversing the order of indexes so that I can delete them from\n",
    "# back to front, without changing indexes yet to be deleted.\n",
    "# This will become important in the modelingprocess module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "donttrainon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720\n",
      "720\n",
      "[582, 499, 420, 368, 200, 7]\n",
      "[582, 499, 420, 368, 200, 7]\n",
      "uc2.ark+=13960=t3fx7436h\n",
      "uc1.b4104728\n",
      "{'firstpub': 1849, 'pubdate': 1861, 'nation': 'uk', 'author': 'Tennyson, Alfred Tennyson,', 'pubname': 'ER', 'birthdate': 1809, 'canonicity': 'Norton, in-set', 'obscure': 'known', 'title': 'In memoriam', 'reviewed': 'rev', 'gender': 'm'}\n",
      "{'firstpub': 1859, 'pubdate': 1859, 'nation': 'uk', 'author': 'Tennyson, Alfred Tennyson,', 'pubname': 'WR', 'birthdate': 1809, 'canonicity': 'Norton, in-set', 'obscure': 'known', 'title': 'Idyls of the King', 'reviewed': 'rev', 'gender': 'm'}\n"
     ]
    }
   ],
   "source": [
    "#DEFACTORING INSPECTION\n",
    "print(len(orderedIDs))\n",
    "print(len(authormatches))\n",
    "print(authormatches[7])\n",
    "print(authormatches[582])\n",
    "print(orderedIDs[582])\n",
    "print(orderedIDs[499])\n",
    "print(metadict['uc2.ark+=13960=t3fx7436h'])\n",
    "print(metadict['uc1.b4104728'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all rote data preparation. He compiles a list that shouldn't be used as training data if the date is incorrect or if the books are part of the \"addedbecauseofcanon\" class (representing a well known author). From this list he compiles a list of authors who have been reviewed and then removing their other works from the list. \n",
    "\n",
    "The inspection code above was necessary for us to understanding the complicated data structure Ted was creating.\n",
    "\n",
    "The list `authormatches` is a list of all the poems and each is a list of other poems by the same author. Essentially this data structure is describing the relations of each poem to other poems, that relation being \"other poems by the same author\". It took us a bit of work to figure out because all of this is obscured by the fact the relations are expressed by list indexes. We are working with abstractions which we assume are important for the `modelingprocess` which is invoked below.\n",
    "\n",
    "The purpose of this is because\n",
    "\n",
    "> ...we did exclude who were already in our reviewed sample for a given genre. (page 34)\n",
    "\n",
    "What we seen in the code is the necessary steps to be able to exclude author already in the reviewed sample, but we don't yet see any reference to genre. Either this is implicit (because all these data are the same genre, poetry) or hasn't been addressed as of yet in the code.\n",
    "\n",
    "Aside: we should try an annotate Ted's narrative in the article to point to the specific moments in the code where he is describing (in english) what is happening in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DEFACTORING \n",
    "# we need get_features and get_features_with_data when reading in the files in poems/ directory\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "usedate = False\n",
    "# Leave this flag false unless you plan major\n",
    "# surgery to reactivate the currently-deprecated\n",
    "# option to use \"date\" as a predictive feature.\n",
    "\n",
    "\n",
    "def get_features(wordcounts, wordlist):\n",
    "    numwords = len(wordlist)\n",
    "    wordvec = np.zeros(numwords)\n",
    "    for idx, word in enumerate(wordlist):\n",
    "        if word in wordcounts:\n",
    "            wordvec[idx] = wordcounts[word]\n",
    "\n",
    "    return wordvec\n",
    "\n",
    "# In an earlier version of this script, we sometimes used\n",
    "# \"publication date\" as a feature, to see what would happen.\n",
    "# In the current version, we don't. Some of the functions\n",
    "# and features remain, but they are deprecated. E.g.:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `get_features` simply takes the wordcounts from the parsed poem and filters out any words that are not part of `wordlist` which is the selected features described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "volsizes = dict()\n",
    "voldata = list()\n",
    "classvector = list()\n",
    "\n",
    "for volid, volpath in volspresent:\n",
    "\n",
    "    with open(volpath, encoding = 'utf-8') as f:\n",
    "        voldict = dict()\n",
    "        totalcount = 0\n",
    "        for line in f:\n",
    "            fields = line.strip().split('\\t')\n",
    "            if len(fields) > 2 or len(fields) < 2:\n",
    "                continue\n",
    "\n",
    "            word = fields[0]\n",
    "            count = int(fields[1])\n",
    "            voldict[word] = count\n",
    "            totalcount += count\n",
    "\n",
    "    date = infer_date(metadict[volid], datetype)\n",
    "    date = date - 1700\n",
    "    if date < 0:\n",
    "        date = 0\n",
    "\n",
    "    if usedate:\n",
    "        features = get_features_with_date(voldict, vocablist, date, totalcount)\n",
    "        voldata.append(features)\n",
    "    else:\n",
    "        features = get_features(voldict, vocablist)\n",
    "        voldata.append(features / (totalcount + 0.001))\n",
    "\n",
    "\n",
    "    volsizes[volid] = totalcount\n",
    "    classflag = classdictionary[volid]\n",
    "    classvector.append(classflag)\n",
    "\n",
    "data = pd.DataFrame(voldata)\n",
    "\n",
    "sextuplets = list()\n",
    "for i, volid in enumerate(orderedIDs):\n",
    "    listtoexclude = authormatches[i]\n",
    "    asixtuple = data, classvector, listtoexclude, i, usedate, regularization\n",
    "    sextuplets.append(asixtuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code opens up each of the files in the poems/ directory, parses the file, does some data cleaning with respect to the structure of those files and with dates of poems older than 1700 (this is probably vestage code because we are not suing data a a predictive feature. The important bit is the call to the `get_features` function which throws out the word features that are not part of the list of selected word features as determined by the `most_common()` call above and stored in the `vocablist` variable. \n",
    "\n",
    "\n",
    "We are curious about this code:\n",
    "\n",
    "```\n",
    "voldata.append(features / (totalcount + 0.001))\n",
    "```\n",
    "\n",
    "We understand he is normalizing the data to make volumes of differing sizes comparable. Turning absolute frequencies into relative frequencies. However, we don't know why he is adding 0.001 to the total count. Is this to prevent a potential divide by zero? NOTE: we think that the 0.001 is added so the logistic regession doesn't result in infinite at zero when running the cost function.\n",
    "\n",
    "Sextuplets is a list of tuples with six values. This is the datastructure which will be passed to the modeling process. Each row of `sextuplets` contains all of the necessary data structures to model each poem. because we are generating a new model for each poem, we need to pass a couple parameters to the modeling process (such as which poems to ignore because they are by the same author, `listtoexclude`, when modeling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A quick investigation of the six key datastructures\n",
    "\n",
    "The modeling function described in the next section of the notebook relies upon six data structures from \n",
    "\n",
    "- data: a document features matrix. Word features are the columns and volumes are the rows. 720 x 3200\n",
    "- classvector: the classification of documents as either 'reviewed' (1) or 'random' (0).\n",
    "- listtoexclude: the\n",
    "- i: the index of the volume\n",
    "- usedate: a flag indicating if date is a feature. It is not in our case\n",
    "- regularization: a parameter for the scikit-learn LogisticRegression function. hardcoded parameter from the beginning of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0         1         2         3         4         5         6     \\\n",
      "0  0.006835  0.008289  0.005817  0.005708  0.004799  0.042972  0.007198   \n",
      "1  0.006666  0.006467  0.007163  0.006467  0.004145  0.031172  0.008523   \n",
      "2  0.013064  0.002969  0.004157  0.010095  0.001781  0.029691  0.007126   \n",
      "3  0.007202  0.006251  0.000775  0.002776  0.001175  0.035708  0.002926   \n",
      "4  0.010812  0.005189  0.004248  0.001786  0.005599  0.032099  0.012767   \n",
      "\n",
      "       7         8         9       ...         3190      3191      3192  \\\n",
      "0  0.013961  0.052534  0.017014    ...     0.000000  0.000000  0.000000   \n",
      "1  0.013928  0.051733  0.020229    ...     0.000033  0.000000  0.000033   \n",
      "2  0.013658  0.049881  0.029097    ...     0.000000  0.000000  0.000000   \n",
      "3  0.008302  0.042335  0.005976    ...     0.000000  0.000025  0.000000   \n",
      "4  0.011995  0.037674  0.018415    ...     0.000048  0.000097  0.000000   \n",
      "\n",
      "       3193      3194      3195      3196      3197      3198      3199  \n",
      "0  0.000036  0.000073  0.000000  0.000036  0.000000  0.000000  0.000036  \n",
      "1  0.000000  0.000000  0.000133  0.000000  0.000099  0.000000  0.000000  \n",
      "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "3  0.000025  0.000000  0.000000  0.000050  0.000000  0.000000  0.000050  \n",
      "4  0.000024  0.000000  0.000000  0.000000  0.000000  0.000024  0.000000  \n",
      "\n",
      "[5 rows x 3200 columns]\n",
      "[1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0]\n",
      "[582, 499, 420, 368, 200, 7]\n",
      "582\n",
      "False\n",
      "7e-05\n"
     ]
    }
   ],
   "source": [
    "# DEFACTORING INSPECTION\n",
    "print(sextuplets[582][0].head()) #data\n",
    "print(sextuplets[582][1]) #classvector \n",
    "print(sextuplets[582][2]) #listtoexclude\n",
    "print(sextuplets[582][3]) #i\n",
    "print(sextuplets[582][4]) #usedate\n",
    "print(sextuplets[582][5]) #regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diving Into model_one_volume()\n",
    "\n",
    "We are now about to step down into the very heart of the project, the modeling of each individual text.\n",
    "\n",
    "To do this we need to bring the function, model_one_volume into the global namespace of the notebook. This means we need to dig into a new module, modelingprocess and extract a few functions from it. The main modeling functin, `model_one_volume` depends upon two helper functions, `normalizearray()` and `sliceframe.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# modelingprocess.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def sliceframe(dataframe, yvals, excludedrows, testrow):\n",
    "    numrows = len(dataframe)\n",
    "    newyvals = list(yvals)\n",
    "    for i in excludedrows:\n",
    "        del newyvals[i]\n",
    "        # NB: This only works if we assume that excluded rows\n",
    "        # has already been sorted in descending order !!!!!!!\n",
    "        # otherwise indexes will slide around as you delete\n",
    "\n",
    "    trainingset = dataframe.drop(dataframe.index[excludedrows])\n",
    "\n",
    "    newyvals = np.array(newyvals)\n",
    "    testset = dataframe.iloc[testrow]\n",
    "\n",
    "    return trainingset, newyvals, testset\n",
    "\n",
    "def normalizearray(featurearray, usedate):\n",
    "    '''Normalizes an array by centering on means and\n",
    "    scaling by standard deviations. Also returns the\n",
    "    means and standard deviations for features.\n",
    "    '''\n",
    "\n",
    "    numinstances, numfeatures = featurearray.shape\n",
    "    means = list()\n",
    "    stdevs = list()\n",
    "    lastcolumn = numfeatures - 1\n",
    "    for featureidx in range(numfeatures):\n",
    "\n",
    "        thiscolumn = featurearray.iloc[ : , featureidx]\n",
    "        thismean = np.mean(thiscolumn)\n",
    "\n",
    "        thisstdev = np.std(thiscolumn)\n",
    "\n",
    "        if (not usedate) or featureidx != lastcolumn:\n",
    "            # If we're using date we don't normalize the last column.\n",
    "            means.append(thismean)\n",
    "            stdevs.append(thisstdev)\n",
    "            featurearray.iloc[ : , featureidx] = (thiscolumn - thismean) / thisstdev\n",
    "        else:\n",
    "            print('FLAG')\n",
    "            means.append(thismean)\n",
    "            thisstdev = 0.1\n",
    "            stdevs.append(thisstdev)\n",
    "            featurearray.iloc[ : , featureidx] = (thiscolumn - thismean) / thisstdev\n",
    "            # We set a small stdev for date.\n",
    "\n",
    "    return featurearray, means, stdevs\n",
    "\n",
    "def model_one_volume(data5tuple):\n",
    "    data, classvector, listtoexclude, i, usedate, regularization = data5tuple\n",
    "    trainingset, yvals, testset = sliceframe(data, classvector, listtoexclude, i)\n",
    "    newmodel = LogisticRegression(C = regularization)\n",
    "    trainingset, means, stdevs = normalizearray(trainingset, usedate)\n",
    "    newmodel.fit(trainingset, yvals)\n",
    "\n",
    "    testset = (testset - means) / stdevs\n",
    "    prediction = newmodel.predict_proba(testset.reshape(1, -1))[0][1] #DEFACTOR FIX\n",
    "    if i % 50 == 0:\n",
    "        print(i)\n",
    "    # print(str(i) + \"  -  \" + str(len(listtoexclude)))\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "What is happening here:\n",
    "\n",
    "- iterates over every volume\n",
    "- removes that volume and any other volumes by the same author from the training set\n",
    "- normalizes the training set by computing the z-score for each feature/feature set\n",
    "- fits the model on the z-scores\n",
    "- normalizes the test data by computing the z-score\n",
    "- using the fitted model, predicts the probability of the test data that it is either reviewed or random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### sliceframe()\n",
    "\n",
    "This function prepares the data for training a model for a specific volume. Given all of the data, all of the classifications, a list of volumes to exclude, and the index of the specific volume to model, this function removes the specific volume and the volumes by the same authors (indicated by `excluderows`) from the training data set (because we are holding out one volume to be classified by the logistic regression). This function then returns a training set, a list of classifications, and the held-out volume to-be-classified once the model has been trained. \n",
    "\n",
    "\n",
    "#### normalizearray()\n",
    "\n",
    "This function computes the z-score for each value in the training set. That is, it loops over each each column in the data structure, subtracts the column mean from each value, and then divides that by the standard deviation. \n",
    "\n",
    "\n",
    "Question:\n",
    "- Why is he normalizing the data by computing z-scores?\n",
    "\n",
    "> In training the model we “normalize” word frequencies by the standard deviation for each word (across the whole dataset). So when we use the model to illuminate specific passages, we also divide coefficients by the standard deviation. This tells us, roughly, how much a single occurrence of a given word would affect the model’s prediction, which is what we’re trying to dramatize when we quote a passage. (page 35)\n",
    "\n",
    "\n",
    "The answer appears to have something to do with using the model's coefficients in the interpretation of the effect of individual words.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens when you logistically regress...\n",
    "\n",
    "Our investigation into the mechanisms of logistic regression were driven by the question \"what is the regularization value?\"\n",
    "\n",
    "This lead us spend some time watching [Andrew Ng explain classification using Logistic Regression](https://class.coursera.org/ml-005/lecture) on his Coursera course.\n",
    "\n",
    "At a very high level, logistic regression is a machine learning algorithm for performing classification. Logistic regression works by estimating the parameters of a function, the *hypothesis representation*, that divides a multidimensional space into two parts (note, in this case we are talking about binomial or binary logistic regression, which classifies things into one of two bins). The hypothesis representation describes a line that winds its way through the space creating what is called the *decision boundary.* Every data point that lands on one side the boundary gets one label and every data point on the other side of the boundary gets the other label. Similar to linear regression, the goal is to find the best hypothesis representation, that is, the function that best draws a line that divides the space given your already classified data. Once you have a good hypothesis representation, and appropriately *fit* model, you can begin to classify *new* data.\n",
    "\n",
    "The key to logistic regression is estimating the parameters of the hypothesis representation. We can derive the parameters by using the *features* of existing data combined with their already known labels; this is called *training data*. The modeling process, the function call to `newmodel.fit(trainingset, yvals)` in Ted's code above, uses training data–the matrix of poem features in the `data` variable and known labels ('reviewed' or 'random') in the `classvector` variable–to \"learn\" the parameters through a process called *gradient descent* (note: scikit-learn uses a different process called [*coordinate descent*](http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression) which is exceptionally complex). How gradient descent or the more advanced optimization functions like coordinate descent work are well beyond the scope of the discussion (and our explanatory power) so we will just nod and gesture towards the mathematical magic performed by the `newmodel.fit(trainingset, yvals)` function call.\n",
    "\n",
    "#### Overfitting\n",
    "\n",
    "One of the problems when fitting a logistic regression model is a tendency towards *overfitting*. Crudely this means the model, the function and set of parameters, you estimated have tailored themselves so that they are overly optimized to the data you have. As such, the model becomes less useful for *prediction* or classifying any new data you might encounter. In Ted's case, he is fitting a model based upon all of the poems and their classifications *except one (or a few by the same author)* which he then uses to predict if the *held out* poem was 'reviewed' or 'random.' If he *overfits* the model, it will to a terrible job guessing the status of the held out poem. \n",
    "\n",
    "Regularization is a technique for logistic regression (and other machine learning algorithms) which helps smooth out the tendency toward overfitting with some more mathematical gymnastics that we don't quite have the power to explain with word, but we can explain visually. The diagram below shows how regularization can help with the fitness of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Regularization](notebook_resources/regression_figures.png)\n",
    "\n",
    "*On the left side is a linear regression which doesn't quite fit the data. In the middle is an overfit logistic regression. On right side is a regularized logistic regression.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the diagrams show, the regularized logistic expression (the right side) does have a bit of error, there are pink and blue dots on the wrong sides of the decision boundary, but as more data get added it will generally be more right than the overfitted model as represented by the middle diagram (the squiggly decision boundary). \n",
    "\n",
    "The LinearRegression function of the scikit-learn library allows users to specify a regularization parameter when instantiating a model (`newmodel = LogisticRegression(C = regularization)` in Ted's code). Ted has set the `regularization` parameter to 0.00007. Our question is *why?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "The code cell below is the scaffolding code that runs the modeling function. This code cell takes a long time to execute, it is training a new model for each book. It uses Python's built in parallel processing modules (`Pool(processes = 4)`) to speed up the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning multiprocessing.\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "450\n",
      "500\n",
      "400\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "Multiprocessing concluded.\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool # DEFACTORING ADD\n",
    "\n",
    "# Now do leave-one-out predictions.\n",
    "print('Beginning multiprocessing.')\n",
    "\n",
    "pool = Pool(processes = 4)\n",
    "res = pool.map_async(model_one_volume, sextuplets)\n",
    "\n",
    "# After all files are processed, write metadata, errorlog, and counts of phrases.\n",
    "res.wait()\n",
    "resultlist = res.get()\n",
    "\n",
    "assert len(resultlist) == len(orderedIDs)\n",
    "\n",
    "logisticpredictions = dict()\n",
    "for i, volid in enumerate(orderedIDs):\n",
    "    logisticpredictions[volid] = resultlist[i]\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "print('Multiprocessing concluded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "truepositives = 0\n",
    "truenegatives = 0\n",
    "falsepositives = 0\n",
    "falsenegatives = 0\n",
    "allvolumes = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(outputpath, mode = 'w', encoding = 'utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    header = ['volid', 'reviewed', 'obscure', 'pubdate', 'birthdate', 'gender', 'nation', 'allwords', 'logistic', 'author', 'title', 'pubname', 'actually', 'realclass']\n",
    "    writer.writerow(header)\n",
    "    for volid in IDsToUse:\n",
    "        metadata = metadict[volid]\n",
    "        reviewed = metadata['reviewed']\n",
    "        obscure = metadata['obscure']\n",
    "        pubdate = infer_date(metadata, datetype)\n",
    "        birthdate = metadata['birthdate']\n",
    "        gender = metadata['gender']\n",
    "        nation = metadata['nation']\n",
    "        author = metadata['author']\n",
    "        title = metadata['title']\n",
    "        canonicity = metadata['canonicity']\n",
    "        pubname = metadata['pubname']\n",
    "        allwords = volsizes[volid]\n",
    "        logistic = logisticpredictions[volid]\n",
    "        realclass = classdictionary[volid]\n",
    "        outrow = [volid, reviewed, obscure, pubdate, birthdate, gender, nation, allwords, logistic, author, title, pubname, canonicity, realclass]\n",
    "        writer.writerow(outrow)\n",
    "        allvolumes.append(outrow)\n",
    "\n",
    "        if logistic > 0.5 and classdictionary[volid] > 0.5:\n",
    "            truepositives += 1\n",
    "        elif logistic <= 0.5 and classdictionary[volid] < 0.5:\n",
    "            truenegatives += 1\n",
    "        elif logistic <= 0.5 and classdictionary[volid] > 0.5:\n",
    "            falsenegatives += 1\n",
    "        elif logistic > 0.5 and classdictionary[volid] < 0.5:\n",
    "            falsepositives += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interrogating the model through its coefficients\n",
    "\n",
    "\n",
    "The code below represents a shift in the focal object of the analysis. In the previous section 720 distinct logistic regressions were trained in order to predict the classification of a single, held-out poem. This is the data that was used to produce the main figure, *Figure 1. Predicted probabilities that volumes come from the reviewed set.* \n",
    "\n",
    "The code below generates a single logistic regression model, trained n *all of the data* with nothing held-out (at least when using the 'full' execution path). The reason no data are held out is because this model is not being used for prediction purposes. Instead, the properties of this model are interrogated directly to better understand how individual features, words, had an effect upon the prediction. This is the analysis that allowed them to label individuals worlds as either red or blue based upon their effect when quoting individual passages.\n",
    "\n",
    "\n",
    "This isn't using computational modeling to *predict* a phenomena, it is using the model to *explore* and *explain* patterns and features of the phenomena. This code describes a process where by the model is being deployed for *exploratory data analysis*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "donttrainon.sort(reverse = True)\n",
    "trainingset, yvals, testset = sliceframe(data, classvector, donttrainon, 0)\n",
    "newmodel = LogisticRegression(C = regularization)\n",
    "trainingset, means, stdevs = normalizearray(trainingset, usedate)\n",
    "newmodel.fit(trainingset, yvals)\n",
    "\n",
    "coefficients = newmodel.coef_[0] * 100\n",
    "\n",
    "coefficientuples = list(zip(coefficients, (coefficients / np.array(stdevs)), vocablist + ['pub.date']))\n",
    "coefficientuples.sort()\n",
    "if verbose:\n",
    "    for coefficient, normalizedcoef, word in coefficientuples:\n",
    "        print(word + \" :  \" + str(coefficient))\n",
    "\n",
    "print()\n",
    "accuracy = (truepositives + truenegatives) / len(IDsToUse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to above, we don't quite understand is why Ted is normalizing the coefficients. What we don't know is what set of values, the coefficients or the normalized coefficients he is using when he highlights words as either red or blue in the narrative.\n",
    "\n",
    "> In training the model we “normalize” word frequencies by the standard deviation for each word (across the whole dataset). So when we use the model to illuminate specific passages, we also divide coefficients by the standard deviation. This tells us, roughly, how much a single occurrence of a given word would affect the model’s prediction, which is what we’re trying to dramatize when we quote a passage. (page 35)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3200"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEFACTORING\n",
    "len(newmodel.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### Writing word coefficients to disk\n",
    "\n",
    "the code below generates the `mainmodelcoefficients.csv` which contains the word, its coefficient and its normalized coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coefficientpath = outputpath.replace('.csv', '.coefs.csv')\n",
    "with open(coefficientpath, mode = 'w', encoding = 'utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for triple in coefficientuples:\n",
    "        coef, normalizedcoef, word = triple\n",
    "        writer.writerow([word, coef, normalizedcoef])\n",
    "\n",
    "rawaccuracy = accuracy #DEFACTOR\n",
    "#return accuracy, allvolumes, coefficientuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## popping out of create_model back replicate.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final function of the analysis is to test the accuracy of the model(s). The code below generates a best fit line, using [numpy.polyfit](http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.polyfit.html), for all of the results. He then calculates which are predicted-as-reviewed, which is those that fall above this dividing line\n",
    "\n",
    "The accuracy of the dividing line is computed by the total number of predicted-as-reviewed that were actually reviewed divided by the total.\n",
    "\n",
    "Note, this is the accuracy of the dividing line, not the accuracy of the model(s) to predict the reviewed stats of the poems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # DEFACTORING\n",
    "%matplotlib inline\n",
    "\n",
    "def diachronic_tilt(allvolumes, modeltype, datelimits):\n",
    "    ''' Takes a set of predictions produced by a model that knows nothing about date,\n",
    "    and divides it along a line with a diachronic tilt. We need to do this in a way\n",
    "    that doesn't violate crossvalidation. I.e., we shouldn't \"know\" anything\n",
    "    that the model didn't know. We tried a couple of different ways to do this, but\n",
    "    the simplest and actually most reliable is to divide the whole dataset along a\n",
    "    linear central trend line for the data!\n",
    "    '''\n",
    "\n",
    "    listofrows = list()\n",
    "    classvector = list()\n",
    "\n",
    "    # DEPRECATED\n",
    "    # if modeltype == 'logistic' and len(datelimits) == 2:\n",
    "    #     # In this case we construct a subset of data to model on.\n",
    "    #     tomodeldata = list()\n",
    "    #     tomodelclasses = list()\n",
    "    #     pastthreshold, futurethreshold = datelimits\n",
    "\n",
    "    for volume in allvolumes:\n",
    "        date = volume[3]\n",
    "        logistic = volume[8]\n",
    "        realclass = volume[13]\n",
    "        listofrows.append([logistic, date])\n",
    "        classvector.append(realclass)\n",
    "\n",
    "        # DEPRECATED\n",
    "        # if modeltype == 'logistic' and len(datelimits) == 2:\n",
    "        #     if date >= pastthreshold and date <= futurethreshold:\n",
    "        #         tomodeldata.append([logistic, date])\n",
    "        #         tomodelclasses.append(realclass)\n",
    "\n",
    "    y, x = [a for a in zip(*listofrows)]\n",
    "    plt.axis([min(x) - 2, max(x) + 2, min(y) - 0.02, max(y) + 0.02])\n",
    "    reviewedx = list()\n",
    "    reviewedy = list()\n",
    "    randomx = list()\n",
    "    randomy = list()\n",
    "\n",
    "    for idx, reviewcode in enumerate(classvector):\n",
    "        if reviewcode == 1:\n",
    "            reviewedx.append(x[idx])\n",
    "            reviewedy.append(y[idx])\n",
    "        else:\n",
    "            randomx.append(x[idx])\n",
    "            randomy.append(y[idx])\n",
    "\n",
    "    plt.plot(reviewedx, reviewedy, 'ro')\n",
    "    plt.plot(randomx, randomy, 'k+')\n",
    "\n",
    "    if modeltype == 'logistic':\n",
    "        # all this is DEPRECATED\n",
    "        print(\"Hey, you're attempting to use the logistic-tilt option\")\n",
    "        print(\"that we deactivated. Go in and uncomment the code.\")\n",
    "\n",
    "        # if len(datelimits) == 2:\n",
    "        #     data = pd.DataFrame(tomodeldata)\n",
    "        #     responsevariable = tomodelclasses\n",
    "        # else:\n",
    "        #     data = pd.DataFrame(listofrows)\n",
    "        #     responsevariable = classvector\n",
    "\n",
    "        # newmodel = LogisticRegression(C = 100000)\n",
    "        # newmodel.fit(data, responsevariable)\n",
    "        # coefficients = newmodel.coef_[0]\n",
    "\n",
    "        # intercept = newmodel.intercept_[0] / (-coefficients[0])\n",
    "        # slope = coefficients[1] / (-coefficients[0])\n",
    "\n",
    "        # p = np.poly1d([slope, intercept])\n",
    "\n",
    "    elif modeltype == 'linear':\n",
    "        # what we actually do\n",
    "\n",
    "        z = np.polyfit(x, y, 1)\n",
    "        p = np.poly1d(z)\n",
    "        slope = z[0]\n",
    "        intercept = z[1]\n",
    "\n",
    "    plt.plot(x,p(x),\"b-\")\n",
    "    plt.show(block = False)\n",
    "\n",
    "    x = np.array(x, dtype='float64')\n",
    "    y = np.array(y, dtype='float64')\n",
    "    classvector = np.array(classvector)\n",
    "    dividingline = intercept + (x * slope)\n",
    "    predicted_as_reviewed = (y > dividingline)\n",
    "    really_reviewed = (classvector == 1)\n",
    "\n",
    "    accuracy = sum(predicted_as_reviewed == really_reviewed) / len(classvector)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD7CAYAAABgzo9kAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noJCnUAReQUFBIU+hCLyCgoJCnuL/BzVaRcSNhw32AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x105abb048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If we divide the dataset with a horizontal line at 0.5, accuracy is:  0.7736111111111111\n",
      "Divided with a line fit to the data trend, it's  0.791666666667\n"
     ]
    }
   ],
   "source": [
    "tiltaccuracy = diachronic_tilt(allvolumes, 'linear', [])\n",
    "\n",
    "print('If we divide the dataset with a horizontal line at 0.5, accuracy is: ', str(rawaccuracy))\n",
    "\n",
    "print(\"Divided with a line fit to the data trend, it's \", str(tiltaccuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "This notebook contains all of the code necessary to run through a full execution of the computational analysis.\n",
    "\n",
    "\n",
    "Open questions for Ted:\n",
    "- why does he normalize the word frequences (using `normalizearray()`) before modeling?\n",
    "- Why does he normalize the coefficients (by divided by standard deviation)?\n",
    "\n",
    "Basically, we don't have a firm understanding of why he is normalizing, mainly because we don't understand the meaning or significance of these steps on the interpretibility of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### TODO: We want to infer a 'standard' set of practices when defactoring\n",
    "\n",
    "  - We chose 1 execution path, 1 namespace\n",
    "  - To do this we broke up the code into various pieces that expressed that one execution path\n",
    "  - We inspected and explicitly reported the values as a form of reporting 'state' of the execution\n",
    "  - We DEFACTORED (refactored for single namespace execution) the code at places- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "widgets": {
   "state": {},
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
