{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So what did we do?\n",
    "\n",
    "At a practical level, we have *forked* code from the Pace of Change [git repository](https://github.com/tedunderwood/paceofchange) and created a new *branch* containing the Jupyter Notebook you are reading now. Transforming Underwood and Seller's code from a set of python script into the notebook involved a series of transformation to their code. The provenance of this effort has been preserved because we did all of our work in the the fork of their version repository, visible via the `git history` command.\n",
    "\n",
    "Underwood and Sellers' have crafted their code in such a way that it is possible to replicate their results by running a single command. One of our first tasks was to trace the code and follow the path of execution from the initial command to the output data files. In reading through the code (spanning across multiple python scripts) we could piece together a rough idea of how Underwood and Sellers' performed their data preparation, normalization, and analysis. While we could read their python source code, plain text files leave much to be desired with trying to critically engage code. This is why we made the effort to translate their scripts into a Jupyter Notebook.\n",
    "\n",
    "This process of translation involved copying bits of code from various `.py` files into a single executable notebook.  Not all of the code in the git repository was copied into the notebook for three main reasons. First, not all of the code included in the files was actually needed to replicate the analysis process. The repository includes extra code, one could call it \"cruft\", from Underwood and Seller's exploratory analysis or earlier iterations of the analytical process. For example, the file `SonicScrewdiver.py` is never called upon although, based upon the name, we might hypothesize it is an important catch-all module for a variety of different tasks. Other sections of the code are commented out (such as `binormal_select` discussed below) and never executed. As part of the defactoring process, we opted to not include unused functions or code blocks. The second reason for not include could was not all of the possible execution paths are represented in this Notebook. There six allowable options for slicing the data in `replicate.py`, the entry-point for re-running the analysis. In this notebook we follow the default, \"full\", which models the entire dataset. The third reason we did not include some code was because of the use of external libraries. While most of the scripts are bespoke code for working with unique data, some sections of the code lik the logistic regression are part of a third-party library, `scikit-learn`, that for a multitude of reasons (including practicality) we decided to \"step over\" and not defactor. When trying to follow the path of execution for any program one needs to recognize it is \"[turtles all the way down](https://en.wikipedia.org/wiki/Turtles_all_the_way_down).\" To make our task possible, we have to make a decision about how far down the path of execution the defactoring process will go. We make a decision to focus only on the code written by Underwood and Sellers and leave an analysis of subsequent third-party libraries to a later, and more ambitious, defactoring effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Defactoring code from python files to the notebook](notebook_resources/defactoring.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Underwood and Seller's code is stored within four files, `replicate.py`, `parallel_crossvalidate.py`, `metafilter.py`, and `modlingprocess.py`. Through the process of *defactoring* we have copied|move|transferred|imported code from text files to a Jupyter Notebook. This process has transformed the flow of the code, but not the outputs. \n",
    "\n",
    "In order for the code to execute seamlessly within the notebook, we had to make minor changes and tweaks to the code. These changes fall into 5 categories:\n",
    "\n",
    "* *defactoring functions* - This is the most significant of the changes. When we defactor a function we take the function's code and move it to the global namespace. This has the effect of elimiating the function and just making it part of the main execution path. \n",
    "* *defactoring function calls* - When a function has been defactored, it can no longer be called since there is no explicit definitional code.\n",
    "* *defactoring definitions* - Not all functions can be fully defactored. Functions that are called more than once or those that are short have been kept as re-usable functions. Defactoring Definition cells define the functions above the code cells that use them (preventing errors).\n",
    "* *defactoring namespace* - Because we have defactored some of the functions and their function calls some of the variables in the namespace need to be mapped to eachother. This happens we the return value of a defactored function needs to be stored in a differently named variable.\n",
    "* *defactoring inspections* - When we want to inspect the state of the process, we insert an inspection cell that prints the values of the variables of interest.\n",
    "* *defactoring import* - Because the code is reliant upon external and third party functions, we need to import that code into the global namespace. This cell contains all of those imports.\n",
    "\n",
    "\n",
    "One of the advantages to defactoring a function is it affords us the ability to insert critical commentary (in the form of markdown cells) into the code constituting the function itself. An unfortunate side-effect is that this makes keeping track of one's place in the code a bit difficult. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diving into the Code\n",
    "\n",
    "\n",
    "\n",
    "The code expressed below has nine steps:\n",
    "* [Setting Parameters](#Setting-Parameters) - Specifies parameters for the loading, cleaning, and labeling of data as well as sets conditions for the logistic regression.\n",
    "* [Preparing Metadata](#Preparing-MetaData) - Generates a list of *.tsv files from the `poems/` directory. \n",
    "    * [Cleaning Metadata](#Cleaning-Metadata) - Loads the metadata file, `poemetadata.csv` and performs some cleaning of the metadata to make labeling easier.\n",
    "    * [Sorting Data](#Sorting-Data) - Sort the volumes into two bins, reviewed and not reviewed using the cleaned metadata.\n",
    "* [Transforming Words into Features](#Transforming-Words-into-Features) - Identifies the 3,200 most common words in the corpus. Those most common words will be the features for the regression.\n",
    "    * [Filtering Authors](#Filtering-Authors) - Removes poems by authors who have been reviewed.\n",
    "    * [Filtering Words](#Filtering-Words) - Remove any words from the poem data that are not in the most-common feature list.\n",
    "* [Training Predictive Models](#Training-Predictive-Models) - Run a separate logistic regression for each volume, using a single volume as held-out data and measure each model's predictive power.\n",
    "* [Modeling Coefficients](#Modeling-Coefficients) - Run a single logistic regression over all the data to inspect the salient coefficients.\n",
    "* [Saving Output](#Saving-Output) - Save the results of the predictions and the coefficents to disk as CSV files.\n",
    "* [Plotting Results](#Plotting Results) - Generate a plot showing the accuracy of the predictive models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to read this notebook\n",
    "\n",
    "This notebook is divided into a series of cells, *code cells* and *text cells*. Code cells contain python code mostly written by Underwood and Selles, but with a few modificaitons by us. We have broken up their code and inserted text cells, written by us, which discuss and explain what is happening in the code cells. In most cases the expository text is *below* the code cell.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### DEFACTORING IMPORT\n",
    "import os\n",
    "import csv \n",
    "import random \n",
    "from collections import Counter \n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from multiprocessing import Pool \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note to the reader. If you see a red warning box don't fret, this is a by-product of our effort to Dockerize this analysis. Hopefully this will someday go away.*\n",
    "\n",
    "We begin this journey into code by importing a series of python libraries into working memory. Some of these libraries are part of the standard library–`os`, `csv`, `random,` and `multiprocesing`–but other libraries are third-party–`numpy`, `pandas`, and most importantly `sklearn`. These import statements tell us what libraries the authors will be relying upon for their analysis. The last line imports the `LogisticRegression` implementation from the [scikit-learn library](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). The authors are [standing on the shoulders of giants](https://en.wikipedia.org/wiki/Standing_on_the_shoulders_of_giants) to use this popular python implementation of logistic regression to conduct their analysis. Another interesting import statement is the `Pool` function from the python `multiprocessing` library. Normally python operates as a [single-threaded process](https://en.wikipedia.org/wiki/Global_interpreter_lock), which means it requires some extra work to do parallel processing. This import statement enables the use of a [worker pool](https://docs.python.org/2/library/multiprocessing.html#using-a-pool-of-workers) to execute code in parallel. It is important to recognize these two libraries as they will play an important role in the analysis below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Parameters\n",
    "\n",
    "The first section of the code sets a series of parameters specifying what data to process, where the data are located, and parameters for the logistic regression. While there is no complex logic or work being done in this section, many assumptions and important distinctions that shape the execution of subsequent code are defined here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PATHS.\n",
    "\n",
    "sourcefolder = 'poems/'\n",
    "extension = '.poe.tsv'\n",
    "classpath = 'poemeta.csv'\n",
    "outputpath = 'mainmodelpredictions.csv'\n",
    "\n",
    "## EXCLUSIONS.\n",
    "\n",
    "excludeif = dict()\n",
    "excludeif['pubname'] = 'TEM'\n",
    "# We're not using reviews from Tait's.\n",
    "\n",
    "excludeif['recept'] = 'addcanon'\n",
    "# We don't ordinarily include canonical volumes that were not in either sample.\n",
    "# These are included only if we're testing the canon specifically.\n",
    "\n",
    "excludeifnot = dict()\n",
    "excludeabove = dict()\n",
    "excludebelow = dict()\n",
    "\n",
    "excludebelow['firstpub'] = 1700\n",
    "excludeabove['firstpub'] = 1950\n",
    "sizecap = 360\n",
    "\n",
    "# For more historically-interesting kinds of questions, we can limit the part\n",
    "# of the dataset that gets TRAINED on, while permitting the whole dataset to\n",
    "# be PREDICTED. (Note that we always exclude authors from their own training\n",
    "# set; this is in addition to that.) The variables futurethreshold and\n",
    "# pastthreshold set the chronological limits of the training set, inclusive\n",
    "# of the threshold itself.\n",
    "\n",
    "## THRESHOLDS\n",
    "\n",
    "futurethreshold = 1925\n",
    "pastthreshold = 1800\n",
    "\n",
    "# CLASSIFY CONDITIONS\n",
    "\n",
    "positive_class = 'rev'\n",
    "category2sorton = 'reviewed'\n",
    "datetype = 'firstpub'\n",
    "numfeatures = 3200\n",
    "regularization = .00007\n",
    "\n",
    "\n",
    "paths = (sourcefolder, extension, classpath, outputpath)\n",
    "exclusions = (excludeif, \n",
    "              excludeifnot, \n",
    "              excludebelow, \n",
    "              excludeabove, \n",
    "              sizecap)\n",
    "thresholds = (pastthreshold, \n",
    "              futurethreshold)\n",
    "classifyconditions = (category2sorton, \n",
    "                      positive_class, \n",
    "                      datetype, \n",
    "                      numfeatures, \n",
    "                      regularization)\n",
    "\n",
    "### DEFACTORING FUNCTION CALL\n",
    "### rawaccuracy, allvolumes, coefficientuples = pc.create_model(paths, exclusions, thresholds, classifyconditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters defined in the code cell above are a set of knobs and switches used to tweak the performance and execution of the computational modeling process below. There isn't any complex or hard-to-discern logic in the code above, it is mainly setting the parameters of a set of variables. Interestingly the authors have grouped the individual variables into four categories,  *paths*, *exclusions*, *thresholds*, and *classifyconditions*. These categories are simultaneously distingushed discursively through the code comments (the lines beginning with a `#`) and technologically through the four variables  `paths`, `exclusions`, `thresholds`, and `classifyconditions`. Each of these four variables, represented as [tuples](https://docs.python.org/3/tutorial/datastructures.html#tuples-and-sequences), embody stylistic choices of the authors as a means of organizing and structuring the information they are encoding in python.      \n",
    "\n",
    "The variables in `paths` specify the location of the data and metadata files as well as where to write the output files at the completetion of the analysis. The variables in `exclusions` specify data and types of data to be excluded from the analysis, such as reviews from [Tait's Endinburgh Magazine](https://en.wikipedia.org/wiki/Tait%27s_Edinburgh_Magazine), which we suppose from the author's comments. Additional exclusions specify temporal boundaries from 1700 to 1950. An additiona set of two variables in `thresholds` also articulate a temporal boundary from 1800 to 1925. The authors comments indicate this distinguishes the temporal window for datasets using in *training* versus *prediction.* The variables in `classifyconditions` are important parameters for the logistic regression, specifying the number of variables to train the model upon as well as setting the regularization parameter (`regularization`) for the logistic regression. What is not well documented here, is why the value .00007 was chosen over other values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Metadata\n",
    "\n",
    "This section of the code is where we begin to see some logical work being conducted. The code in this section has two subsections that clean the metadata and sort the training data. All of the work in this focuses on preparing the metadata, stored in the `classpath` variable and the filenames of the individual data files in the `sourcefolder`. The main task of this section is to gather the set of identifiers and their associated labels (positive or negative) for training the logistic regression. All of the code in this section attends to the cleanlyness of the metadata, we won't start digging into the data itself until the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### DEFACTORING FUNCTION\n",
    "### def create_model(paths, exclusions, thresholds, classifyconditions):\n",
    "''' This is the main function in the module.\n",
    "It can be called externally; it's also called\n",
    "if the module is run directly.\n",
    "'''\n",
    "verbose = False\n",
    "\n",
    "if not sourcefolder.endswith('/'):\n",
    "    sourcefolder = sourcefolder + '/'\n",
    "\n",
    "# This just makes things easier.\n",
    "\n",
    "# Get a list of files.\n",
    "allthefiles = os.listdir(sourcefolder)\n",
    "# random.shuffle(allthefiles)\n",
    "\n",
    "volumeIDs = list()\n",
    "volumepaths = list()\n",
    "\n",
    "for filename in allthefiles:\n",
    "\n",
    "    if filename.endswith(extension):\n",
    "        volID = filename.replace(extension, \"\")\n",
    "        # The volume ID is basically the filename minus its extension.\n",
    "        # Extensions are likely to be long enough that there is little\n",
    "        # danger of accidental occurrence inside a filename. E.g.\n",
    "        # '.fic.tsv'\n",
    "        path = sourcefolder + filename\n",
    "        volumeIDs.append(volID)\n",
    "        volumepaths.append(path)\n",
    "        \n",
    "### DEFACTORING FUNCTION CALL\n",
    "### metadict = metafilter.get_metadata(classpath, volumeIDs, excludeif, excludeifnot, excludebelow, excludeabove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first item in volumeIDs is:  dul1.ark+=13960=t5fb5xg2z\n",
      "The first item in volumepaths is:  poems/dul1.ark+=13960=t5fb5xg2z.poe.tsv\n"
     ]
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "### Inspect the two variables defined in the codecell above.\n",
    "### We know they are lists so lets just look at the first item.\n",
    "print(\"The first item in volumeIDs is: \", volumeIDs[0])\n",
    "print(\"The first item in volumepaths is: \",volumepaths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code assembles a list of volume identifiers (`volumeIDs`) and file paths (`volumepaths`) by readings the directory listing of files in the `poems/` directory (`sourcefolder`). The filenames are in and of themselves a source of metadata, but as we see in the code below, they need to be reconciled with the metadata stored separately from the data files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### DEFACTORING FUNCTION DEFINITION\n",
    "### we need these helper functions for execute the next code cell\n",
    "\n",
    "def dirty_pairtree(htid):\n",
    "    period = htid.find('.')\n",
    "    prefix = htid[0:period]\n",
    "    postfix = htid[(period+1): ]\n",
    "    if '=' in postfix:\n",
    "        postfix = postfix.replace('+',':')\n",
    "        postfix = postfix.replace('=','/')\n",
    "    dirtyname = prefix + \".\" + postfix\n",
    "    return dirtyname\n",
    "\n",
    "def forceint(astring):\n",
    "    try:\n",
    "        intval = int(astring)\n",
    "    except:\n",
    "        intval = 0\n",
    "\n",
    "    return intval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code cell defines two functions used in the code below. The first is `dirty_pairtree()`, which cleans up the identifiers in the data. The issue stems from the fact that the HathiTrust uses IDs that cannot be expressed on the filesystem, the \"/\" and \":\" characters cannot be expressed on the file system so because the volumes are stored as individual files they have a \"+\" and an \"=\" instead. However, the IDs are stored in the original format in the metadata file so the IDS have to be transformed back into the orginal HathiTrust format. The `forceint()` function transforms integer values expressed as python strings into the python integer data type with a it of error handling in the case of zero values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poemeta.csv\n",
      "DEFACTORING: Excluding volume with id loc.ark:/13960/t8sb4zz1q\n",
      "DEFACTORING: Excluding volume with id mdp.39015013402501\n",
      "DEFACTORING: Excluding volume with id mdp.39015011913525\n",
      "DEFACTORING: Excluding volume with id hardywessexpoems189.hardywessexpoems1898\n",
      "DEFACTORING: Excluding volume with id gerardmhopkins191.gerardmhopkins1918\n",
      "DEFACTORING: Excluding volume with id loc.ark:/13960/t3fx82c2q\n",
      "DEFACTORING: Excluding volume with id emilydickinso.emilydickinson\n",
      "DEFACTORING: Excluding volume with id ellisbell184.ellisbell1848\n",
      "We have 8 volumes in missing in metadata, and\n",
      "0 volumes missing in the directory.\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "### DEFACTORING FUNCTION \n",
    "### def get_metadata(classpath, volumeIDs, excludeif, excludeifnot, excludebelow, excludeabove):\n",
    "'''\n",
    "As the name would imply, this gets metadata matching a given set of volume\n",
    "IDs. It returns a dictionary containing only those volumes that were present\n",
    "both in metadata and in the data folder.\n",
    "\n",
    "It also accepts four dictionaries containing criteria that will exclude volumes\n",
    "from the modeling process.\n",
    "'''\n",
    "print(classpath)\n",
    "metadict = dict()\n",
    "\n",
    "with open(classpath, encoding = 'utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "\n",
    "    anonctr = 0\n",
    "\n",
    "    for row in reader:\n",
    "        volid = dirty_pairtree(row['docid'])\n",
    "        theclass = row['recept'].strip()\n",
    "\n",
    "        # I've put 'remove' in the reception column for certain\n",
    "        # things that are anomalous.\n",
    "        if theclass == 'remove':\n",
    "            continue\n",
    "\n",
    "        bail = False\n",
    "        for key, value in excludeif.items():\n",
    "            if row[key] == value:\n",
    "                bail = True\n",
    "        for key, value in excludeifnot.items():\n",
    "            if row[key] != value:\n",
    "                bail = True\n",
    "        for key, value in excludebelow.items():\n",
    "            if forceint(row[key]) < value:\n",
    "                bail = True\n",
    "        for key, value in excludeabove.items():\n",
    "            if forceint(row[key]) > value:\n",
    "                bail = True\n",
    "\n",
    "        if bail:\n",
    "            print(\"DEFACTORING: Excluding volume with id \"+volid) ### DEFACTORING CODE\n",
    "            continue\n",
    "\n",
    "        birthdate = forceint(row['birth'])\n",
    "\n",
    "        pubdate = forceint(row['inferreddate'])\n",
    "\n",
    "        gender = row['gender'].rstrip()\n",
    "        nation = row['nationality'].rstrip()\n",
    "\n",
    "        #if pubdate >= 1880:\n",
    "            #continue\n",
    "\n",
    "        if nation == 'ca':\n",
    "            nation = 'us'\n",
    "        elif nation == 'ir':\n",
    "            nation = 'uk'\n",
    "        # I hope none of my Canadian or Irish friends notice this.\n",
    "\n",
    "        notes = row['notes'].lower()\n",
    "        author = row['author']\n",
    "        if len(author) < 1 or author == '<blank>':\n",
    "            author = \"anonymous\" + str(anonctr)\n",
    "            anonctr += 1\n",
    "\n",
    "        title = row['title']\n",
    "        canon = row['canon']\n",
    "\n",
    "        # I'm creating two distinct columns to indicate kinds of\n",
    "        # literary distinction. The reviewed column is based purely\n",
    "        # on the question of whether this work was in fact in our\n",
    "        # sample of contemporaneous reviews. The obscure column incorporates\n",
    "        # information from post-hoc biographies, which trumps\n",
    "        # the question of reviewing when they conflict.\n",
    "\n",
    "        if theclass == 'random':\n",
    "            obscure = 'obscure'\n",
    "            reviewed = 'not'\n",
    "        elif theclass == 'reviewed':\n",
    "            obscure = 'known'\n",
    "            reviewed = 'rev'\n",
    "        elif theclass == 'addcanon':\n",
    "            print(\"DEFACTORING: adding volume\") ### DEFACTORING CODE\n",
    "            obscure = 'known'\n",
    "            reviewed = 'addedbecausecanon'\n",
    "        else:\n",
    "            print(\"Missing class\" + theclass)\n",
    "\n",
    "        if notes == 'well-known':\n",
    "            obscure = 'known'\n",
    "        if notes == 'obscure':\n",
    "            obscure = 'obscure'\n",
    "\n",
    "        if canon == 'y':\n",
    "            if theclass == 'addcanon':\n",
    "                actually = 'Norton, added'\n",
    "            else:\n",
    "                actually = 'Norton, in-set'\n",
    "        elif reviewed == 'rev':\n",
    "            actually = 'reviewed'\n",
    "        else:\n",
    "            actually = 'random'\n",
    "\n",
    "        metadict[volid] = dict()\n",
    "        metadict[volid]['reviewed'] = reviewed\n",
    "        metadict[volid]['obscure'] = obscure\n",
    "        metadict[volid]['pubdate'] = pubdate\n",
    "        metadict[volid]['birthdate'] = birthdate\n",
    "        metadict[volid]['gender'] = gender\n",
    "        metadict[volid]['nation'] = nation\n",
    "        metadict[volid]['author'] = author\n",
    "        metadict[volid]['title'] = title\n",
    "        metadict[volid]['canonicity'] = actually\n",
    "        metadict[volid]['pubname'] = row['pubname']\n",
    "        metadict[volid]['firstpub'] = forceint(row['firstpub'])\n",
    "\n",
    "# These come in as dirty pairtree; we need to make them clean.\n",
    "\n",
    "cleanmetadict = dict()\n",
    "allidsinmeta = set([x for x in metadict.keys()])\n",
    "allidsindir = set([dirty_pairtree(x) for x in volumeIDs])\n",
    "missinginmeta = len(allidsindir - allidsinmeta)\n",
    "missingindir = len(allidsinmeta - allidsindir)\n",
    "print(\"We have \" \n",
    "      + str(missinginmeta) \n",
    "      + \" volumes in missing in metadata, and\")\n",
    "print(str(missingindir) + \" volumes missing in the directory.\")\n",
    "print(allidsinmeta - allidsindir)\n",
    "\n",
    "for anid in volumeIDs:\n",
    "    dirtyid = dirty_pairtree(anid)\n",
    "    if dirtyid in metadict:\n",
    "        cleanmetadict[anid] = metadict[dirtyid]\n",
    "\n",
    "# Now that we have a list of volumes with metadata, we can select the groups of IDs\n",
    "# that we actually intend to contrast. If we want to us more or less everything,\n",
    "# this may not be necessary. But in some cases we want to use randomly sampled subsets.\n",
    "\n",
    "# The default condition here is\n",
    "\n",
    "# category2sorton = 'reviewed'\n",
    "# positive_class = 'rev'\n",
    "# sizecap = 350\n",
    "# A sizecap less than one means, no sizecap.\n",
    "\n",
    "### DEFACTORING FUNCTION CALL\n",
    "### IDsToUse, classdictionary = metafilter.label_classes(metadict, category2sorton, positive_class, sizecap)\n",
    "\n",
    "### DEFACTORING NAMESPACE \n",
    "metadict = cleanmetadict  # put the data into the global namespace so execution can continue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output immediately above is a mixture of the author's code and our own DEFACTORING inspection statements. We have added a `print` statement so we can see the IDs of the volumes being excluded in the code.\n",
    "\n",
    "The structure of the code this code cell is large due to the `for` loop processing the metadata. At a high level, the code in this cell is loading the metadata and determining what volumes to exclude in the analysis. It does this by loading the `poemeta.csv` file and excluding rows based upon the parameters specified in the  `excludeif`, `excludeifnot`, `excludeabove`, and `excludebelow` variables.\n",
    "\n",
    "This code also normalizes some of the `nation` data, which is a pretty clinical way saying they lump Canada with the United States and Ireland with the UK. Nationality is not a factor in the Pace of Change analysis, but it is interesting to see this code here, it implies this code was used in other explorations of the data.\n",
    "\n",
    "Additionally, this code cell splits the `recept` column of the metadata file into two columns, `obscure` and `reviewed`. From what we can tell from the code and the author's comments, there poems that are reviewed, there are poems that are obscure, and there are poems that are not in the reviewed set but are never-the-less part of the cannon. This means they are \"known\" and, according to the authors's comment, trumps the conflict when the author is known (`obscure = 'known'`) but not explicitly in the reviewed set. \n",
    "\n",
    "We have discovered after adding a `### DEFACTORING CODE` snippet that this code never actually runs. All of the poems with the `addcanon` property are tossed out and the conflict, where the poem is known by in the random set, never appears to occur. Conjector: is this a remnant of the author's refactoring the code due to changes in the analysis process or just working with different data or something we cannot possible conceive. What was the author's intent? \n",
    "\n",
    "We know that poems with the `addcanon` in the `recept` column are being excluded because they are included in the `excludeif` dictionary. Why? The code in the code cell above in the [setting parameters section](#setting-parameters) provides somewhat of an explanation:\n",
    "\n",
    "```\n",
    "excludeif['recept'] = 'addcanon'\n",
    "# We don't ordinarily include canonical volumes that were not in either sample.\n",
    "# These are included only if we're testing the canon specifically.\n",
    "```\n",
    "\n",
    "It should noted we spent a considerable amount of time interpreting the code that handled this particular situation before realizing that it would never be executed because of the settings in the `excludeif` dictionary. That makes us look stupid, but we also now have a more intimate understanding and relationship with the code. or maybe we are still stupid.\n",
    "\n",
    "The main thing this code cell produces is a data dictionary, `cleanmetadict`, of the cleaned and filtered metadata. Let's take a closer look at the raw data from the csv file and the transformed data stored in `cleanmetadict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>actualdate</th>\n",
       "      <th>inferreddate</th>\n",
       "      <th>firstpub</th>\n",
       "      <th>recept</th>\n",
       "      <th>recordid</th>\n",
       "      <th>OCLC</th>\n",
       "      <th>author</th>\n",
       "      <th>imprint</th>\n",
       "      <th>enumcron</th>\n",
       "      <th>...</th>\n",
       "      <th>judge</th>\n",
       "      <th>impaud</th>\n",
       "      <th>yrrev</th>\n",
       "      <th>pubname</th>\n",
       "      <th>birth</th>\n",
       "      <th>gender</th>\n",
       "      <th>nationality</th>\n",
       "      <th>othername</th>\n",
       "      <th>notes</th>\n",
       "      <th>canon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>loc.ark+=13960=t8sb4zz1q</td>\n",
       "      <td>1921</td>\n",
       "      <td>1921</td>\n",
       "      <td>1921</td>\n",
       "      <td>addcanon</td>\n",
       "      <td>537314.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lawrence, D. H.</td>\n",
       "      <td>New York;T. Seltzer;1921.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1885</td>\n",
       "      <td>m</td>\n",
       "      <td>uk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uc1.b3342759</td>\n",
       "      <td>1919</td>\n",
       "      <td>1919</td>\n",
       "      <td>1919</td>\n",
       "      <td>random</td>\n",
       "      <td>7930862.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wigren, Bessie C.</td>\n",
       "      <td>Boston;The Poet Lore Company;c1919</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1874</td>\n",
       "      <td>f</td>\n",
       "      <td>us</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uc1.b4100590</td>\n",
       "      <td>1918</td>\n",
       "      <td>1918</td>\n",
       "      <td>1918</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>6154122.0</td>\n",
       "      <td>2143179.0</td>\n",
       "      <td>Waugh, Alec,</td>\n",
       "      <td>London;G. Richards;1918.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1918.0</td>\n",
       "      <td>EGO</td>\n",
       "      <td>1898</td>\n",
       "      <td>m</td>\n",
       "      <td>uk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uc1.b3340220</td>\n",
       "      <td>1918</td>\n",
       "      <td>1918</td>\n",
       "      <td>1918</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>7917249.0</td>\n",
       "      <td>12688503.0</td>\n",
       "      <td>Nightingale, M.</td>\n",
       "      <td>Oxford [Oxfordshire;B.H. Blackwell;1918.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>neg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1919.0</td>\n",
       "      <td>EGO</td>\n",
       "      <td>1879</td>\n",
       "      <td>f</td>\n",
       "      <td>uk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>uc2.ark+=13960=t0ft8gj1k</td>\n",
       "      <td>1918</td>\n",
       "      <td>1918</td>\n",
       "      <td>1918</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>7657411.0</td>\n",
       "      <td>2518108.0</td>\n",
       "      <td>Faber, Geoffrey,</td>\n",
       "      <td>Oxford;B. H. Blackwell;New York;Longmans, Gree...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1918.0</td>\n",
       "      <td>EGO</td>\n",
       "      <td>1889</td>\n",
       "      <td>m</td>\n",
       "      <td>uk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      docid actualdate  inferreddate  firstpub    recept  \\\n",
       "0  loc.ark+=13960=t8sb4zz1q       1921          1921      1921  addcanon   \n",
       "1              uc1.b3342759       1919          1919      1919    random   \n",
       "2              uc1.b4100590       1918          1918      1918  reviewed   \n",
       "3              uc1.b3340220       1918          1918      1918  reviewed   \n",
       "4  uc2.ark+=13960=t0ft8gj1k       1918          1918      1918  reviewed   \n",
       "\n",
       "    recordid        OCLC             author  \\\n",
       "0   537314.0         NaN    Lawrence, D. H.   \n",
       "1  7930862.0         NaN  Wigren, Bessie C.   \n",
       "2  6154122.0   2143179.0       Waugh, Alec,   \n",
       "3  7917249.0  12688503.0    Nightingale, M.   \n",
       "4  7657411.0   2518108.0   Faber, Geoffrey,   \n",
       "\n",
       "                                             imprint enumcron  ...  judge  \\\n",
       "0                          New York;T. Seltzer;1921.      NaN  ...    NaN   \n",
       "1                 Boston;The Poet Lore Company;c1919      NaN  ...    NaN   \n",
       "2                           London;G. Richards;1918.      NaN  ...    NaN   \n",
       "3           Oxford [Oxfordshire;B.H. Blackwell;1918.      NaN  ...    neg   \n",
       "4  Oxford;B. H. Blackwell;New York;Longmans, Gree...      NaN  ...    NaN   \n",
       "\n",
       "  impaud   yrrev pubname  birth gender nationality othername notes canon  \n",
       "0    NaN     NaN     NaN   1885      m          uk       NaN   NaN     y  \n",
       "1    NaN     NaN     NaN   1874      f          us       NaN   NaN   NaN  \n",
       "2    NaN  1918.0     EGO   1898      m          uk       NaN   NaN   NaN  \n",
       "3    NaN  1919.0     EGO   1879      f          uk       NaN   NaN   NaN  \n",
       "4    NaN  1918.0     EGO   1889      m          uk       NaN   NaN   NaN  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "### LOOKING AT THE RAW METADATA FILE\n",
    "\n",
    "pd.read_csv(classpath).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': 'Wigren, Bessie C.',\n",
       " 'birthdate': 1874,\n",
       " 'canonicity': 'random',\n",
       " 'firstpub': 1919,\n",
       " 'gender': 'f',\n",
       " 'nation': 'us',\n",
       " 'obscure': 'obscure',\n",
       " 'pubdate': 1919,\n",
       " 'pubname': '',\n",
       " 'reviewed': 'not',\n",
       " 'title': 'Summer wind'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "### looking up an ID listed earlier\n",
    "cleanmetadict['uc1.b3342759']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The authors have taken the data expressed in the CSV file and transformed it into a python dictionary, `cleanmetadict`. During the transformation process they have removed eight of the volumes, including the first volume which is why we look up the second volumed in the inspection cell immediately above. We also observe the raw metadata csv file has many additional columns that are not reflected in the python dictionary. What we see reflected in `cleanmetadict` is only the metadata necessary for the analysis with any dirty or unecessary information removed. Furthermore, the metadata now lives in a native python datastructure making it very easy to manipulate in code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### DEFACTORING FUNCTION\n",
    "### def label_classes(metadict, category2sorton, positive_class, sizecap):\n",
    "''' This takes as input the metadata dictionary generated\n",
    "by get_metadata. It subsets that dictionary into a\n",
    "positive class and a negative class. Instances that belong\n",
    "to neither class get ignored.\n",
    "'''\n",
    "\n",
    "all_instances = set([x for x in metadict.keys()])\n",
    "\n",
    "# The first stage is to find positive instances.\n",
    "\n",
    "all_positives = set()\n",
    "\n",
    "for key, value in metadict.items():\n",
    "    if value[category2sorton] == positive_class:\n",
    "        all_positives.add(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of code reads the metadata properties and puts all entries into a variable, `all_positives`, which contains all of the volume ids for reviewed poems. If poem metadata has the value `rev` (specified by the `positive_class` variable) for the `reviewed` property (specified by the `category2sorton` variable then it is labeled as a positive. The next cell then populates the negative instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_negatives = all_instances - all_positives\n",
    "iterator = list(all_negatives)\n",
    "for item in iterator:\n",
    "    if metadict[item]['reviewed'] == 'addedbecausecanon':\n",
    "        all_negatives.remove(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negative labels are assigned to all instances that are not in the set of positive instances. There is additional code that filters out anything with `addedbecausecannon` set for the `reviewed` property, but this code should never execute because the canon should already be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360\n"
     ]
    }
   ],
   "source": [
    "if sizecap > 0 and len(all_positives) > sizecap:\n",
    "    positives = random.sample(all_positives, sizecap)\n",
    "else:\n",
    "    positives = list(all_positives)\n",
    "    print(len(all_positives))\n",
    "\n",
    "# If there's a sizecap we also want to ensure classes have\n",
    "# matching sizes and roughly equal distributions over time.\n",
    "\n",
    "numpositives = len(all_positives)\n",
    "\n",
    "if sizecap > 0 and len(all_negatives) > numpositives:\n",
    "    if not 'date' in category2sorton:\n",
    "        available_negatives = list(all_negatives)\n",
    "        negatives = list()\n",
    "\n",
    "        for anid in positives:\n",
    "            date = metadict[anid]['pubdate']\n",
    "\n",
    "            available_negatives = sort_by_proximity(available_negatives, \n",
    "                                                    metadict, date)\n",
    "            selected_id = available_negatives.pop(0)\n",
    "            negatives.append(selected_id)\n",
    "\n",
    "    else:\n",
    "        # if we're dividing classes by date, we obvs don't want to\n",
    "        # ensure equal distributions over time.\n",
    "\n",
    "        negatives = random.sample(all_negatives, sizecap)\n",
    "\n",
    "else:\n",
    "    negatives = list(all_negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So most of the code in the cell above does not execute because the number of entries in the `all_positives` and `all_negatives` lists are not greater than `sizecap.` The `if` statements on line 1 and line 12 will not be true so the the accompanying blocks of code never execute. If the `sizecap` variable was smaller, or the number of entries  larger, this code would use random sampling to select smaller number of entries from the positives entries. \n",
    "\n",
    "Looking at the block of code for the negative entries is a bit more interesting. This block of code (from lines 13 to 29) makes an un-excuted reference to a function `sort_by_proximity` that samples from the negative elements with an equal distribution based upon some function of proximity. Because this code isn't executing we are not going to spend more time and analytical attention to exactly how this fuction operates. Furthermore, we have not included the code for `sort_by_proximity()` in the notebook because it is not part of the execution path we are tracing. In the code's garden of forking paths, this is a path not taken.\n",
    "\n",
    "These issues point to properties of code that make it difficult to review or critique, that is, we are in this case, reviewing a *live* execution of the code, not simply the code as text. Leveraging the affordances of the notebook platform allow us the ability to interact with the execution environment described in the code. At each step of the incremental process building this environment we can ask it questions by inspecting the state of variables (or even change them). This is more than simply treating the code as a text, the code is but one part of a complex assemblage we have been manipulating with the authors's code (and some of our own). However, as we *defactor* the authors's code, we make choices about how much to include for the argument we are trying to make (and for the sake of our time and attention). So we are dealing with a *code-criticism conundrum*: What is the required or adequate breadth and depth of the critique? The decision to include or not include `sort_by_proximity()` is a breadth issue. How broad should we be in including code that does not execute? Note, we are including code from a conditional block that doesn't execute, but are not going out the additional step to include non-executed functions defined elsewhere in the code. The decision to include or not include code from the standard library, code not written by the authors, is a depth issue. While there are many functions we are *stepping over*, like `len`, `list`, `append`, `pop`, `random.sample`, we argue there is no need to *step into* these functions because they were not written by the authors. Again, this raises the problematic issue of our decision to step over `sort_by_proximity()` even though it was written by the authors.\n",
    "\n",
    "Full reflexivity here would also mean that we note that the 'rules of the game' for code criticism aren't quite clear yet and therefore we are possibly feeling our way through an emerging methodological standard of practice for code criticism. As we see vestiges of the authors's evolution in thinking in their code, this notebook is capturing the evolution of our thinking about DEFACTORING as a practice. \n",
    "\n",
    "REF: Hiller and Joris about the tension between code's textual and processual dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "We have 360 positive, and\n",
      "360 negative instances.\n"
     ]
    }
   ],
   "source": [
    "# Now we have two lists of ids.\n",
    "\n",
    "IDsToUse = set()\n",
    "classdictionary = dict()\n",
    "\n",
    "print()\n",
    "print(\"We have \" + str(len(positives)) + \" positive, and\")\n",
    "print(str(len(negatives)) + \" negative instances.\")\n",
    "\n",
    "for anid in positives:\n",
    "    IDsToUse.add(anid)\n",
    "    classdictionary[anid] = 1\n",
    "\n",
    "for anid in negatives:\n",
    "    IDsToUse.add(anid)\n",
    "    classdictionary[anid] = 0\n",
    "\n",
    "for key, value in metadict.items():\n",
    "    if value['reviewed'] == 'addedbecausecanon':\n",
    "        print(\"DEFACTORING: Adding cannon supplement\") ### DEFACTORING CODE\n",
    "        IDsToUse.add(key)\n",
    "        classdictionary[key] = 0\n",
    "# We add the canon supplement, but don't train on it.\n",
    "\n",
    "### DEFACTORING FUNCTION RETERN\n",
    "### return IDsToUse, classdictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell we are seeing yet another instance of metadata being shaped and transformed in preparation for analysis. The code first prints out the number of positive and negative instancies by checking the length (using `len()`) of the volume ids stored in the `positives` and `negatives` variables. Two `for` loops iterate over these lists and populate two more variables, `IDsToUse` and `classdictionary`. The first, `IDsToUse` contains a master list of all the volume identifiers to be used in the analysis. It is of the python set datatype meaning there will be no duplicate identifiers in the set list. The second, `classdictionary` is a python dictionary that allows a simple boolean lookup to see if a volumn ID is in the positive or negative class–as indicated by a `0` or a `1`. There is a final loop whose logic checks to see if any volumes have the a specific metadata flag. We have added a defactoring statement to see if this logic is ever triggered. The output indicates the `if` statement's conditions were never satistfied.\n",
    "\n",
    "We have come to the end of the preparing metadata section. All of the code up to this point has focused on loading, normalizing, and transforming the metadata–namely the identifiers of the volumes to be analyzed. Based upon the values in the metadata fields and assumptions built into the logic of the code, the authors have assembled the list of volume ids and their associated class. Because this is a *supervised* machine learning exercise, the authors need labeled data to train the model. All of the work in this section of the code was dedicated to assigning a class label (positive or negative) to the identifiers of the data files. The next section dives into the actual data itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Words into Features\n",
    "\n",
    "Now that we know exactly which volumes of poetry we will be anayzing, we can venture into the datafiles and begin the work of transforming the volume data files into a datastructure suitable for analysis. The logistic regression requires the data to be in a specific shape, a matrix of binary features. This section does the work of *getting the data into shape*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### DEFACTORING FUNCTION DEFINITIONS\n",
    "### We need to define the infer_date function\n",
    "\n",
    "def infer_date(metadictentry, datetype):\n",
    "    if datetype == 'pubdate':\n",
    "        return metadictentry[datetype]\n",
    "    elif datetype == 'firstpub':\n",
    "        firstpub = metadictentry['firstpub']\n",
    "        if firstpub > 1700 and firstpub < 1950:\n",
    "            return firstpub\n",
    "        else:\n",
    "            return metadictentry['pubdate']\n",
    "    else:\n",
    "        sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code cell defines a helper function, `infer_date()`, which is used in the code below to deal with differences in the `pubdate` and `firstpub` columns in the metadata. When `firstpub` falls between 1700 and 1950 the codes uses that as the date, otherwise it returns the value in `pubdate` (or it exists the script in the case of bad data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a vocabulary list and a volsize dict\n",
    "wordcounts = Counter()\n",
    "\n",
    "volspresent = list()\n",
    "orderedIDs = list()\n",
    "\n",
    "positivecounts = dict()\n",
    "negativecounts = dict()\n",
    "\n",
    "for volid, volpath in zip(volumeIDs, volumepaths):\n",
    "    if volid not in IDsToUse:\n",
    "        continue\n",
    "    else:\n",
    "        volspresent.append((volid, volpath))\n",
    "        orderedIDs.append(volid)\n",
    "\n",
    "    date = infer_date(metadict[volid], datetype)\n",
    "    if date < pastthreshold or date > futurethreshold:\n",
    "        continue\n",
    "    else:\n",
    "        with open(volpath, encoding = 'utf-8') as f:\n",
    "            for line in f:\n",
    "                fields = line.strip().split('\\t')\n",
    "                if len(fields) > 2 or len(fields) < 2:\n",
    "                    # print(line)\n",
    "                    continue\n",
    "                word = fields[0]\n",
    "                if len(word) > 0 and word[0].isalpha():\n",
    "                    count = int(fields[1])\n",
    "                    wordcounts[word] += 1\n",
    "                    # for initial feature selection we use the number of\n",
    "                    # *documents* that contain a given word,\n",
    "                    # so it's just +=1.\n",
    "\n",
    "vocablist = [x[0] for x in wordcounts.most_common(numfeatures)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an important section of code because it contains the code that opens the data files and selects the word-features used in the logistic regression. The main block of code in the cell above loops over each data file (representing a poem) and reading the data into a Python [Counter](https://docs.python.org/3.5/library/collections.html#collections.Counter) data structure. To understand what is happening, it might help to start by looking at one of the data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 10 lines of the file:  poems/dul1.ark+=13960=t5fb5xg2z.poe.tsv\n",
      ",\t2745\r\n",
      "the\t1445\r\n",
      "and\t1182\r\n",
      ".\t672\r\n",
      "of\t468\r\n",
      "to\t442\r\n",
      ":\t386\r\n",
      "in\t384\r\n",
      ";\t324\r\n",
      "a\t253\r\n"
     ]
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "print(\"The first 10 lines of the file: \", volumepaths[0])\n",
    "!head {volumepaths[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above shows the first 10 lines of one of the poem data files. As we can plainly see, the volume has already been [pre-processed into a list of words and their frequencies](https://figshare.com/articles/Understanding_Genre_in_a_Collection_of_a_Million_Volumes_Interim_Report/1281251). This particular volume has 2,745 commas and 1445 instances of the word \"the.\" The author's code parses each of thes files and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word    Count\n",
      "-------------\n",
      "all         720\n",
      "the         720\n",
      "by          720\n",
      "in          720\n",
      "at          720\n",
      "to          720\n",
      "but         720\n",
      "that        720\n",
      "not         720\n",
      "as          720\n",
      "for         720\n",
      "a           720\n",
      "and         720\n",
      "on          720\n",
      "is          720\n",
      "with        720\n",
      "i           720\n",
      "of          720\n",
      "when        719\n",
      "they        719\n"
     ]
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "print(\"Word    Count\")\n",
    "print(\"-------------\")\n",
    "for word, count in wordcounts.most_common(n=20):\n",
    "    print(\"{:8}  {:5}\".format(word, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance it might seem strange that the count is 720 for all of the top words in the corpus. However, when we dig deeper into the code we can see that the authors are not tabulating the total word frequencies across all volumes in the corpus, rather they are associating words and the number of documents. The code loops over each file, opening it, and parses each line by splitting on the tab character (\"\\t\"). What is interesting when you closely inspect the code is that the authors are only paying attention to the word and ignoring the frequency. They check to see if the word is longer than zero and use the [isalpha()](https://docs.python.org/3/library/stdtypes.html#str.isalpha) function to make sure the characters are alphabetic as opposed to punctuation. The comments in the code explain that the authors are just using the \"number of documents that contain a given word\". This helps explain the unexpected result from looking at the most common words in the `wordcounts` variable. \n",
    "\n",
    "The authors are selecting their list of features (stored in the `vocablist` variable) by selecting words ranked by the number of documents in which they appear. The total number of documents we are working with is 720, so the table we generated above tells us that the top ten words appear in all of the documents. If we look at more than just the top ten, we can start to see the distribution of words in documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fe3f4244f28>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsoAAACUCAYAAACKjQ/uAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3VtsHNX9B/DvmRk79toTO05s/iYmiiBFUbdcKhKEoFzS\nICGiSI16cZWilqhItEna0jwAbRGiEpGAEkKiuoQ+VAK1D2AeEok+8AKJuKkoFlDQolREgjSpE1/W\nt3XWa+/OzP9hPePZ2Znd2dlZ747z/UgV692Zc35zfrPrU+d39gjDMAwQEREREVEBqd4BEBERERE1\nIk6UiYiIiIhccKJMREREROSCE2UiIiIiIhecKBMRERERueBEmYiIiIjIhVLugGQyiYGBAUxPT0MI\ngXvvvRf3338/3njjDbz99tvo6OgAAOzevRs333xz2Q4TiQTi8Xj1kVNdMH/RxdxFG/MXXcxdtDF/\n0VZt/spOlGVZxoMPPoiNGzcik8ng8ccfx4033ggA2LlzJ3bu3FlRh7zhoo35iy7mLtqYv+hi7qKN\n+Yu2mk+UOzs70dnZCQBoaWnB+vXrMTExAQDgXiVEREREtFJVVKM8OjqKc+fO4Rvf+AYA4K233sKj\njz6Kl19+Gel0uiYBEhERERHVg/C7hXUmk8Ef//hH/OAHP8DWrVsxMzMDVVUhhMBrr72GyclJ7N27\nt+i8RCKBRCJh/dzf3x9e9EREREREJQwODlqP4/F4RaUYvibKmqbh2Wefxbe//W3s2LGj6PWxsTE8\n99xzOHTokK9Oh4eHfQdIjUVVVaRSqXqHQQEwd9HG/EUXcxdtzF+0XX311VWd76v04tixY+jr6yuY\nJE9NTVmPP/roI1xzzTVVBUJERERE1EjKLuY7c+YM3nvvPWzYsAGPPfYYhBDYvXs33n//fXz99dcQ\nQqC7uxsPP/zwcsRLRERERLQsyk6UN2/ejNdff73oeT/fmUxEREREFFXcmY+IiIiIyAUnykRERERE\nLjhRJiIiIiJywYkyEREREZELTpSJiIiIiFxwokxERERE5IITZSIiIiIiF5woExERERG54ESZiIiI\niMjFipwoy5PjkCfHfb0uSe5DUK6NoMcSERERUTSU3cI6kibG8v9ds67s65IkQdf1ytsIeiwRERER\nRcKK/IsyEREREVG1OFEmIiIiInKxMksvPNSijlieHIfIZWEoTaG3TURERET1U3ainEwmMTAwgOnp\naQghsH37duzYsQOzs7M4cuQIxsbG0NPTgwMHDiAWiy1HzMGZtcRht5nNApwoExEREa0oZSfKsizj\nwQcfxMaNG5HJZPD444/jpptuwsmTJ3HDDTfge9/7Hk6cOIHjx4/jgQceWI6YiYiIiIhqrmyNcmdn\nJzZu3AgAaGlpwfr165FMJjE0NIS7774bAHDPPffg9OnTNQ2UiIiIiGg5VbSYb3R0FOfOncP111+P\n6elpdHZ2AshPpqenp2sSIBERERFRPfhezJfJZHD48GHs2bMHLS0tRa8LIVzPSyQSSCQS1s/9/f1Q\nVTVAqP7Ny/nLijn6MZ83xVQVmqahtbXVdxtuxxlCQJKVsseuBM3NzTXPH9UGcxdtzF90MXfRxvxF\n3+DgoPU4Ho8jHo/7PtfXRFnTNLzwwgu46667sHXrVgD5vyJPTU1Z/+3o6HA91y2gVCrlO8AgZC3n\n2o/5vD0ORVGQyxU+X6oNt+OEYUDXcjW/rkagquoVcZ0rEXMXbcxfdDF30cb8RZuqqujv7w98vq/S\ni2PHjqGvrw87duywnrvllltw6tQpAMCpU6ewZcuWwEEQERERETWasn9RPnPmDN577z1s2LABjz32\nGIQQ2L17N3bt2oUXX3wRJ0+eRHd3Nw4cOLAc8RIRERERLYuyE+XNmzfj9ddfd33tySefDD2gIMyN\nRLQ16+ocCRERERGtFCtjZz5zIxFOlImIiIgoJBV9PRwRERER0ZWCE2UiIiIiIhcrbqIsT45D5LKh\ntCUUxap/JiIiIgqDPDnO+UVErLiJMibGgGw4E2WkZpbqn4mIiIjCMDHG+UVErLyJMhERERFRCDhR\nJiIiIiJywYkyEREREZGLFTtRti/EK1U0L5IjLKivAy5kICIioka3MjYccZOaAXK5/CYkJQrmjeQY\nYBjcrGS5cZMYIiIianAr9i/KRERERETV4ESZiIiIiMjFFTFRFkIsPW5rr3l/V2r97ZV63URERLQy\nla1RPnbsGD7++GN0dHTg0KFDAIA33ngDb7/9Njo6OgAAu3fvxs0331zbSEMitanQLs/WtpMrtf72\nSr1uIiIiWpHKTpS3bduG+++/HwMDAwXP79y5Ezt37qxZYERERERE9VS29GLz5s1oa2sret4wjJoE\nRERERETUCAJ/Pdxbb72Fd999F9dddx1+9rOfIRaLhRkXEREREVFdBZoo33ffffjhD38IIQRee+01\nvPrqq9i7d6/rsYlEAolEwvq5v78fqqoGi9bDvKwAsTbEVBXzsgJjcfGeJCtomp2GZhiQ5PylCiFB\nXnwcU1XMSxIAgZgtpuzFC9A0DRACkqxYr2UvXijot6m3z+rPfty8rX1d1yFJtV0zacbV1NtX034A\noLm52TN/9usup5Jjg1jOMYmKUrmjxsf8RRdzF221yF+tfwdSocHBQetxPB5HPB73fW6gifLq1aut\nx9u3b8dzzz3neaxbQKlUKki3nmQtB7k1hlQqBVnLQSyWhehaDtrIMCQtB8081tChaTkrDkXXYRhG\nQUzyyDDEwrzVhvmaPDJc0G+mvcPqr+A4e/uKglwuF+r1OplxZdo7atoPAKiq6pk/+3WXU8mxQSzn\nmERFqdxR42P+oou5i7Za5K/WvwNpiaqq6O/vD3y+rz91GoZRUJM8NTVlPf7oo49wzTXXBA6AiIiI\niKgRlf2L8tGjR/HFF18glUph79696O/vRyKRwNdffw0hBLq7u/Hwww8vR6xERERERMum7ET5kUce\nKXpu27ZtNQlmOQlFKdoco9LNMuTJcYhctvJz5jMwVrVAK/N9w5IkQdf1kjGWa8Pt3ErOiYpKr60W\nY+GWLyIiIoquwN96EXmpGSCXA2y79lkbZvg1MQZkK5soY2IMmEsDrbGyG3O4TrycMVYy0VvJG4JU\nem01GAtOlImIiFaWK2ILayIiIiKiSnGiTERERETkghNlIiIiIiIXK2qiHGRxHbC0sE/Y65VDikfK\npEu+XskCwnLXV2l7tVQqlqB5KtduPdQynka7ViIioivNylrMF2RxHeC+sC+seBQFUJu9XwcqW4CW\nzQJKUzjt1VKpWMpdR9B266GW8TTatRIREV1hVtRflImIiIiIwsKJMhERERGRC06UyzDrl/3U1Qat\nKfU6TyRHArUnSd5pFW3tVbcRWiy2TV+UqWBj5ydO1vpe2SrJP+8VIiKyW1k1yrVg1i8D5etqK92w\nxHmeoxbVSI4BhlFxjWqpjS+kNrXqNsKKxRrbNeuAifHQr9XCWt8rWyX5571CREQ2/IsyEREREZEL\nTpSJiIiIiFxwokxERERE5KJsjfKxY8fw8ccfo6OjA4cOHQIAzM7O4siRIxgbG0NPTw8OHDiAWCxW\n82DrRSgKkJmrqg0/C4QkSYJIjgIANFuNpPNcK56W1hW58Mi8Jq1Enah13d3/Zy20NIJ8L3MNeMXv\nzFWp64sCZ324n7x58XNuWHXzREREfpX9i/K2bdvwxBNPFDx34sQJ3HDDDTh69Cji8TiOHz9eswAb\nQmom2EYmdhNjZRf7SZLkfpzzOTOe1EzwBYSNzMdYFRwTdKOZWvGK33zez/VFQNE3jlRzXX7fH0RE\nRMuo7G+ezZs3o62treC5oaEh3H333QCAe+65B6dPn65NdEREREREdRLoTzTT09Po7OwEAHR2dmJ6\nejrUoIiIiIiI6i2U71EWQni+lkgkkEgkrJ/7+/uhqv6+y9eveVmBEBJkWYFhi0WS85dnaBrkxcf2\n4/KvGwWPgfz5hsc1mW0i1gakL1vHWc/nz7baEUKCqqqYt72+1J+tHUmCvDAPffH5mKpiXpKsdpz9\nFcVjozQ3o7W11XWcMJdGi62Npt4+67Gu65AkCdmLF4BYO1o7Oq3XshcvAACa1671zN+8rMCQJDTN\nTlvXYY/FbH/elidJVgquNWYbq9hiP2bfTb19BeNotp+z5dM8x96G+bhpdhp6+jKkWFvBOFfD3o+m\naWhtbS2K33msyatv+zj5Oc4PXdfR3Nwc6nvPvF5TuXhL8XOusz8/shcvQNM0iOZVRW3b76tK4qiX\nsPNHy4e5i7ZS+XP7HPGjkT9rVqLBwUHrcTweRzwe931uoIlyZ2cnpqamrP92dHR4HusWUCqVCtKt\nJ1nLQTZ0aFoOwjCs53Utv1GIBAPa4mP7cbqWy0/ybY+NxfPt7diZbcqtMWipaes483kABe3Iho5U\nKgXZ/vpifwXtzKahX05ZcaZSKSi6vtSOoz9nPAXPLSwglyt+XtZykKaSyKaW/gUg076UO0VRkMvl\nII8MQ+7pRUqSl84dGQYALPT2eeZP1nIQs2lkswuusVjt2/Kka7mCa7WPldmP2XemvaNgHM32DVs+\nrXNsbZiPtZFhiLk0tNalhafV3ov2fuzX59a2M3avvsu14zzOD0VRIElSqO89Z//l4i3Fz7mVXK/V\n7sgwxMI8dFkuzoftvqokjnpRVbUh46LymLtoK5U/t88RPxr5s2alUVUV/f39gc/39ecowzCsCRsA\n3HLLLTh16hQA4NSpU9iyZUvgAIiIiIiIGlHZvygfPXoUX3zxBVKpFPbu3Yv+/n7s2rULL774Ik6e\nPInu7m4cOHBgOWIlIiIiIlo2ZSfKjzzyiOvzTz75ZOjBEBERERE1Cn4x6TISigKRK/99v0JRAm8k\nIhQFUiYNeXI8lM1I5MlxKJcuQOSyEIqC7MULUKYK2/azoEyeHIeUSVfct9lPNWNSKefYlRtLr9jC\nykEjCpLPcpYzx5XidzjX3kp+v4SB40NUH6F86wX5lJrxf1wuB5T4NpGS586llzZvqHb3t4mxfHsA\nkM1CNwDoGmAYVtu+dkybGAMUBVCbK+vbZI7JcnCOXbmxNGPr/r/S7awkQfJZjjmODThe3BVwGazk\n90sYOD5EdcE/kxARERERueBEmYiIiIjIBUsvloE8Oe6rNjko+/c2h8WYS0MOodRBJEcg696xOetS\nzbEylCbrdWTmfPfn9U/kXu2YfWs+/znTbL+etYJW385SD8frXtfkp4zAfn1iPgNjVYvvMfLbB60M\nlb6HiK4k9veH/feH0DQYslzmbGoEnCgvh4kxIFu7iXJNTCUBl13/KmUkx/L1zF6c9djmWC1OlH3X\ndS/ynKB5tVNh3Z/Vvr1+ermZfXtMlMtdk++actNcGmiNVVQbyYnyFYS1s0TebO8P++8PAQPh/nmL\naoWlF0RERERELjhRJiIiIiJywYkyEREREZEL1iiHxFwsZi5CaxTlFtpUuxBHnhyHpK6GnJqxFiyW\nGgu/CxsrXcRXKfsCSKEoUC6nkGtTC/q3NjuxLS4Mrf+2du/XbAsg7ceVWkAYJI9hLsJyLsKsRR9F\n/c1nAADGqhbr+XovKHPmqN7xhKHe9xYFt5LyUIt1D6Xer9X2x81hVg5OlMNiLhZrsIly2YU21S7E\nMTeesC9YLDUWfhc2VriIryqpGUhd3cX9m9/6YV9cGBLJNil3si+ALDiu1ALCIHkMcxGWcxFmLfpw\ntmtuhNMaW3q+3hMCZ47qHU8Y6n1vUXArKA81WSBc4v1adX/1XPBNoWLpBRERERGRC06UiYiIiIhc\nVFV6sX//fsRiMQghIMsynnnmmbDiomUg2tphXJ4Ndq5jo5B6EIoCKZOGZn/OVt8rtbYCFcRo1b06\na6ir+FJ4c5xK1Qia/Zba1MOq7W5pLar/9qoPrpQ9Dq/XgeJ6R7e6c2edtb1tIQBk3PvxGi+zFh5K\ns2tsZs25PUa3uPz+c6rf2s7lrAHNXrwAOZsNfb1Bra8h7LptZWochtFYdbfLXZteqr9637thbHjk\n1p79e+Mrib2S/vy06/YZ02hrkyhcVU2UhRB46qmn0N7uvTCJGpfUpkILOFEuqOGtl9TMUp3qooIN\nTmZTwFwFCwLtda82QpaDfzG8OU6lPtDNfktt6mHWAJv13fYPZq/64ErZ4/B6HSiO0aXu3FlnrY1e\nXGpbCCB92b0fr/Eya+FV94mya4wucfn+pem3tnMZa0D18VFAK3EvBY2l1tcQdt32xHj+3mqgifKy\n16aX6q/e924YGx65tWffYKmC2Cvqz0+7bp8xnCivaFWVXhiGEfrWyUREREREjaDqvygfPHgQkiRh\n+/btuPfee8OKi4iIiIiorqqaKD/99NNYs2YNZmZm8PTTT6Ovrw+bN28uOCaRSCCRSFg/9/f3Q1W9\nvxoriHlZgRASZFmBIYT1vCTnL8/QNMiLj+3H5V83Ch4D+fPt7dhJLu0Uv77UTunjCl93i81fO4Vx\nW8dKUv67iFta0TQ7DV1WXMcCAGKqioXh81CEKDquYEwlAUCyxRjWWPgfe8TagPTlsmPmeQ2lxqzE\n/RNTVWiahtbWVszLS28ds28xOb40fubYL76mpKagaxpE8yprLFRVRfbiBWiaZh0XU1XMS1LBmKmq\ninmX2JTmZisWQwhIakf+fFtOTZqmobm5GarLNbS0tUGSpKV2zOt2XIPJ7MMtNq9rqOQ9Z7ZhZ+/P\nPmZu+USszYrRbFM0NaNpdhpSTy9aW1utNkRnV0Ff9rya15q9eAGItUNJ58uUmnr78s8B0B1j7Xn+\n4nluzNflq66GJEmex2clAaBwbOzHOvOevXgBaGpG07oe1/7M9u3n2V/zukYnt/ttfvE9YH7uOMek\nVDtNs9Ou128dt3hvubXjvDbPsSyTE6/jdF13zZFb3u2am5vRMjsNPX0ZUqytMD4fObL3YX+Pu/Vn\nH0dt8TPH7T6056jcOPjh937RNA3KVLJoLICl8bU/Nq/H/Hw12/C6LnssTb191uedG+c4muyf9QAg\ny3LhZ4/9d6oQ1udLvtN8Pt1yZP/stf9eodobHBy0HsfjccTjcd/nVjVRXrNmDQBg9erVuPXWW3H2\n7NmiibJbQKlUqppui8haDrKhQ9NyELZSEF3L19BKMKAtPrYfp2s5CCEA22NrEwqPkhLdpR3n6/Z2\nSh3nfN1wic1PO864rXZmF+ttF+aRzS54jgWQz4kydslqx35cwZjqRr7eazHGsMaiorFvjUFLTZcd\nM89rKDVmJe6fVCoFRVGQy+Uga0v12VbftvGzxt7sb+wSsDAPfXFhoGzoSKVSkEeGIRbmreNSqRQU\nXS8Ys1QqBdklNn1hwYpFGAZEa8w61sypSVEUyLLseg36/HxBO+Z1O6/BZPbhFpvXNVTynjPbsCvo\nzzZmbvmUbeNgjdnMFLLZBYjOtflrXWzDHDN7P3Zmf3JPb77OGkCmvQPyyHDRcaXON89zY76es8Xm\ndnyzvnQfOs/NtHcU5d2Mu2gsHe3bzytoz+Mai+J3ud/kxc8f83Onkna0MuNl3ltu7RRdm0db5XLi\ndZz13vEYQ5MzNlVVkR0ZhphLQ2uNFcTnJ0f2Puzvcbf+7OMoFj9z3O5De47KjYMffu8XRVFguIyF\n+Vpuce2L83Oq4PO1xHXZY8m0dxS0WRSz5v68/XMSAGKxGNLp9NJnj+13qjAM6/MFgJVPtxzZr8n+\ne4VqS1VV9Pf3Bz4/cI3y/Pw8Mpn8zliZTAafffYZrrnmmsCBEBERERE1ksB/UZ6ensbzzz8PIQQ0\nTcOdd96Jm266KczYiIiIiIjqJvBEuaenB88//3yYsRARERERNYyqapRpZWiEzUOiwNz0Qk7NWF8y\nLzwWHvptz7khRiXsm3o423RuSCKpq5GdSkLOZgu/j7QoKO/NVex1wM7+wuRnk5Z68fNeEYpibQpj\nLkwCFseqeRVybWrhcx4bFjg3NjAW5l3zUy4m50YyZn/2c4SiQLl0Ib/ZTkur9bhoYxvHhjTamnWu\n90U59rEI47PHvlmQn7F0GwPz9YI2A24m4dy4otINL4Ioda79Pqym7VKbiJSLxxx7SV1tbRJV7r1e\nqw09luv3nXNTL7fPbGp8nCjT0iYPVUz6rgjmphdhbvCRDT7JLNhcxdmmc0MSRYGeHMtvWFFiolzx\n5ipVXoMrP5u01IufjXZSM/n/Lk6UrfGcGIPU01t4bKl7qWhjgwVAdlm9Xy4m50YyZn/2TStSM0tx\nZ7MF1+DZjhlbEPaxcG6eEbA9a7MgX2PpMgbm6/Zzgr7PHRtX+P4/EtWMRYlzC+7DatouMVEuat9t\nbM3PUFO593qtNvQI457zwbmpl+tnNjW86v5vJhERERHRCsWJMhERERGRC5ZeEDUIr1rTMOuAhaJA\nyqQL66xDrpuTJ8chdN1z45iq2/YxFl7HVXKtoq29drE5an0rja0USZJca2LN2mnr5wpqi521lmZ7\nzvvV+bO9XrsSUmsr4KNOXZkaL/qXbLPe2jm+Tl6xSZIEkRwtyKV1XY6ypYJjW1oL2jPvH3lyHLq+\nxjMOYy4NOZfzXZNv3WeO/gquzSVfRdfi8pyzRtv5XBDmfeb5ngyyJqHEWopK6rHLvQessYpt8N2m\neV7YazeofjhRJmoUXrXiYdYBp2bytZy22r/Q6+YmxgBdAzx2vaq6bT9j4XFcJdcqtVW4Y1YlsTlr\nfSuMrRSvibJVdxykTUetpdWe83511n661Tr7MZsC5ubK10BPjBePmVlv7RjfIh6xSZIEw5lL87pc\nJsrWsebxi+1Z98/EGIymZu84ppL594rfiaJHfwVxueXLeS1uzzlrtJ3PVcPr/RFgTUKptRRV12Pb\nmWOwvrKJck3WblDdsPSCiIiIiMgFJ8pERERERC4iP1GW5ufKH0REREREVKFI1ygLIaBMTVhfXk5E\neX4XNAVpdzm+rN/aAAMI/RrceF2T5wIk2yKgShf9+VG0UYGjbt254My+MK3WmxqUWijmZ4GgfUzL\nbShhbkxiLMwDqG5hmWhrL5tP54YYgPfYmgtjS/3+qWRRl9tYOOvNvca+3DiK5Ii1GYszR85FjQUb\n0djubWv8FheiCpEvD7fnJOjiTXuc5vg681CO24JLM59Qmguecxsrc2ytzzhbO9rIMGTHd5U7F8eW\nvC5u6hVpkZ4oA4DBlaVExfwuaArSbrkNN8Lqx1xsFfY1uPHagMDHopyKF/35UHIxFlwWnNkWptV6\nU4NysZVlj7vchhKLG5NoISwsk9pUaKMXS+fTuSEG4D225sLYUipZ1OUyFs6JsufYlxlHI1nideei\nRtt7wX5vW+NnLkQ1Z8r2nARdvGmP0xxfZx7KcVtwaeZTbS58zmUsrLE1P+Ns7RjJUUBzTNsrWRzL\nTb0iLfKlF0REREREtcCJMhERERGRC06UieiKZtYsCkUJtEmAMZf2rD8UyRHPNv3057Wph5Qp80/+\ndWBuNBNEpWMR9ZrPUpti2MdCJEegXLrg71pLbMLhR9ANj9zuxyDvpSD3j9/3rjw57n8cF5V679KV\npaoa5U8//RSvvPIKDMPAtm3bsGvXrrDiIiJaHj42cCjJ3DDCha/a0FK8NvWotH5zOVSz0Yyf2lb7\nWES85tNzUxiguCY6fdlXnX6pTTh88Vp/UK7Oukxdt29B7h+/712PTX5KKfnepStK4L8o67qOv/3t\nb3jiiSfwwgsv4IMPPsD//ve/MGMjIiIiIqqbwBPls2fPore3F93d3VAUBXfccQdOnz4dZmxERERE\nRHUTeKI8MTGBtWvXWj93dXVhYmIilKAqIa3tXvY+iYiIiGjlE0a5b4f38K9//Qv//ve/8Ytf/AIA\n8O677+Ls2bP4+c9/XnBcIpFAIpGwfu7v768iXCIiIiIi/wYHB63H8Xgc8Xjc97mB/6Lc1dWF8fGl\nFaQTExPo6uoqOi4ej6O/v9/6nz1Yih7mL7qYu2hj/qKLuYs25i/aBgcHC+ahlUySgSomyps2bcKl\nS5cwNjaGXC6HDz74AFu2bAnaHBERERFRQwn8HUOSJOGhhx7CwYMHYRgGvvvd76Kvry/M2IiIiIiI\n6qaqL+O8+eabcfTo0YrOqfRP3tRYmL/oYu6ijfmLLuYu2pi/aKs2f4EX8xERERERrWTcwpqIiIiI\nyAUnykRERERELjhRJiIiIiJyUdViPqdkMomBgQFMT09DCIHt27djx44dmJ2dxZEjRzA2Noaenh4c\nOHAAsVgMAHD8+HGcPHkSsixjz549uOmmm8IMiSqQzWbx1FNPIZfLQdM03HbbbfjRj37E/EWIruv4\n/e9/j66uLjz++OPMXYTs378fsVgMQgjIsoxnnnmG+YuQdDqNl19+GefPn4cQAnv37kVvby/z1+CG\nh4dx5MgRCCFgGAZGRkbw4x//GHfddRdzFxH//Oc/cfLkSQghsGHDBuzbtw+ZTCa8/BkhmpycNL76\n6ivDMAxjbm7O+M1vfmNcuHDB+Pvf/26cOHHCMAzDOH78uPGPf/zDMAzDOH/+vPHoo48auVzOGBkZ\nMX71q18Zuq6HGRJVKJPJGIZhGJqmGX/4wx+ML7/8kvmLkDfffNM4evSo8eyzzxqGYTB3EbJ//34j\nlUoVPMf8RcfAwIDxzjvvGIZhGLlczrh8+TLzFzGaphkPP/ywMTY2xtxFRDKZNPbv329ks1nDMAzj\n8OHDxsmTJ0PNX6ilF52dndi4cSMAoKWlBevXr0cymcTQ0BDuvvtuAMA999yD06dPAwCGhoZw++23\nQ5Zl9PT0oLe3F2fPng0zJKrQqlWrAOT/uqxpGgAwfxGRTCbxySefYPv27dZzzF10GIYBw/ElRMxf\nNKTTaZw5cwbbtm0DAMiyjFgsxvxFzOeff46rrroK69atY+4iRNd1ZDIZaJqGhYUFdHV1hZq/UEsv\n7EZHR3Hu3Dlcf/31mJ6eRmdnJ4D8ZHp6ehpAftvr66+/3jqnq6sLExMTtQqJfNB1Hb/73e8wMjKC\n++67D5s2bWL+IuLVV1/FT3/6U6TTaes55i46hBA4ePAgJEnCvffei+3btzN/ETE6OgpVVfHSSy/h\n3LlzuPaeTuRYAAADP0lEQVTaa7Fnzx7mL2I+/PBDfOc73wHAz86o6Orqws6dO7Fv3z6sWrUKN954\nI2688cZQ81eTiXImk8Hhw4exZ88etLS0FL0uhKhFtxQCSZLwpz/9Cel0GocOHcL58+eLjmH+Gs/H\nH3+Mjo4ObNy4EYlEwvM45q5xPf3001izZg1mZmZw8OBBXH311UXHMH+NSdd1fPXVV3jooYdw3XXX\n4ZVXXsGJEyeKjmP+Glcul8PQ0BAeeOAB19eZu8Z0+fJlDA0N4aWXXkIsFsPhw4fx3nvvFR1XTf5C\nnyhrmoYXXngBd911F7Zu3QogP5ufmpqy/tvR0QEgP5MfHx+3zk0mk+jq6go7JAogFovhm9/8Jj79\n9FPmLwLOnDmDoaEhfPLJJ1hYWMDc3Bz+/Oc/M3cRsmbNGgDA6tWrsXXrVpw9e5b5i4iuri6sXbsW\n1113HQDgtttuw4kTJ5i/CPn0009x7bXXYvXq1QA4b4mKzz//HD09PWhvbwcA3HrrrfjPf/4Tav5C\n/3q4Y8eOoa+vDzt27LCeu+WWW3Dq1CkAwKlTp7BlyxYAwJYtW/Dhhx8il8thdHQUly5dwqZNm8IO\niXyamZmx/tl+YWEBn3/+OdavX8/8RcBPfvITHDt2DAMDA/jtb3+Lb33rW/j1r3/N3EXE/Pw8MpkM\ngPy/yH322WfYsGED8xcRnZ2dWLt2LYaHhwHkf3n39fUxfxHy/vvv44477rB+Zu6iYd26dfjyyy+x\nsLAAwzBq8t4LdQvrM2fO4KmnnsKGDRsghIAQArt378amTZvw4osvYnx8HN3d3Thw4ADa2toA5L+m\n45133oGiKPyalTr773//i7/85S/QdR2GYeD222/H97//fczOzjJ/EfLFF1/gzTfftL4ejrlrfKOj\no3j++echhICmabjzzjuxa9cu5i9Cvv76a/z1r39FLpfDVVddhX379kHXdeYvAubn57Fv3z4MDAyg\ntbUVAPjei5A33ngDH374IWRZxsaNG/HLX/4SmUwmtPyFOlEmIiIiIlopuDMfEREREZELTpSJiIiI\niFxwokxERERE5IITZSIiIiIiF5woExERERG54ESZiIiIiMgFJ8pERERERC7+HxdcutXBqR0oAAAA\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe3f41cc048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "plt.style.use('ggplot')\n",
    "pd.Series([x[1] for x in wordcounts.most_common(n=3200)]).hist(bins=720, figsize=(12,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows a histogram of the top 3,200 words and how they are expressed across corpus. The spike on the right end of this chart shows the 18 words that appear in all 720 documents (as we can see in the text table above). As a whole, most of the higher bars are on the left side of the chart indicating most of the words appear in fewer documents. In the course of data preparation it is important to inspect the shape of our data, in this case to visualize the distribution of our features. While it may not have a direct impact upon our analysis, it does provide a deeper perspective on the various transformations of our data. The chart above looks a lot different than the word frequency files of the poetry volumes at the start (or even of the original poems themselves). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# vocablist = binormal_select(vocablist, positivecounts, negativecounts, totalposvols, totalnegvols, 3000)\n",
    "# Feature selection is deprecated. There are cool things\n",
    "# we could do with feature selection,\n",
    "# but they'd improve accuracy by 1% at the cost of complicating our explanatory task.\n",
    "# The tradeoff isn't worth it. Explanation is more important.\n",
    "# So we just take the most common words (by number of documents containing them)\n",
    "# in the whole corpus. Technically, I suppose, we could crossvalidate that as well,\n",
    "# but *eyeroll*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The author's code above does not actually perform any work as each line has been commented out, however we have decided to include it because it points towards an execution path and an interesting rationale for why it was not followed. In the \"production\" code the heuristic for feature selection is to simply to select the 3200 most common words by their appearance in the 720 documents. This is a simple and easy technique to implement and–more importantly–explain. Selecting the top words is a well established practice in text analysis and it has a high degree of methodologically face validity. It a good mechanism for removing features that have diminishing returns. However, the commented code above tells a different, and methodologically significant, story.\n",
    "\n",
    "The comment discusses an alternative technique for feature selection using binormal selection, which he has implemented in the function `binormal_selection`. Because this function is commented out and not used in the analysis, we have opted to not include it as part of the defactoring. Instead, we have decided to focus on the more interesting  rationale about *why* `binormal_selection` is not being used in the analysis as indicated in the author's comments:\n",
    "\n",
    "> There are cool things we could do with feature selection, but they'd improve accuracy by 1% at the cost of complicating our explanatory task.\n",
    "> The tradeoff isn't worth it. Explanation is more important.\n",
    "\n",
    "This comment reveals much about the reasoning, the effort, and energy focused on the important, but oft neglected in the humanities, work of discussing methodology. As Underwood argued in the *[Literary uses of high-dimensional space](http://journals.sagepub.com/doi/abs/10.1177/2053951715602494)* (2015) and repeatedly [on](https://tedunderwood.com/2013/06/11/on-not-trusting-people-who-promise-to-use-their-powers-for-good/) [his](https://tedunderwood.com/2015/12/02/emerging-conversations-between-literary-history-and-sociology/) [blog](https://tedunderwood.com/2016/05/04/versions-of-disciplinary-history/), while there is enourmous potential for the application of statistical methods in humanistic fields like literary history there is resistance to these methods because there is a resistance to *methodology*. Underwood has described the humanities disciplines relationship to methodology as an [\"insistence on staging methodology as ethical struggle\"](https://tedunderwood.com/2013/12/14/the-imaginary-conflicts-disciplines-create/). In this commented code we can see the meterial manifestation of Underwood's sentiment, in this case embodied by self-censorship in the decision to not use more statistically robust techniques for featues selection. We do not argue this choice compromises the analysis or final conclusions, rather we want to highlight the practical and material ways a resistance to diversity in research methods manifests in the digital humanities. By focusing on a close reading of the code, by *defactoring*, we provide a methodology to assist with the omnipresent *explanatory* task commensurate with the use of computational research methods in the humanities. \n",
    "\n",
    "In an algorithmic, data driven analysis, the selection of features is a *crucial* step because it effects the performance of the algorithm. In the digital humanities, feature selection is deeply inflected with the theory of the analysis and the context of the data. Claims made in and through this kind of analysis must attend to the representational configuration of the data. That is to say, we cannot take for granted how we have transformed data and what data are included or excluded from the analysis. Care, in the form of thorough documentation and thoughtful reflection, must be taken–especially at this unique moment as we are still learning how algorithmic, data-driven techniques can be leveraged to better understand our objects of study. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "donttrainon = list()\n",
    "\n",
    "# Here we create a list of volumed IDs not to be used for training.\n",
    "# For instance, we have supplemented the dataset with volumes that\n",
    "# are in the Norton but that did not actually occur in random\n",
    "# sampling. We want to make predictions for these, but never use\n",
    "# them for training.\n",
    "\n",
    "for idx1, anid in enumerate(orderedIDs):\n",
    "    reviewedstatus = metadict[anid]['reviewed']\n",
    "    date = infer_date(metadict[anid], datetype)\n",
    "    if reviewedstatus == 'addedbecausecanon':\n",
    "        donttrainon.append(idx1)\n",
    "    elif date < pastthreshold or date > futurethreshold:\n",
    "        donttrainon.append(idx1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the comments describe, this block of code creates a list of volume IDs not to be used in the training. What that means *in code* is that any volume with the metadata label `addedbecauseofcanon` or with a `date` outside of the thresholds defined by `pastthreshold` and `futurethreshold.` If we inspect the `donttrainon` variable we can see how many volumes satisfy these criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The variable donttrainon contains 0 volume IDs\n"
     ]
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "print(\"The variable donttrainon contains {} volume IDs\".format(len(donttrainon)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would appear there are no volumes to be filtered out by these criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "authormatches = [list(donttrainon) for x in range(len(orderedIDs))]\n",
    "# For every index in authormatches, identify a set of indexes that have\n",
    "# the same author. Obvs, there will always be at least one.\n",
    "\n",
    "# Since we are going to use these indexes to exclude rows, we also add\n",
    "# all the ids in donttrainon to every volume\n",
    "\n",
    "for idx1, anid in enumerate(orderedIDs):\n",
    "    thisauthor = metadict[anid]['author']\n",
    "    for idx2, anotherid in enumerate(orderedIDs):\n",
    "        otherauthor = metadict[anotherid]['author']\n",
    "        if thisauthor == otherauthor and not idx2 in authormatches[idx1]:\n",
    "            authormatches[idx1].append(idx2)\n",
    "\n",
    "for alist in authormatches:\n",
    "    alist.sort(reverse = True)\n",
    "\n",
    "# I am reversing the order of indexes so that I can delete them from\n",
    "# back to front, without changing indexes yet to be deleted.\n",
    "# This will become important in the modelingprocess module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of code is a bit more interesting than the last. Here the Underwood and Sellers group the volumes by the same author. The list `authormatches` is a list of lists for each volume. Each sub-list contains the IDS of all the volumes by the same author. Essentially this data structure represents the potential relations of each volume to other volumes, with that relation being \"other volumes by the same author.\" This raises the question, how many volumes share the same author in this corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fe3f291db00>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANQAAAJTCAYAAABw9DhRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe3f2973f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "pd.Series([len(x) for x in authormatches]).hist(bins=6,\n",
    "                                                figsize=(3, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This histogram tells us a majority of volumes are written by unique authors but that there are some authors who have written up to six volumes in the corpus. Note, we are generating this graph by counting the length of the list containing the volume IDs of other volumes by the same author. This means volumes written by the same author are counted twice. This isn't an issue for the purposes of our inspection, just that the sum total number of volumes represented by this histogram is greater than 720. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### DEFACTORING DEFINITIONS\n",
    "\n",
    "usedate = False\n",
    "# Leave this flag false unless you plan major\n",
    "# surgery to reactivate the currently-deprecated\n",
    "# option to use \"date\" as a predictive feature.\n",
    "\n",
    "def get_features(wordcounts, wordlist):\n",
    "    numwords = len(wordlist)\n",
    "    wordvec = np.zeros(numwords)\n",
    "    for idx, word in enumerate(wordlist):\n",
    "        if word in wordcounts:\n",
    "            wordvec[idx] = wordcounts[word]\n",
    "\n",
    "    return wordvec\n",
    "\n",
    "# In an earlier version of this script, we sometimes used\n",
    "# \"publication date\" as a feature, to see what would happen.\n",
    "# In the current version, we don't. Some of the functions\n",
    "# and features remain, but they are deprecated. E.g.:\n",
    "\n",
    "def get_features_with_date(wordcounts, wordlist, date, totalcount):\n",
    "    numwords = len(wordlist)\n",
    "    wordvec = np.zeros(numwords + 1)\n",
    "    for idx, word in enumerate(wordlist):\n",
    "        if word in wordcounts:\n",
    "            wordvec[idx] = wordcounts[word]\n",
    "\n",
    "    wordvec = wordvec / (totalcount + 0.0001)\n",
    "    wordvec[numwords] = date\n",
    "    return wordvec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code cell defines two functions to be used below when opening and parsing the raw data files (in the `poems/` diretory). The function `get_features()` simply takes the wordcounts from the parsed volume and filters out any words that are not part of `wordlist`, which contains the list of word features that had been selected for this analysis. We have also included a second function, `get_features_with_date()`, even though it is not executed. This residual code points to yet another path not taken, one that uses the volume's publication date as a feature. As Underwood and Seller's comment indicates, this was an experiment from an \"earlier version of this script...to see what would happen.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "volsizes = dict()\n",
    "voldata = list()\n",
    "classvector = list()\n",
    "\n",
    "for volid, volpath in volspresent:\n",
    "\n",
    "    with open(volpath, encoding = 'utf-8') as f:\n",
    "        voldict = dict()\n",
    "        totalcount = 0\n",
    "        for line in f:\n",
    "            fields = line.strip().split('\\t')\n",
    "            if len(fields) > 2 or len(fields) < 2:\n",
    "                continue\n",
    "\n",
    "            word = fields[0]\n",
    "            count = int(fields[1])\n",
    "            voldict[word] = count\n",
    "            totalcount += count\n",
    "\n",
    "    date = infer_date(metadict[volid], datetype)\n",
    "    date = date - 1700\n",
    "    if date < 0:\n",
    "        date = 0\n",
    "\n",
    "    if usedate:\n",
    "        features = get_features_with_date(voldict, \n",
    "                                          vocablist, \n",
    "                                          date, \n",
    "                                          totalcount)\n",
    "        voldata.append(features)\n",
    "    else:\n",
    "        features = get_features(voldict, vocablist)\n",
    "        voldata.append(features / (totalcount + 0.001))\n",
    "\n",
    "\n",
    "    volsizes[volid] = totalcount\n",
    "    classflag = classdictionary[volid]\n",
    "    classvector.append(classflag)\n",
    "    \n",
    "data = pd.DataFrame(voldata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an important code block because we are now pulling the raw data files from teh `poems/` directory into memory, filtering out the unselected word features, and putting the data into a vectorized data structure. The code loops over the `volspresent` variable and parses each individual volume into the `voldict` dictionary. At this stage the code is reading in all of words, their frequencies, and tabulating the total number of words in that volume. Once all of the data for the volume has been read into memory, the code calls the `get_features` function that throws out the words not part of the selected word features stored in the `vocablist` variable. This is where the top 3200 words are foregrounded and the remaining, less commonly used words, are discarded. \n",
    "\n",
    "At this point, any prosaic resemblance left in the data is gone and now we are dealing entirely with textual data in a numeric form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vector representation of The croakers by Drake, Joseph Rodman,\n",
      "The vector has a length of 3200.\n",
      "The first 100 elements of the vector:\n",
      "[  87.  857.   73.  297.   86.  365.   67.  141.   40.   99.  134.  322.\n",
      "  751.   99.  110.  143.  106.  464.   63.   48.   39.   46.   73.   88.\n",
      "  129.   69.   62.   40.   54.   41.   35.   31.   60.   83.   54.   90.\n",
      "   26.   48.   15.   18.   50.   34.   48.   45.   57.   64.   28.   24.\n",
      "   75.   16.   33.   30.   30.   23.   76.   17.   32.   31.   17.   15.\n",
      "   21.   22.   10.   18.   21.   16.   20.    9.   24.   17.   24.   31.\n",
      "   10.    4.   10.   25.   15.   11.   61.    8.   12.   10.    5.   31.\n",
      "   31.   19.   19.   21.   36.    6.   13.   10.   32.   15.   21.   11.\n",
      "   19.   12.    5.   17.]\n"
     ]
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "print(\"The vector representation of {} by {}\".format(metadict[volid]['title'], metadict[volid]['author']))\n",
    "print(\"The vector has a length of {}.\".format(len(features)))\n",
    "print(\"The first 100 elements of the vector:\")\n",
    "print(features[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inspection above shows us the last volume processed by the loop, The croakers by Joseph Rodman Drake. As we can see, the words for this volume of poetry are now represended as a list of numbers (representing word frequencies). However, this list of numbers still requires additional transformation in order to be consumable by the Logistic Regression. The word frequencies need to be normalized so they are comparable across volumes. To do this Underwood and Sellers divide the frequency of each individual word (each number in the list above) by the total number of words in that volume (the `totalcount` variable. This makes volumes of different lengths comparable by turning absolute frequecies into relative frequencies. One thing we didn't quite understand at first is why the value of 0.001 has been added to the `totalcount` variable. When we asked, it turns out this is a [\"lazy\" way to prevent divide-by-zero errors](https://twitter.com/Ted_Underwood/status/826098272225918977).\n",
    "\n",
    "The end result of the code we have executed thus far in the notebook is a very neat and tidy [DataFrame](http://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe) of numbers between zero and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>3190</th>\n",
       "      <th>3191</th>\n",
       "      <th>3192</th>\n",
       "      <th>3193</th>\n",
       "      <th>3194</th>\n",
       "      <th>3195</th>\n",
       "      <th>3196</th>\n",
       "      <th>3197</th>\n",
       "      <th>3198</th>\n",
       "      <th>3199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>0.003924</td>\n",
       "      <td>0.049048</td>\n",
       "      <td>0.002785</td>\n",
       "      <td>0.012278</td>\n",
       "      <td>0.003924</td>\n",
       "      <td>0.012594</td>\n",
       "      <td>0.004050</td>\n",
       "      <td>0.009430</td>\n",
       "      <td>0.002721</td>\n",
       "      <td>0.004050</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>0.004961</td>\n",
       "      <td>0.055889</td>\n",
       "      <td>0.002720</td>\n",
       "      <td>0.011242</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.020723</td>\n",
       "      <td>0.005241</td>\n",
       "      <td>0.012362</td>\n",
       "      <td>0.004081</td>\n",
       "      <td>0.004321</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>0.005563</td>\n",
       "      <td>0.044256</td>\n",
       "      <td>0.004339</td>\n",
       "      <td>0.014797</td>\n",
       "      <td>0.004094</td>\n",
       "      <td>0.018757</td>\n",
       "      <td>0.004984</td>\n",
       "      <td>0.011971</td>\n",
       "      <td>0.005563</td>\n",
       "      <td>0.004517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>0.004545</td>\n",
       "      <td>0.060580</td>\n",
       "      <td>0.002397</td>\n",
       "      <td>0.012445</td>\n",
       "      <td>0.003961</td>\n",
       "      <td>0.016135</td>\n",
       "      <td>0.003252</td>\n",
       "      <td>0.006796</td>\n",
       "      <td>0.001855</td>\n",
       "      <td>0.005629</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>0.004370</td>\n",
       "      <td>0.043048</td>\n",
       "      <td>0.003667</td>\n",
       "      <td>0.014919</td>\n",
       "      <td>0.004320</td>\n",
       "      <td>0.018334</td>\n",
       "      <td>0.003365</td>\n",
       "      <td>0.007083</td>\n",
       "      <td>0.002009</td>\n",
       "      <td>0.004973</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6     \\\n",
       "715  0.003924  0.049048  0.002785  0.012278  0.003924  0.012594  0.004050   \n",
       "716  0.004961  0.055889  0.002720  0.011242  0.003000  0.020723  0.005241   \n",
       "717  0.005563  0.044256  0.004339  0.014797  0.004094  0.018757  0.004984   \n",
       "718  0.004545  0.060580  0.002397  0.012445  0.003961  0.016135  0.003252   \n",
       "719  0.004370  0.043048  0.003667  0.014919  0.004320  0.018334  0.003365   \n",
       "\n",
       "         7         8         9       ...         3190      3191     3192  \\\n",
       "715  0.009430  0.002721  0.004050    ...     0.000063  0.000000  0.00000   \n",
       "716  0.012362  0.004081  0.004321    ...     0.000000  0.000000  0.00004   \n",
       "717  0.011971  0.005563  0.004517    ...     0.000045  0.000022  0.00000   \n",
       "718  0.006796  0.001855  0.005629    ...     0.000042  0.000000  0.00000   \n",
       "719  0.007083  0.002009  0.004973    ...     0.000050  0.000000  0.00000   \n",
       "\n",
       "         3193    3194      3195      3196      3197      3198      3199  \n",
       "715  0.000063  0.0000  0.000000  0.000000  0.000063  0.000063  0.000000  \n",
       "716  0.000000  0.0000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "717  0.000022  0.0000  0.000067  0.000022  0.000000  0.000045  0.000000  \n",
       "718  0.000000  0.0000  0.000104  0.000063  0.000000  0.000021  0.000021  \n",
       "719  0.000000  0.0001  0.000000  0.000050  0.000000  0.000000  0.000000  \n",
       "\n",
       "[5 rows x 3200 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last row in that table, 719, is the The croaker by Joseph Rodman Drake. It is just one of 720 relatively indistinguishable rows of numbers in this representation of 19th century poetry. This is a far cry from the original, prosaic representation literary historians are probably used to seeing:\n",
    "\n",
    "![Screenshot of the Google Books site for The Croakers](notebook_resources/the-croakers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sextuplets = list()\n",
    "for i, volid in enumerate(orderedIDs):\n",
    "    listtoexclude = authormatches[i]\n",
    "    asixtuple = (data, \n",
    "                 classvector, \n",
    "                 listtoexclude, \n",
    "                 i, \n",
    "                 usedate, \n",
    "                 regularization)\n",
    "    sextuplets.append(asixtuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the last step before we move away from the transformation of features and into the actual analysis of the data. This very simple bit of code gathers all of the relevant data and metadata necssary for performing the statistical analysis. The `sextuplets` variable is a list of 720 tuples containting six elements. Each item in the `sextuplets` list contains the necessary data structures to model each poem. The contents of each row of `sextuplets` is as follows:\n",
    "\n",
    "- data: a normalized feature matrix. Word features are the columns and volumes are the rows. The dimensions are 720 x 3200\n",
    "- classvector: the classification of documents as either 'reviewed' (1) or 'random' (0).\n",
    "- listtoexclude: the list of poems to ignore because they are the same author\n",
    "- i: the index of the volume\n",
    "- usedate: a flag indicating if date is a feature. It is false in the default execution path.\n",
    "- regularization: a parameter for the scikit-learn LogisticRegression function. hardcoded parameter from the beginning of the notebook.\n",
    "\n",
    "With all of the data assembled and in the right shape, a process we call *data fitness*, we can now venture into the algorithmic territory and perform the statistical analysis. \n",
    "\n",
    "As we can see, the *fitted* representation of features has traveled a great distance from the original poetry. One of the most important aspects of *distant reading* is the work of cleaning, preparing, and normalizing texts to be \"read\" by an algorithm. When considering distance, we should think not only of the perspective that we, the analyst, are reading from, but also the distance traveled in terms of successive transformations and representations of the data. If computational literary history is a triathalon, we have only completed the first endurance test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Predictive Models\n",
    "\n",
    "We are now about to dive into the very heart of the analysis, training predictive models on each volume. The code cells below do the following:\n",
    "\n",
    "\n",
    "- Loop over every volume in the corpus. Within each loop:\n",
    "    - Create a training set by removing the selected volume and other volumes by the same author from the corpus (performed by the `sliceframe()` function). This is the held out data.\n",
    "    - Normalize the training set by computing the z-score for each feature/feature set (performed by the `normalizearray()` function).\n",
    "    - Fit the model on the training data (performed by the `model_one_volume()` function).\n",
    "    - Use the fitted model to predict the probability the (normalized) held out data was \"reviewed\" or \"random.\"\n",
    "    \n",
    "What is important to understand is this section of the code doesn't train one model, it trains 720 independant models–one for each volume. As the code loops over the volumes, it holds out one volume, trains the model on all of the remaining volumes, and then uses that model to predict the status of the held out volume. Lather, Rinse, Repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### DEFACTORING DEFINITION\n",
    "\n",
    "def sliceframe(dataframe, yvals, excludedrows, testrow):\n",
    "    numrows = len(dataframe)\n",
    "    newyvals = list(yvals)\n",
    "    for i in excludedrows:\n",
    "        del newyvals[i]\n",
    "        # NB: This only works if we assume that excluded rows\n",
    "        # has already been sorted in descending order !!!!!!!\n",
    "        # otherwise indexes will slide around as you delete\n",
    "\n",
    "    trainingset = dataframe.drop(dataframe.index[excludedrows])\n",
    "\n",
    "    newyvals = np.array(newyvals)\n",
    "    testset = dataframe.iloc[testrow]\n",
    "\n",
    "    return trainingset, newyvals, testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function prepares the data for training a model by separating volumes using for training from a single volume to test the predictive power of the model. The function takes a dataframe containing the feature vectors, a list of the classifications for each volume, a list of volumes to exclude(because of shared authorship), and the the index of the specific volume to be held out. This function returns the dataframe with the hold out removed(`trainingset`), a list of classifications(`newyvals`) that maps to the modified dataframe, and the held-out volume that will be classified once the model has been trained(`testset`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalizearray(featurearray, usedate):\n",
    "    '''Normalizes an array by centering on means and\n",
    "    scaling by standard deviations. Also returns the\n",
    "    means and standard deviations for features.\n",
    "    '''\n",
    "\n",
    "    numinstances, numfeatures = featurearray.shape\n",
    "    means = list()\n",
    "    stdevs = list()\n",
    "    lastcolumn = numfeatures - 1\n",
    "    for featureidx in range(numfeatures):\n",
    "\n",
    "        thiscolumn = featurearray.iloc[ : , featureidx]\n",
    "        thismean = np.mean(thiscolumn)\n",
    "\n",
    "        thisstdev = np.std(thiscolumn)\n",
    "\n",
    "        if (not usedate) or featureidx != lastcolumn:\n",
    "            # If we're using date we don't normalize the last column.\n",
    "            means.append(thismean)\n",
    "            stdevs.append(thisstdev)\n",
    "            featurearray.iloc[ : , featureidx] = \\\n",
    "                (thiscolumn - thismean) / thisstdev\n",
    "        else:\n",
    "            print('FLAG')\n",
    "            means.append(thismean)\n",
    "            thisstdev = 0.1\n",
    "            stdevs.append(thisstdev)\n",
    "            featurearray.iloc[ : , featureidx] = \\\n",
    "                (thiscolumn - thismean) / thisstdev\n",
    "            # We set a small stdev for date.\n",
    "\n",
    "    return featurearray, means, stdevs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function *standardizes* the features by computing the z-score for the feature vectors. That is, it loops over each each column of the data, subtracts the column mean from each value, and then divides that value by the standard deviation. This is an important step in the data preparatin pipeline because it ensures all of the data values are on the same scale. For an extended discussion on features scaling, see [this blog post](http://sebastianraschka.com/Articles/2014_about_feature_scaling.html) by Sebastian Raschka who is the author of [Python Machine Learning](https://github.com/rasbt/python-machine-learning-book). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_one_volume(data5tuple):\n",
    "    data, classvector, listtoexclude, i, usedate, regularization = \\\n",
    "        data5tuple\n",
    "    trainingset, yvals, testset = sliceframe(data, \n",
    "                                             classvector, \n",
    "                                             listtoexclude, \n",
    "                                             i)\n",
    "    newmodel = LogisticRegression(C = regularization)\n",
    "    trainingset, means, stdevs = normalizearray(trainingset, usedate)\n",
    "    newmodel.fit(trainingset, yvals)\n",
    "\n",
    "    testset = (testset - means) / stdevs\n",
    "    prediction = newmodel.predict_proba(testset.reshape(1, -1))[0][1]\n",
    "    if i % 50 == 0:\n",
    "        print(i)\n",
    "    # print(str(i) + \"  -  \" + str(len(listtoexclude)))\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many respects, this is the most important block of code in the entire document. The code above actually runs the logistic regression and does the magical, algorithmic work that generates a prediction about each individual volume. This function builds upon the two previous functions to assemble a normalized set of training data (`trainingset`) distinct from the single volume to be predicted (`testset`).\n",
    "\n",
    "There are three lines of code involved in the computational modeling of the data. First, Underwood an Sellers instantiate a model object with the regularization parameter (more on that below):\n",
    "```\n",
    "newmodel = LogisticRegression(C = regularization)\n",
    "```\n",
    "Then they \"fit\" the model using the normalized trainting data:\n",
    "```\n",
    "newmodel.fit(trainingset, yvals)\n",
    "```\n",
    "Once a model has been \"fit\" to the data they can use that model to make predictions about unseen or held-out data. This is what they do with the `predict_proba()` function in this line:\n",
    "```\n",
    "prediction = newmodel.predict_proba(testset.reshape(1, -1))[0][1]\n",
    "```\n",
    "\n",
    "Those three lines are all it takes to do the computational part of of the analysis, the rest of the code up until this point has all been data preparation, cleaning, normalization, and re-shaping. This ratio of analytical to preperatory code is interesting and indicates claims that machines are taking our jobs are greatly exaggerated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization and Logistic Regression\n",
    "\n",
    "The three lines of code above hide a significant amount of intellectual and computational work. The call to the `newmodel.fit()` function is a crucial step in the analytical process, but unfortunately far outside of the scope of effort we delinated for the current defactoring effort. Underwood and Sellers are using an implementation of Logistic Regression from the 3rd party Python library [`scikit-learn`](http://scikit-learn.org/stable/). Curious readers can look at the source code [here](https://github.com/scikit-learn/scikit-learn/blob/14031f6/sklearn/linear_model/logistic.py#L1138), but instead of diving into the code we are going to provide a non-mathy description of what is happening in that function call.\n",
    "\n",
    "At a very high level, logistic regression is a machine learning algorithm for performing classification. Logistic regression works by estimating the parameters of a function, the *hypothesis representation*, that divides a multidimensional space into two parts (note, in this case we are talking about binomial or binary logistic regression, which classifies things into one of two bins). The hypothesis representation describes a line that winds its way through the space creating what is called the *decision boundary.* Every data point that lands on one side the boundary gets one label and every data point on the other side of the boundary gets the other label. Similar to linear regression, the goal is to find the best hypothesis representation, that is, the function that best draws a line dividing the space given the known data points. Once you have a good hypothesis representation, an appropriately *fit* model, you can begin to classify *new* data by dropping the into the multidimensional space and seeing on which side of the decision boundary they land.\n",
    "\n",
    "The key to logistic regression is estimating the parameters of the hypothesis representation–the parameters to the function that draws a line through the multidimensional space. We can derive the parameters by using the *features* of existing data combined with their known labels; this is called *training data*. The modeling process, the function call to `newmodel.fit(trainingset, yvals)` in Underwood and Sellers's code, uses training data–the matrix of word features in the `data` variable and known labels ('reviewed' or 'random') in the `classvector` variable–to \"learn\" the parameters through a process called *gradient descent* (note: scikit-learn uses a different process called [*coordinate descent*](http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression) that is exceptionally complex). How gradient descent or the more advanced optimization functions like coordinate descent work are well beyond the scope of the discussion (and our explanatory power) so we will just nod and gesture towards the mathematical magic performed by the `newmodel.fit(trainingset, yvals)` function call.\n",
    "\n",
    "For readers yearning for more, watch [Andrew Ng explain classification using Logistic Regression](https://class.coursera.org/ml-005/lecture) as part of his Coursera MOOC on machine learning.\n",
    "\n",
    "##### Overfitting\n",
    "\n",
    "One of the problems when fitting a logistic regression model is a tendency towards *overfitting*. Crudely this means the model, the function witht the learned parameters, that you estimated have tailored themselves such that they are overly optimized to the particular training data you provided. As such, the model becomes less useful for *prediction* or classifying new data because they are outside the fitness of the model. An overfit model is like a snug pear of jeans, once you put on a few pounds (add new data) they don't fit. In Underwood and Sellers's case, they are fitting models on all volumes *except one*, which is *held out.* Then they test the predictive performance of the model by seeing if it correctly classifies the held-out volume. If they *overfit* the models, the model will to a terrible job guessing the status of the held out volumes. \n",
    "\n",
    "When Underwood and Sellers instantiated the model (`newmodel = LogisticRegression(C = regularization)`), the set a regularization parameter on the model. Regularization is a technique for logistic regression (and other machine learning algorithms) that smooths out the tendency toward overfitting with some more mathematical gymnastics that we don't quite have the power to explain with words, but can explain visually. The diagram below shows how regularization can help with the fitness of the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Regularization](notebook_resources/regression_figures.png)\n",
    "\n",
    "*On the left side is a linear regression which doesn't quite fit the data. In the middle is an overfit logistic regression. On right side is a regularized logistic regression.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the diagrams show, the regularized logistic expression (the right side) does have a bit of error, there are pink and blue dots on the wrong sides of the decision boundary, but as more data get added it will generally be more right than the overfitted model as represented by the middle diagram (the squiggly decision boundary). \n",
    "\n",
    "In the grand scheme of things, logistic regression isn't actually that complicated of a supervised learning algorithm (compared to algorithms like [support vector machines](https://en.wikipedia.org/wiki/Support_vector_machine) or [deep learning](https://en.wikipedia.org/wiki/Deep_learning)). Logistic regression has the advantage of being powerful, theoretically grounded (in the statistical sense), and easy to understand how it makes assertions (unlike deep learning). This relative simplicity is exceptually important because when we make computationally inflected claims in the digital humanities, we need to be able to *explain* them and understand just *how* such assertions are derived. We cannot permit \"black box\" computational scholarship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now do leave-one-out predictions.\n",
    "print('Beginning multiprocessing.')\n",
    "\n",
    "pool = Pool(processes = 4)\n",
    "res = pool.map_async(model_one_volume, sextuplets)\n",
    "\n",
    "# After all files are processed, write metadata, errorlog, and counts of phrases.\n",
    "res.wait()\n",
    "resultlist = res.get()\n",
    "\n",
    "assert len(resultlist) == len(orderedIDs)\n",
    "\n",
    "logisticpredictions = dict()\n",
    "for i, volid in enumerate(orderedIDs):\n",
    "    logisticpredictions[volid] = resultlist[i]\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "print('Multiprocessing concluded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have describe the code that does the actual computational modeling, we need to work our way through the scaffolding that automates the training each individual models for all 720 volumes. Training or fitting a logistic regression model takes time (the computer has to do some hard work) and training 720 different models takes 720 times longer! Fortunately, this is an [embarrassingly parallel](https://en.wikipedia.org/wiki/Embarrassingly_parallel) computational task and so we can train the models using parallel processing instead of one after the other. Using Python's built in parallel processing modules (`Pool(processes = 4)`), this code can speed up the process. Still, this block of code takes a fair amount of time to execute, around twenty minutes on a quad core MacBook Pro. \n",
    "\n",
    "Start 1:46pm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "print(\"There are {} predictions.\".format(len(logisticpredictions)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What emerges from the other side of this computationally intensive task are a series of predictions, 720 to be specific, for each of the modeled volumes. These predictions, stored in the `logisticpredictions` variable, are the algorithm's assertions of each volumes reviewed status. Additionally, because we already know the status of the modeled volumes we can compare the performance of the predictive model to the \"[ground truth](https://en.wikipedia.org/wiki/Ground_truth)\" and see if the algorithm was able to detect patterns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Coefficients\n",
    "\n",
    "\n",
    "The code below represents a shift in the focal object of the analysis. In the previous section 720 distinct logistic regressions were trained in order to predict the classification of a single, held-out poem. This is the data that was used to produce the main figure, *Figure 1. Predicted probabilities that volumes come from the reviewed set.* \n",
    "\n",
    "The code below generates a single logistic regression model, trained on *all of the data* with nothing held-out. This model has no held-out data because it is not being used for predictive purposes. Instead, the properties of this model, the parameters of the *hypothesis representation*, are interrogated to better understand the influence of individual features, words, on reviewed vs. unreviewed volumes. This isn't using computational modeling to *predict* a phenomena, it is using the computational model to *explore* and *explain* patterns and features of the phenomena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "donttrainon.sort(reverse = True)\n",
    "trainingset, yvals, testset = sliceframe(data, \n",
    "                                         classvector, \n",
    "                                         donttrainon, \n",
    "                                         0)\n",
    "newmodel = LogisticRegression(C = regularization)\n",
    "trainingset, means, stdevs = normalizearray(trainingset, usedate)\n",
    "newmodel.fit(trainingset, yvals)\n",
    "\n",
    "coefficients = newmodel.coef_[0] * 100\n",
    "\n",
    "coefficientuples = list(zip(coefficients, \n",
    "                            (coefficients / np.array(stdevs)), \n",
    "                            vocablist + ['pub.date']))\n",
    "coefficientuples.sort()\n",
    "if verbose:\n",
    "    for coefficient, normalizedcoef, word in coefficientuples:\n",
    "        print(word + \" :  \" + str(coefficient))\n",
    "\n",
    "print()\n",
    "accuracy = (truepositives + truenegatives) / len(IDsToUse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: analysis of this code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Output\n",
    "\n",
    "This section of the code saves the data produced by the logistic regression to disk for further analysis and plotting. There is extant code, not included in this analysis, that produces the plots used in publication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "truepositives = 0\n",
    "truenegatives = 0\n",
    "falsepositives = 0\n",
    "falsenegatives = 0\n",
    "allvolumes = list()\n",
    "\n",
    "with open(outputpath, mode = 'w', encoding = 'utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    header = ['volid', \n",
    "              'reviewed', \n",
    "              'obscure', \n",
    "              'pubdate', \n",
    "              'birthdate', \n",
    "              'gender', \n",
    "              'nation', \n",
    "              'allwords', \n",
    "              'logistic', \n",
    "              'author', \n",
    "              'title', \n",
    "              'pubname', \n",
    "              'actually', \n",
    "              'realclass']\n",
    "    writer.writerow(header)\n",
    "    for volid in IDsToUse:\n",
    "        metadata = metadict[volid]\n",
    "        reviewed = metadata['reviewed']\n",
    "        obscure = metadata['obscure']\n",
    "        pubdate = infer_date(metadata, datetype)\n",
    "        birthdate = metadata['birthdate']\n",
    "        gender = metadata['gender']\n",
    "        nation = metadata['nation']\n",
    "        author = metadata['author']\n",
    "        title = metadata['title']\n",
    "        canonicity = metadata['canonicity']\n",
    "        pubname = metadata['pubname']\n",
    "        allwords = volsizes[volid]\n",
    "        logistic = logisticpredictions[volid]\n",
    "        realclass = classdictionary[volid]\n",
    "        outrow = [volid, \n",
    "                  reviewed, \n",
    "                  obscure, \n",
    "                  pubdate,\n",
    "                  birthdate, \n",
    "                  gender, \n",
    "                  nation, \n",
    "                  allwords, \n",
    "                  logistic, \n",
    "                  author, \n",
    "                  title, \n",
    "                  pubname, \n",
    "                  canonicity, \n",
    "                  realclass]\n",
    "        writer.writerow(outrow)\n",
    "        allvolumes.append(outrow)\n",
    "\n",
    "        if logistic > 0.5 and classdictionary[volid] > 0.5:\n",
    "            truepositives += 1\n",
    "        elif logistic <= 0.5 and classdictionary[volid] < 0.5:\n",
    "            truenegatives += 1\n",
    "        elif logistic <= 0.5 and classdictionary[volid] > 0.5:\n",
    "            falsenegatives += 1\n",
    "        elif logistic > 0.5 and classdictionary[volid] < 0.5:\n",
    "            falsepositives += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is a bit simpler than its predecessors. This block writes a CSV file to disk containing 720 rows of volume metadata, the predicted classification, and the actual classification.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coefficientpath = outputpath.replace('.csv', '.coefs.csv')\n",
    "with open(coefficientpath, mode = 'w', encoding = 'utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for triple in coefficientuples:\n",
    "        coef, normalizedcoef, word = triple\n",
    "        writer.writerow([word, coef, normalizedcoef])\n",
    "\n",
    "### DEFACTORING FUNCTION RETURN\n",
    "### return accuracy, allvolumes, coefficientuples\n",
    "\n",
    "### DEFACTORING NAMESPACE\n",
    "rawaccuracy = accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code generates the `mainmodelcoefficients.csv` output file, which contains the word, its coefficient and its normalized coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Results\n",
    "\n",
    "The final function of the analysis is to test the accuracy of the model(s). This code produces a plot giving a sense of how the model performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD7CAYAAABkO19ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPrTcOQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fec38ba5630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### DEFACTORING FUNCTION PARAMETERS\n",
    "modeltype = 'linear'\n",
    "datelimits = []\n",
    "''' Takes a set of predictions produced by a model that knows nothing about date,\n",
    "and divides it along a line with a diachronic tilt. We need to do this in a way\n",
    "that doesn't violate crossvalidation. I.e., we shouldn't \"know\" anything\n",
    "that the model didn't know. We tried a couple of different ways to do this, but\n",
    "the simplest and actually most reliable is to divide the whole dataset along a\n",
    "linear central trend line for the data!\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "listofrows = list()\n",
    "classvector = list()\n",
    "\n",
    "# DEPRECATED\n",
    "# if modeltype == 'logistic' and len(datelimits) == 2:\n",
    "#     # In this case we construct a subset of data to model on.\n",
    "#     tomodeldata = list()\n",
    "#     tomodelclasses = list()\n",
    "#     pastthreshold, futurethreshold = datelimits\n",
    "\n",
    "for volume in allvolumes:\n",
    "    date = volume[3]\n",
    "    logistic = volume[8]\n",
    "    realclass = volume[13]\n",
    "    listofrows.append([logistic, date])\n",
    "    classvector.append(realclass)\n",
    "\n",
    "    # DEPRECATED\n",
    "    # if modeltype == 'logistic' and len(datelimits) == 2:\n",
    "    #     if date >= pastthreshold and date <= futurethreshold:\n",
    "    #         tomodeldata.append([logistic, date])\n",
    "    #         tomodelclasses.append(realclass)\n",
    "\n",
    "y, x = [a for a in zip(*listofrows)]\n",
    "plt.axis([min(x) - 2, max(x) + 2, min(y) - 0.02, max(y) + 0.02])\n",
    "reviewedx = list()\n",
    "reviewedy = list()\n",
    "randomx = list()\n",
    "randomy = list()\n",
    "\n",
    "for idx, reviewcode in enumerate(classvector):\n",
    "    if reviewcode == 1:\n",
    "        reviewedx.append(x[idx])\n",
    "        reviewedy.append(y[idx])\n",
    "    else:\n",
    "        randomx.append(x[idx])\n",
    "        randomy.append(y[idx])\n",
    "\n",
    "plt.plot(reviewedx, reviewedy, 'ro')\n",
    "plt.plot(randomx, randomy, 'k+')\n",
    "\n",
    "if modeltype == 'logistic':\n",
    "    # all this is DEPRECATED\n",
    "    print(\"Hey, you're attempting to use the logistic-tilt option\")\n",
    "    print(\"that we deactivated. Go in and uncomment the code.\")\n",
    "\n",
    "    # if len(datelimits) == 2:\n",
    "    #     data = pd.DataFrame(tomodeldata)\n",
    "    #     responsevariable = tomodelclasses\n",
    "    # else:\n",
    "    #     data = pd.DataFrame(listofrows)\n",
    "    #     responsevariable = classvector\n",
    "\n",
    "    # newmodel = LogisticRegression(C = 100000)\n",
    "    # newmodel.fit(data, responsevariable)\n",
    "    # coefficients = newmodel.coef_[0]\n",
    "\n",
    "    # intercept = newmodel.intercept_[0] / (-coefficients[0])\n",
    "    # slope = coefficients[1] / (-coefficients[0])\n",
    "\n",
    "    # p = np.poly1d([slope, intercept])\n",
    "\n",
    "elif modeltype == 'linear':\n",
    "    # what we actually do\n",
    "\n",
    "    z = np.polyfit(x, y, 1)\n",
    "    p = np.poly1d(z)\n",
    "    slope = z[0]\n",
    "    intercept = z[1]\n",
    "\n",
    "plt.plot(x,p(x),\"b-\")\n",
    "plt.show(block = False)\n",
    "\n",
    "x = np.array(x, dtype='float64')\n",
    "y = np.array(y, dtype='float64')\n",
    "classvector = np.array(classvector)\n",
    "dividingline = intercept + (x * slope)\n",
    "predicted_as_reviewed = (y > dividingline)\n",
    "really_reviewed = (classvector == 1)\n",
    "\n",
    "accuracy = sum(predicted_as_reviewed == really_reviewed) / len(classvector)\n",
    "\n",
    "### DEFACTORING NAMESPACE\n",
    "tiltaccuracy = accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The code above generates a best fit line, using [numpy.polyfit](http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.polyfit.html), for all of the results. He then calculates which are predicted-as-reviewed, which is those that fall above this dividing line\n",
    "\n",
    "The accuracy of the dividing line is computed by the total number of predicted-as-reviewed that were actually reviewed divided by the total.\n",
    "\n",
    "Note, this is the accuracy of the dividing line, not the accuracy of the model(s) to predict the reviewed stats of the poems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If we divide the dataset with a horizontal line at 0.5, accuracy is:  0.775\n",
      "Divided with a line fit to the data trend, it's  0.791666666667\n"
     ]
    }
   ],
   "source": [
    "### DEFACTORING FUNCTION CALL\n",
    "### tiltaccuracy = diachronic_tilt(allvolumes, 'linear', []) \n",
    "\n",
    "print('If we divide the dataset with a horizontal line at 0.5, accuracy is: ', \n",
    "      str(rawaccuracy))\n",
    "\n",
    "print(\"Divided with a line fit to the data trend, it's \", \n",
    "      str(tiltaccuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: describe the diachronic tilt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "This notebook contains all of the code necessary to run through a full execution of the computational analysis.\n",
    "\n",
    "\n",
    "Open questions for Underwood & Sellers:\n",
    "- why does he normalize the word frequences (using `normalizearray()`) before modeling?\n",
    "- Why does he normalize the coefficients ( divided by standard deviation)?\n",
    "\n",
    "Basically, we don't have a firm understanding of why he is normalizing, mainly because we don't understand the meaning or significance of these steps on the interpretibility of the data. These are probably quite normal practices when performing logistic regression with these kinds of data, but there is a taken for granted quality to these actions that we'd like to see described as part of the methodology (or at least citations to a best practice). \n",
    "\n",
    "\n",
    "### Other musings\n",
    "\n",
    "\n",
    "#### Affordances of the Jupyter Platform \n",
    "\n",
    "We should more fully unpack the implications of the Jupyter *platform* and what it means to *read* the code. As we mentioned briefly above, the Jupyter platform allows us to interrogate the code and, more importantly, the execution environment described by the code (TODO: find some theory to talk about the relationship between the code and the computation, Ramsay? [Kittler](https://en.wikipedia.org/wiki/Friedrich_Kittler), there must be something in [critical code studies](https://en.wikipedia.org/wiki/Critical_code_studies) that unpacks this) in a performance of *reading* that is theoretically interesting. What we have is an act of reading at the intersection of Underwood and Seller's code, ourselves as critical interpreters, but also the computer as an interpreter (albiet not critical, but what kind of interpreter? *relentlessly literal*). This intersection is made possible by the Jupyter platform because we can work our way through the code, that is, step through it, at our own pace, creating critically meaningful blocks of Underwood and Seller's undifferentiated scripts. The Jupyter platform allows us to incrementally execute these blocks and inspect the state of the execution environment the code instantiates/describes. In this sense we are simultaneously reading the code as a text and as a set of instructions. We can *read into* the code and the computataional work behind the code to achieve a much greater depth. In some sense we can ***test*** our own assumptions about what the code does by actually doing it (TODO: is this there the Berry's *coping tests* come into play?). The moments when we do a *DEFACTOR INSPECTION* and create new code cells with own own code we are simultaneously doing an act of writing and reading. We are writing a *coping test* in order to read the state of the execution environment and *test* our understanding of that state which we we can interpret from our reading of the code, but can also *directly observe with our own code*. \n",
    "\n",
    "What kind of reading is this, it isn't distant reading. It is more akin to *close reading*, but I am not sure the close reading accounts for the interactivity and the inspection work that occurs in our proposed method. Defactoring is a method of reading complex objects and environments, a way of reading the code and the model of reality it instantiates. In making such a claim it is vitally important to understand the material circumstances that make this possible, that is to understand that the Jupyter *platform* is intimately wrapped up in the methodological construct. Defactoring cannot be pure theoretical method, it has to be practically grounded, we might say, *codified* in a technical platform. This isn't such an outrageous thing to say as book historians have been pointing out the material conditions of the monograph and its relationship to knowledge for ages (CITATION-NEEDED).\n",
    "\n",
    "This insight is nothing new to platform or code studies people who have spilled a shoal of ink discussing and debating this point. However, what has been lacking are efforts to *embody* these insights in an interrogation of code, computations, algorithms, and data. We have gone beyond simply pointing out the material conditions of code and *done something with it*. Queue the theory vs. practice divide, but we argue for the construction of bridges and the crossing back and forth precisely because it allows us an avenue towards deeper insight into the production of data and computationally inflected knowledge–such as the work of Underwood and Sellers in Pace of Change. \n",
    "\n",
    "We argue the attributes of data-driven, computational, and algorithmic scholarship such as Pace of Change warrants a different kind of peer review, a form of review that dives into the code (we'd also argue it warrants a different kind of publishing, but we can only address one issue at a time ;). Given the fact the data and the code are just as important as charts and the write-up that are currently the currency of scholarly publishing and communication. \n",
    "\n",
    "\n",
    "#### Software Layers\n",
    "\n",
    "[This blog post by Konrad Hinsen](http://blog.khinsen.net/posts/2017/01/13/sustainable-software-and-reproducible-research-dealing-with-software-collapse/) talks about the sustainability and reproducibility of scientific software, I think many of the points are relevant here. Specific he talks about *software collapse*, which is the foundations of scientific software become unstable and the code/softare used in a discovery ceases to function. He prefers the *collapse* metaphor to the more common *rot* metaphoe because if emphasises the fact the software hasn't change but the context surrounding that software, the foundations, have changed.\n",
    "\n",
    "Another interestion point from the post are the four layers of scientific software. These are useful for us with respect to Defactoring because it helps illuminate the specific kind of software/code we are looking to focus our attention upon. In Hinsen's model there are four layers of scientific software:\n",
    "\n",
    "1. Non-scientific, or generalized computational infrastructure like operating systems, compilers, and user interfaces. In `paceofchange` this would be the Docker container and Python runtimes (and everything underneath them) that make up the environment.\n",
    "2. Scientific computational infrastructure, which includes the generalized  libraries implementing particular algorithms or data structures commonly used across scientific settings. For us, this includes the 3rd party libraries like the Logistic Regession implementation from `scikit-learn.`\n",
    "3. Hinsen posits there is a third layer of disciplin-specific research software that are similar to those in layer two, but specialized for a specific disciplinary community. There isn't an example with this specific project, but Matt Jocker's [syuzhet](https://github.com/mjockers/syuzhet) package for R would be a good example for literary analysis.\n",
    "4. The fourth layer is project specific code developed in pursuit of the very specific set of tasks associated with a particular analysis. This code often includes plumbing code that connects the other layers to accomplish a desired outcome, such as performing a logistic regression to create a predictive model of reviewed and unreviewed poetry volumes. The code that we have described in this notebook would be an example of this kind of project specific software.\n",
    "\n",
    "The model of different software layers helps explain the kinds of code most suitable for Defactoring, the fourth player–project specific code. As Hinson points out, layer four software is the least likely to be shared or preserved because it is bespoke code intended only for a specific use case. What we hope to achieve is the creation of ag mechnism that would make this code integral to the publication process and thus make publishing forth layer code a requirement for computational and data intensive research. While it is wonderful to share this code, there needs to be stronger incentives to distribute this kind of code, so argue layer four code needs to be peer-reviewed alongside the narrative that uses it. Defactoring provides a method for reviewing this kind of code and contribute to the editorial aspect of formal scholarly publishing pipelines.\n",
    "\n",
    "\n",
    "#### Publishing\n",
    "\n",
    "* [A blog post on preparing a notebook for publication](http://blog.juliusschulz.de/blog/ultimate-ipython-notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
